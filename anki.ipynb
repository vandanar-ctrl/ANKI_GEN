{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bbdfe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ab2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3498e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b269ff8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# BASE_DIR = \"/content/drive/MyDrive/anki_mindmap_LLM\"\n",
    "\n",
    "# OLLAMA_DIR = f\"{BASE_DIR}/ollama\"\n",
    "# MODELS_DIR = f\"{OLLAMA_DIR}/models\"\n",
    "\n",
    "# # os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# print(\"âœ… Ollama model directory ready\")\n",
    "# print(\"MODELS:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6361b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%env OLLAMA_MODELS=/content/drive/MyDrive/anki_mindmap_LLM/ollama/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214344a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nohup /usr/local/bin/ollama serve > ollama.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cde5da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce47ec1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bloom-filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d83771",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/content/drive/MyDrive/anki_mindmap_LLM/input/metadata.csv\")\n",
    "\n",
    "print(\"Exists:\", path.exists())\n",
    "print(\"Is file:\", path.is_file())\n",
    "print(\"Parent exists:\", path.parent.exists())\n",
    "print(\"Parent contents:\", list(path.parent.glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1d8a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: CONFIGURATION & CONSTANTS (FIXED VERSION)\n",
    "# ============================================================\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "from threading import Lock\n",
    "from functools import lru_cache\n",
    "from types import MappingProxyType\n",
    "\n",
    "# Setup logging before anything else\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('/tmp/anki_pipeline.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"anki-pipeline\")\n",
    "\n",
    "# Optional bloom filter (graceful degradation if not available)\n",
    "try:\n",
    "    from bloom_filter2 import BloomFilter\n",
    "    BLOOM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BLOOM_AVAILABLE = False\n",
    "    logger.warning(\"bloom_filter2 not installed - using set-based deduplication\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG - IMMUTABLE\n",
    "# ============================================================\n",
    "def load_config():\n",
    "    \"\"\"Load configuration with proper validation\"\"\"\n",
    "    # Determine CSV location\n",
    "    CSV_LOCATIONS = [\n",
    "        \"/content/drive/MyDrive/anki_mindmap_LLM/input/metadata.csv\",\n",
    "        \"./metadata.csv\",\n",
    "        os.getenv(\"ANKI_CSV_PATH\", \"\"),\n",
    "    ]\n",
    "    \n",
    "    CSV_FILE = None\n",
    "    for loc in CSV_LOCATIONS:\n",
    "        if loc and Path(loc).exists():\n",
    "            CSV_FILE = loc\n",
    "            break\n",
    "    \n",
    "    if not CSV_FILE:\n",
    "        CSV_FILE = CSV_LOCATIONS[0]\n",
    "    \n",
    "    # Determine base directory\n",
    "    BASE_DIR = Path(os.getenv(\"ANKI_BASE_DIR\", \"/content/drive/MyDrive/anki_mindmap_LLM\"))\n",
    "    \n",
    "    config = {\n",
    "        # Core paths\n",
    "        \"OLLAMA_URL\": \"http://127.0.0.1:11434\",\n",
    "        \"CSV_FILE\": CSV_FILE,\n",
    "        \"BASE_DIR\": BASE_DIR,\n",
    "        \"OUT_DIR\": BASE_DIR / \"output\",\n",
    "        \n",
    "        # Processing limits\n",
    "        \"BATCH_SIZE\": 5,\n",
    "        \"MAX_WORKERS\": 4,\n",
    "        \"MAX_REELS\": 1000,\n",
    "        \"CONFIDENCE_THRESHOLD\": 0.65,\n",
    "        \"FINGERPRINT_BATCH_SIZE\": 100,\n",
    "        \"CACHE_VERSION\": \"v4_fixed\",\n",
    "        \n",
    "        # Circuit breaker settings\n",
    "        \"CIRCUIT_BREAKER_THRESHOLD\": 5,\n",
    "        \"CIRCUIT_BREAKER_TIMEOUT\": 60,\n",
    "        \n",
    "        # PRODUCTION SAFETY SWITCHES\n",
    "        \"ENABLE_ENRICHMENT\": True,\n",
    "        \"ENABLE_REJECTION_LEARNING\": True,\n",
    "        \"ENABLE_TRADEOFFS\": True,\n",
    "        \"ENABLE_PROMPT_ROUTING\": True,\n",
    "        \"ENABLE_FOUNDATION_EXPANSION\": True,\n",
    "        \n",
    "        # PHASE 1-3: ADVANCED CONTENT PROCESSING\n",
    "        \"ENABLE_CONTENT_FILTERING\": True,\n",
    "        \"ENABLE_TRANSCRIPT_NORMALIZATION\": True,\n",
    "        \"ENABLE_HYBRID_ROUTING\": True,\n",
    "        \n",
    "        # ENRICHMENT CONTROLS\n",
    "        \"MAX_ENRICHMENTS_PER_CONCEPT\": {\n",
    "            \"foundation\": 1,\n",
    "            \"intermediate\": 2,\n",
    "            \"advanced\": 2\n",
    "        },\n",
    "        \"MAX_RETRIES\": 3,\n",
    "        \n",
    "        # COMPLETION THRESHOLDS\n",
    "        \"MIN_CARDS_FOR_FULL\": 3,\n",
    "        \"MIN_CARDS_FOR_PARTIAL\": 2,\n",
    "        \n",
    "        # NORMALIZATION\n",
    "        \"NORMALIZE_TECH_SCORES\": True,\n",
    "        \n",
    "        # Content density thresholds\n",
    "        \"DENSE_CONTENT_MIN_WORDS\": 150,\n",
    "        \"LIGHT_CONTENT_MAX_WORDS\": 100,\n",
    "        \n",
    "        # Validation\n",
    "        \"MIN_TRANSCRIPT_LENGTH\": 80,\n",
    "        \"MIN_CATEGORY_CONFIDENCE\": 70,\n",
    "    }\n",
    "    \n",
    "    # Create directories\n",
    "    config[\"OUT_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create cache directory\n",
    "    CACHE_DIR = config[\"OUT_DIR\"] / \"cache\"\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    config[\"CACHE_DIR\"] = CACHE_DIR\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load and make config immutable\n",
    "_CONFIG = load_config()\n",
    "CONFIG = MappingProxyType(_CONFIG)\n",
    "logger.info(f\"Loaded configuration from {CONFIG['CSV_FILE']}\")\n",
    "\n",
    "# File paths\n",
    "PROGRESS_FILE = CONFIG[\"OUT_DIR\"] / \"processed.txt\"\n",
    "CARD_FINGERPRINTS_FILE = CONFIG[\"OUT_DIR\"] / \"card_fingerprints.json\"\n",
    "BLOOM_FILE = CONFIG[\"OUT_DIR\"] / \"card_bloom.bin\"\n",
    "REJECTION_MEMORY_FILE = CONFIG[\"OUT_DIR\"] / \"rejection_memory.json\"\n",
    "PROMPT_VERSION_FILE = CONFIG[\"OUT_DIR\"] / \"prompt_version_stats.json\"\n",
    "ROUTING_METRICS_FILE = CONFIG[\"OUT_DIR\"] / \"routing_metrics.json\"\n",
    "TERMINAL_REJECTIONS_FILE = CONFIG[\"OUT_DIR\"] / \"terminal_rejections.json\"\n",
    "CONFIDENCE_CALIBRATION_FILE = CONFIG[\"OUT_DIR\"] / \"confidence_calibration.json\"\n",
    "ERROR_LOG_FILE = CONFIG[\"OUT_DIR\"] / \"error_log.json\"\n",
    "\n",
    "# ============================================================\n",
    "# CIRCUIT BREAKER PATTERN\n",
    "# ============================================================\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker to prevent cascade failures\"\"\"\n",
    "    \n",
    "    def __init__(self, name, failure_threshold=5, reset_timeout=60):\n",
    "        self.name = name\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.reset_timeout = reset_timeout\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = 0\n",
    "        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def is_open(self):\n",
    "        \"\"\"Check if circuit breaker is open\"\"\"\n",
    "        with self.lock:\n",
    "            if self.state == \"OPEN\":\n",
    "                # Check if timeout has passed\n",
    "                if time.time() - self.last_failure_time > self.reset_timeout:\n",
    "                    self.state = \"HALF_OPEN\"\n",
    "                    logger.info(f\"Circuit breaker {self.name} moving to HALF_OPEN\")\n",
    "                    return False\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"Record a failure and potentially open the circuit\"\"\"\n",
    "        with self.lock:\n",
    "            self.failures += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            if self.failures >= self.failure_threshold:\n",
    "                if self.state != \"OPEN\":\n",
    "                    self.state = \"OPEN\"\n",
    "                    logger.warning(f\"Circuit breaker {self.name} OPENED after {self.failures} failures\")\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"Record a success and potentially close the circuit\"\"\"\n",
    "        with self.lock:\n",
    "            if self.state == \"HALF_OPEN\":\n",
    "                self.state = \"CLOSED\"\n",
    "                self.failures = 0\n",
    "                logger.info(f\"Circuit breaker {self.name} CLOSED after successful trial\")\n",
    "            elif self.state == \"CLOSED\":\n",
    "                # Decay failures slowly\n",
    "                self.failures = max(0, self.failures - 1)\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Wrap a function call with circuit breaker protection\"\"\"\n",
    "        if self.is_open():\n",
    "            raise RuntimeError(f\"Circuit breaker {self.name} is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self.record_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.record_failure()\n",
    "            raise\n",
    "\n",
    "# Initialize circuit breakers\n",
    "OLLAMA_CIRCUIT_BREAKER = CircuitBreaker(\n",
    "    \"ollama\",\n",
    "    failure_threshold=CONFIG[\"CIRCUIT_BREAKER_THRESHOLD\"],\n",
    "    reset_timeout=CONFIG[\"CIRCUIT_BREAKER_TIMEOUT\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ERROR TRACKING\n",
    "# ============================================================\n",
    "class ErrorTracker:\n",
    "    \"\"\"Track and categorize errors for observability\"\"\"\n",
    "    \n",
    "    def __init__(self, error_file: Path):\n",
    "        self.error_file = error_file\n",
    "        self.errors = []\n",
    "        self.error_counts = defaultdict(int)\n",
    "        self.lock = Lock()\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Load existing errors\"\"\"\n",
    "        if self.error_file.exists():\n",
    "            try:\n",
    "                with open(self.error_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.errors = data.get(\"errors\", [])\n",
    "                    self.error_counts = defaultdict(int, data.get(\"counts\", {}))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load error tracker: {e}\")\n",
    "    \n",
    "    def record(self, error_type: str, message: str, context: Dict = None):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        with self.lock:\n",
    "            error_entry = {\n",
    "                \"type\": error_type,\n",
    "                \"message\": message,\n",
    "                \"context\": context or {},\n",
    "                \"timestamp\": time.time(),\n",
    "                \"iso_timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "            }\n",
    "            \n",
    "            self.errors.append(error_entry)\n",
    "            self.error_counts[error_type] += 1\n",
    "            \n",
    "            # Keep only last 1000 errors\n",
    "            if len(self.errors) > 1000:\n",
    "                self.errors = self.errors[-1000:]\n",
    "            \n",
    "            # Periodic save\n",
    "            if len(self.errors) % 50 == 0:\n",
    "                self.save()\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save errors to disk\"\"\"\n",
    "        with self.lock:\n",
    "            try:\n",
    "                with open(self.error_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        \"errors\": self.errors[-500:],  # Keep only recent errors\n",
    "                        \"counts\": dict(self.error_counts),\n",
    "                        \"total_errors\": len(self.errors)\n",
    "                    }, f, indent=2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save error tracker: {e}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get error statistics\"\"\"\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"total_errors\": len(self.errors),\n",
    "                \"error_counts\": dict(self.error_counts),\n",
    "                \"recent_errors\": self.errors[-20:] if self.errors else []\n",
    "            }\n",
    "\n",
    "# Initialize error tracker\n",
    "error_tracker = ErrorTracker(ERROR_LOG_FILE)\n",
    "\n",
    "# ============================================================\n",
    "# ATOMIC FILE OPERATIONS\n",
    "# ============================================================\n",
    "def atomic_write(data, filepath: Path):\n",
    "    \"\"\"Write data atomically to prevent corruption\"\"\"\n",
    "    import tempfile\n",
    "    temp_path = Path(str(filepath) + '.tmp')\n",
    "    try:\n",
    "        with open(temp_path, 'w') as f:\n",
    "            if isinstance(data, (dict, list)):\n",
    "                json.dump(data, f, indent=2)\n",
    "            else:\n",
    "                f.write(str(data))\n",
    "        temp_path.replace(filepath)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Atomic write failed: {e}\")\n",
    "        # Clean up temp file\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink()\n",
    "        raise\n",
    "\n",
    "def atomic_read(filepath: Path, default=None):\n",
    "    \"\"\"Read data with corruption recovery\"\"\"\n",
    "    if not filepath.exists():\n",
    "        return default\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            if filepath.suffix == '.json':\n",
    "                return json.load(f)\n",
    "            else:\n",
    "                return f.read()\n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        logger.warning(f\"File {filepath} corrupted, attempting recovery: {e}\")\n",
    "        # Try to backup corrupted file\n",
    "        backup_path = filepath.with_suffix('.corrupted')\n",
    "        try:\n",
    "            filepath.rename(backup_path)\n",
    "        except:\n",
    "            pass\n",
    "        return default\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZATION STATUS\n",
    "# ============================================================\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"ANKI GENERATION PIPELINE - INITIALIZED\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Output directory: {CONFIG['OUT_DIR']}\")\n",
    "logger.info(f\"CSV file: {CONFIG['CSV_FILE']}\")\n",
    "logger.info(f\"Max reels: {CONFIG['MAX_REELS']}\")\n",
    "logger.info(f\"Circuit breaker: {CONFIG['CIRCUIT_BREAKER_THRESHOLD']} failures / {CONFIG['CIRCUIT_BREAKER_TIMEOUT']}s timeout\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecee9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: DATA TYPES & SCHEMAS (COMPLETE VERSION)\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# ENUM DEFINITIONS (MUST BE AT TOP LEVEL)\n",
    "# ============================================================\n",
    "class CompletionState(Enum):\n",
    "    FULL = \"full\"\n",
    "    PARTIAL = \"partial\"\n",
    "    INCOMPLETE = \"incomplete\"\n",
    "\n",
    "class RejectionType(Enum):\n",
    "    SEMANTIC = \"rejected_semantic\"\n",
    "    STRUCTURAL = \"rejected_structural\"\n",
    "    MECHANICAL = \"error_mechanical\"\n",
    "    TERMINAL = \"terminal_rejected\"\n",
    "\n",
    "class RejectionReason(Enum):\n",
    "    \"\"\"Typed rejection reasons for analytics\"\"\"\n",
    "    LOW_TECHNICAL_SIGNAL = \"low_technical_signal\"\n",
    "    INSUFFICIENT_SOURCE = \"insufficient_source_material\"\n",
    "    MOTIVATIONAL_CONTENT = \"motivational_content\"\n",
    "    PROMOTIONAL_CTA = \"promotional_cta\"\n",
    "    DUPLICATE_CONTENT = \"duplicate_content\"\n",
    "    INVALID_SCHEMA = \"invalid_schema\"\n",
    "    NO_CARDS_GENERATED = \"no_cards_generated\"\n",
    "    ALL_CARDS_DUPLICATES = \"all_cards_duplicates\"\n",
    "    TOO_SHORT = \"too_short\"\n",
    "    LOW_CONFIDENCE_CATEGORY = \"low_confidence_category\"\n",
    "    REPEATEDLY_FAILED = \"repeatedly_failed_enrichment\"\n",
    "    HANDLED_BY_LOGIC = \"handled_by_logic\"\n",
    "\n",
    "class ContentDensity(Enum):\n",
    "    DENSE = \"dense\"\n",
    "    LIGHT = \"light\"\n",
    "    SKIP = \"skip\"\n",
    "\n",
    "class PromptStrategy(Enum):\n",
    "    STRICT_ADVANCED = \"A_STRICT\"\n",
    "    FOUNDATION_AWARE = \"B_FOUNDATION\"\n",
    "    DSA_FOCUSED = \"C_DSA\"\n",
    "\n",
    "class ContentIntent(Enum):\n",
    "    \"\"\"What the content is trying to achieve\"\"\"\n",
    "    TUTORIAL = \"tutorial\"\n",
    "    PROBLEM_SOLVING = \"problem_solving\"\n",
    "    OVERVIEW = \"overview\"\n",
    "    MOTIVATIONAL = \"motivational\"\n",
    "    COMPARISON = \"comparison\"\n",
    "    WARNING = \"warning\"\n",
    "    BEST_PRACTICE = \"best_practice\"\n",
    "\n",
    "class CardType(Enum):\n",
    "    BASIC = \"basic\"\n",
    "    CLOZE = \"cloze\"\n",
    "    TRADEOFF = \"tradeoff\"\n",
    "\n",
    "# ============================================================\n",
    "# DATA CLASSES\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class QualityDimensions:\n",
    "    correctness_score: float\n",
    "    richness_score: float\n",
    "    combined_score: float\n",
    "\n",
    "    @classmethod\n",
    "    def calculate(cls, atoms: Dict, cards: List[Dict]) -> 'QualityDimensions':\n",
    "        \"\"\"\n",
    "        Calculate quality dimensions with proper validation.\n",
    "        \"\"\"\n",
    "        has_definition = bool(atoms.get(\"definition\", \"\").strip())\n",
    "        tech_points = atoms.get(\"technical_points\", [])\n",
    "        has_tech_points = len([p for p in tech_points if p.strip()]) >= 3\n",
    "        \n",
    "        solutions = atoms.get(\"solutions\", [])\n",
    "        has_solutions = len([s for s in solutions if s.strip()]) >= 1\n",
    "\n",
    "        correctness = (\n",
    "            (0.4 if has_definition else 0) +\n",
    "            (0.4 if has_tech_points else 0) +\n",
    "            (0.2 if has_solutions else 0)\n",
    "        )\n",
    "\n",
    "        card_count = len(cards)\n",
    "        related_concepts = atoms.get(\"related_concepts\", [])\n",
    "        has_related = len([rc for rc in related_concepts if rc.get(\"name\")]) > 0\n",
    "        has_tradeoffs = atoms.get(\"has_tradeoffs\", False)\n",
    "\n",
    "        richness = (\n",
    "            min(card_count / 3, 0.5) +\n",
    "            (0.3 if has_related else 0) +\n",
    "            (0.2 if has_tradeoffs else 0)\n",
    "        )\n",
    "        richness = min(max(richness, 0), 1.0)  # Clamp between 0-1\n",
    "\n",
    "        combined = correctness * 0.6 + richness * 0.4\n",
    "\n",
    "        return cls(\n",
    "            correctness_score=round(correctness, 2),\n",
    "            richness_score=round(richness, 2),\n",
    "            combined_score=round(combined, 2)\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    gpu_layers: int = -1\n",
    "    temperature: float = 0.1\n",
    "    num_predict: int = 2000\n",
    "    top_p: float = 0.9\n",
    "    timeout: int = 600\n",
    "    \n",
    "    def to_ollama_payload(self, prompt: str) -> Dict:\n",
    "        \"\"\"Convert to Ollama API payload\"\"\"\n",
    "        return {\n",
    "            \"model\": self.name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": self.temperature,\n",
    "                \"top_p\": self.top_p,\n",
    "                \"num_predict\": self.num_predict,\n",
    "                \"num_gpu\": self.gpu_layers if self.gpu_layers > 0 else None\n",
    "            }\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class ReelMetadata:\n",
    "    \"\"\"Structured metadata for a reel\"\"\"\n",
    "    reel_id: str\n",
    "    caption: str\n",
    "    transcript: str\n",
    "    category: str = \"\"\n",
    "    category_confidence: int = 0\n",
    "    source_url: str = \"\"\n",
    "    created_at: str = \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'ReelMetadata':\n",
    "        \"\"\"Create from raw dictionary data\"\"\"\n",
    "        return cls(\n",
    "            reel_id=str(data.get(\"reel_id\", data.get(\"id\", \"\"))),\n",
    "            caption=str(data.get(\"caption\", \"\")),\n",
    "            transcript=str(data.get(\"transcript\", \"\")),\n",
    "            category=str(data.get(\"category\", \"\")),\n",
    "            category_confidence=int(data.get(\"category_confidence\", 0) or 0),\n",
    "            source_url=str(data.get(\"source_url\", data.get(\"url\", \"\"))),\n",
    "            created_at=str(data.get(\"created_at\", data.get(\"timestamp\", \"\")))\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    \"\"\"Result of processing a single reel\"\"\"\n",
    "    reel_id: str\n",
    "    status: str\n",
    "    reason: str = \"\"\n",
    "    atoms: Optional[Dict] = None\n",
    "    cards: List[Dict] = None\n",
    "    confidence: float = 0.0\n",
    "    tech_score: int = 0\n",
    "    topic_class: str = \"\"\n",
    "    prompt_strategy: str = \"\"\n",
    "    processing_time: float = 0.0\n",
    "    error_details: Optional[Dict] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for serialization\"\"\"\n",
    "        return {\n",
    "            \"reel_id\": self.reel_id,\n",
    "            \"status\": self.status,\n",
    "            \"reason\": self.reason,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"tech_score\": self.tech_score,\n",
    "            \"topic_class\": self.topic_class,\n",
    "            \"prompt_strategy\": self.prompt_strategy,\n",
    "            \"processing_time\": self.processing_time,\n",
    "            \"card_count\": len(self.cards) if self.cards else 0,\n",
    "            \"error_details\": self.error_details,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# SCHEMA CLASSES WITH VALIDATION\n",
    "# ============================================================\n",
    "class BaseSchema:\n",
    "    required: Set[str] = set()\n",
    "    optional: Set[str] = set()\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls, data: dict) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate schema, return (is_valid, errors)\n",
    "        \"\"\"\n",
    "        if not isinstance(data, dict):\n",
    "            return False, [\"Data must be a dictionary\"]\n",
    "        \n",
    "        errors = []\n",
    "        \n",
    "        # Check required fields\n",
    "        missing = cls.required - set(data.keys())\n",
    "        if missing:\n",
    "            errors.append(f\"Missing required fields: {missing}\")\n",
    "        \n",
    "        # Check field types (basic validation)\n",
    "        for field in cls.required:\n",
    "            if field in data and data[field] is None:\n",
    "                errors.append(f\"Field '{field}' cannot be None\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    @classmethod\n",
    "    def sanitize(cls, data: dict) -> dict:\n",
    "        \"\"\"Sanitize data to match schema\"\"\"\n",
    "        sanitized = {}\n",
    "        all_fields = cls.required.union(cls.optional)\n",
    "        \n",
    "        for field in all_fields:\n",
    "            if field in data:\n",
    "                value = data[field]\n",
    "                # Basic type preservation\n",
    "                sanitized[field] = value\n",
    "        \n",
    "        return sanitized\n",
    "\n",
    "class AtomsSchema(BaseSchema):\n",
    "    required = {\n",
    "        \"concept\", \"category\", \"definition\",\n",
    "        \"technical_points\", \"solutions\", \"impact\", \"has_tradeoffs\",\n",
    "    }\n",
    "    optional = {\n",
    "        \"valid\", \"reject_reason\", \"related_concepts\", \n",
    "        \"learning_key\", \"topic_class\", \"tech_score\",\n",
    "        \"was_enriched\", \"prompt_version\", \"routing_reason\"\n",
    "    }\n",
    "\n",
    "class BasicCardSchema(BaseSchema):\n",
    "    required = {\"type\", \"front\", \"back\"}\n",
    "    optional = {\"concept_source\", \"tags\", \"quality\", \"priority\", \"reel_id\"}\n",
    "\n",
    "class ClozeCardSchema(BaseSchema):\n",
    "    required = {\"type\", \"cloze\"}\n",
    "    optional = {\"concept_source\", \"tags\", \"quality\", \"priority\", \"reel_id\"}\n",
    "\n",
    "class TradeoffCardSchema(BaseSchema):\n",
    "    required = {\"type\", \"tradeoffs\"}\n",
    "    optional = {\"front\", \"concept_source\", \"tags\", \"quality\", \"priority\", \"reel_id\"}\n",
    "\n",
    "class CardsContainerSchema(BaseSchema):\n",
    "    required = {\"cards\"}\n",
    "    optional = {\"count\", \"confidence\", \"completion_state\"}\n",
    "\n",
    "class EnrichmentRequestSchema(BaseSchema):\n",
    "    \"\"\"Schema for enrichment requests\"\"\"\n",
    "    required = {\"original_atoms\", \"reason\", \"attempt_number\"}\n",
    "    optional = {\"strategy_hint\", \"max_retries\", \"timeout\"}\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL STATE (PROPERLY MANAGED)\n",
    "# ============================================================\n",
    "from threading import Lock\n",
    "\n",
    "# Thread-safe metrics with locks\n",
    "class MetricsTracker:\n",
    "    \"\"\"Thread-safe metrics tracking\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "        self.timings = defaultdict(list)\n",
    "        self.lock = Lock()\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def increment(self, key: str, value: int = 1):\n",
    "        \"\"\"Increment a counter\"\"\"\n",
    "        with self.lock:\n",
    "            self.counts[key] += value\n",
    "    \n",
    "    def record_timing(self, key: str, duration: float):\n",
    "        \"\"\"Record a timing measurement\"\"\"\n",
    "        with self.lock:\n",
    "            self.timings[key].append(duration)\n",
    "            # Keep only last 1000 measurements\n",
    "            if len(self.timings[key]) > 1000:\n",
    "                self.timings[key] = self.timings[key][-1000:]\n",
    "    \n",
    "    def get_stats(self, key: str) -> Dict:\n",
    "        \"\"\"Get statistics for a key\"\"\"\n",
    "        with self.lock:\n",
    "            if key in self.timings and self.timings[key]:\n",
    "                values = self.timings[key]\n",
    "                return {\n",
    "                    \"count\": len(values),\n",
    "                    \"mean\": sum(values) / len(values),\n",
    "                    \"min\": min(values),\n",
    "                    \"max\": max(values),\n",
    "                    \"p95\": sorted(values)[int(len(values) * 0.95)] if len(values) > 1 else values[0]\n",
    "                }\n",
    "            return {\"count\": 0}\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Export all metrics\"\"\"\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"counts\": dict(self.counts),\n",
    "                \"timings\": {k: self.get_stats(k) for k in self.timings},\n",
    "                \"uptime\": time.time() - self.start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all metrics (for testing)\"\"\"\n",
    "        with self.lock:\n",
    "            self.counts.clear()\n",
    "            self.timings.clear()\n",
    "            self.start_time = time.time()\n",
    "\n",
    "# Initialize global trackers\n",
    "ROUTING_METRICS = MetricsTracker()\n",
    "PROCESSING_METRICS = MetricsTracker()\n",
    "\n",
    "# Model capability cache with lock\n",
    "MODEL_CAPABILITY_CACHE = {}\n",
    "MODEL_CAPABILITY_LOCK = Lock()\n",
    "\n",
    "# Prompt version stats with lock\n",
    "PROMPT_VERSION_STATS = {}\n",
    "PROMPT_VERSION_LOCK = Lock()\n",
    "\n",
    "# Confidence calibration with lock\n",
    "CONFIDENCE_CALIBRATION = {\n",
    "    \"buckets\": {\n",
    "        \"0.5-0.6\": {\"total\": 0, \"accepted\": 0},\n",
    "        \"0.6-0.7\": {\"total\": 0, \"accepted\": 0},\n",
    "        \"0.7-0.8\": {\"total\": 0, \"accepted\": 0},\n",
    "        \"0.8-0.9\": {\"total\": 0, \"accepted\": 0},\n",
    "        \"0.9-1.0\": {\"total\": 0, \"accepted\": 0}\n",
    "    },\n",
    "    \"total_checks\": 0,\n",
    "    \"calibration_factor\": 1.0\n",
    "}\n",
    "CONFIDENCE_CALIBRATION_LOCK = Lock()\n",
    "\n",
    "# Enrichment budget tracking\n",
    "ENRICHMENT_BUDGET = defaultdict(int)\n",
    "ENRICHMENT_TIMESTAMPS = defaultdict(float)\n",
    "ENRICHMENT_BUDGET_LOCK = Lock()\n",
    "ENRICHMENT_BUDGET_RESET_DAYS = 7\n",
    "\n",
    "# Performance monitoring\n",
    "PERFORMANCE_SAMPLES = {\n",
    "    \"extract_atoms\": [],\n",
    "    \"generate_cards\": [],\n",
    "    \"total_processing\": []\n",
    "}\n",
    "PERFORMANCE_LOCK = Lock()\n",
    "MAX_PERFORMANCE_SAMPLES = 1000\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATION UTILITIES\n",
    "# ============================================================\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Custom exception for validation errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def validate_and_sanitize(data: dict, schema_class) -> dict:\n",
    "    \"\"\"\n",
    "    Validate data against schema and return sanitized version.\n",
    "    Raises ValidationError if invalid.\n",
    "    \"\"\"\n",
    "    is_valid, errors = schema_class.validate(data)\n",
    "    if not is_valid:\n",
    "        raise ValidationError(f\"Schema validation failed: {errors}\")\n",
    "    \n",
    "    return schema_class.sanitize(data)\n",
    "\n",
    "def validate_card(card: dict) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate a card based on its type.\n",
    "    \"\"\"\n",
    "    card_type = card.get(\"type\", \"basic\")\n",
    "    \n",
    "    if card_type == \"basic\":\n",
    "        schema = BasicCardSchema\n",
    "    elif card_type == \"cloze\":\n",
    "        schema = ClozeCardSchema\n",
    "    elif card_type == \"tradeoff\":\n",
    "        schema = TradeoffCardSchema\n",
    "    else:\n",
    "        return False, [f\"Unknown card type: {card_type}\"]\n",
    "    \n",
    "    return schema.validate(card)\n",
    "\n",
    "def validate_atoms(atoms: dict) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate atoms with additional semantic checks.\n",
    "    \"\"\"\n",
    "    # Schema validation\n",
    "    is_valid, errors = AtomsSchema.validate(atoms)\n",
    "    if not is_valid:\n",
    "        return False, errors\n",
    "    \n",
    "    # Semantic validation\n",
    "    if not atoms.get(\"valid\", True):\n",
    "        return False, [\"Atoms marked as invalid\"]\n",
    "    \n",
    "    concept = atoms.get(\"concept\", \"\").strip()\n",
    "    if not concept:\n",
    "        errors.append(\"Concept cannot be empty\")\n",
    "    \n",
    "    definition = atoms.get(\"definition\", \"\").strip()\n",
    "    if not definition:\n",
    "        errors.append(\"Definition cannot be empty\")\n",
    "    \n",
    "    tech_points = atoms.get(\"technical_points\", [])\n",
    "    if not tech_points or len([p for p in tech_points if p.strip()]) < 2:\n",
    "        errors.append(\"Need at least 2 technical points\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "# ============================================================\n",
    "# SERIALIZATION UTILITIES\n",
    "# ============================================================\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder for our data types\"\"\"\n",
    "    \n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Enum):\n",
    "            return obj.value\n",
    "        if isinstance(obj, (ReelMetadata, ProcessingResult, ModelConfig)):\n",
    "            return obj.__dict__\n",
    "        if isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        if hasattr(obj, 'to_dict'):\n",
    "            return obj.to_dict()\n",
    "        \n",
    "        return super().default(obj)\n",
    "\n",
    "def serialize_result(result: ProcessingResult) -> str:\n",
    "    \"\"\"Serialize processing result to JSON\"\"\"\n",
    "    return json.dumps(result.to_dict(), cls=JSONEncoder, indent=2)\n",
    "\n",
    "def deserialize_result(data: dict) -> ProcessingResult:\n",
    "    \"\"\"Deserialize processing result from dictionary\"\"\"\n",
    "    return ProcessingResult(**data)\n",
    "\n",
    "# ============================================================\n",
    "# STATISTICS AND ANALYTICS\n",
    "# ============================================================\n",
    "class PipelineStatistics:\n",
    "    \"\"\"Collect and analyze pipeline statistics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            \"reels_processed\": 0,\n",
    "            \"reels_successful\": 0,\n",
    "            \"reels_failed\": 0,\n",
    "            \"reels_rejected\": 0,\n",
    "            \"cards_generated\": 0,\n",
    "            \"duplicates_filtered\": 0,\n",
    "            \"enrichment_attempts\": 0,\n",
    "            \"enrichment_successes\": 0,\n",
    "            \"avg_confidence\": 0.0,\n",
    "            \"avg_processing_time\": 0.0,\n",
    "            \"by_topic_class\": defaultdict(int),\n",
    "            \"by_content_density\": defaultdict(int),\n",
    "            \"by_rejection_reason\": defaultdict(int),\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def update(self, result: ProcessingResult):\n",
    "        \"\"\"Update statistics with processing result\"\"\"\n",
    "        with self.lock:\n",
    "            self.stats[\"reels_processed\"] += 1\n",
    "            \n",
    "            if result.status == \"success\":\n",
    "                self.stats[\"reels_successful\"] += 1\n",
    "                self.stats[\"cards_generated\"] += len(result.cards or [])\n",
    "                self.stats[\"avg_confidence\"] = (\n",
    "                    (self.stats[\"avg_confidence\"] * (self.stats[\"reels_successful\"] - 1) + result.confidence) \n",
    "                    / self.stats[\"reels_successful\"]\n",
    "                )\n",
    "                self.stats[\"avg_processing_time\"] = (\n",
    "                    (self.stats[\"avg_processing_time\"] * (self.stats[\"reels_successful\"] - 1) + result.processing_time) \n",
    "                    / self.stats[\"reels_successful\"]\n",
    "                )\n",
    "                \n",
    "                if result.topic_class:\n",
    "                    self.stats[\"by_topic_class\"][result.topic_class] += 1\n",
    "            elif result.status == \"rejected\":\n",
    "                self.stats[\"reels_rejected\"] += 1\n",
    "                if result.reason:\n",
    "                    self.stats[\"by_rejection_reason\"][result.reason] += 1\n",
    "            else:\n",
    "                self.stats[\"reels_failed\"] += 1\n",
    "    \n",
    "    def record_enrichment(self, successful: bool):\n",
    "        \"\"\"Record enrichment attempt\"\"\"\n",
    "        with self.lock:\n",
    "            self.stats[\"enrichment_attempts\"] += 1\n",
    "            if successful:\n",
    "                self.stats[\"enrichment_successes\"] += 1\n",
    "    \n",
    "    def record_duplicates(self, count: int):\n",
    "        \"\"\"Record duplicates filtered\"\"\"\n",
    "        with self.lock:\n",
    "            self.stats[\"duplicates_filtered\"] += count\n",
    "    \n",
    "    def record_content_density(self, density: str):\n",
    "        \"\"\"Record content density classification\"\"\"\n",
    "        with self.lock:\n",
    "            self.stats[\"by_content_density\"][density] += 1\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get summary statistics\"\"\"\n",
    "        with self.lock:\n",
    "            summary = self.stats.copy()\n",
    "            summary[\"success_rate\"] = (\n",
    "                self.stats[\"reels_successful\"] / self.stats[\"reels_processed\"] * 100 \n",
    "                if self.stats[\"reels_processed\"] > 0 else 0\n",
    "            )\n",
    "            summary[\"enrichment_success_rate\"] = (\n",
    "                self.stats[\"enrichment_successes\"] / self.stats[\"enrichment_attempts\"] * 100 \n",
    "                if self.stats[\"enrichment_attempts\"] > 0 else 0\n",
    "            )\n",
    "            summary[\"cards_per_reel\"] = (\n",
    "                self.stats[\"cards_generated\"] / self.stats[\"reels_successful\"] \n",
    "                if self.stats[\"reels_successful\"] > 0 else 0\n",
    "            )\n",
    "            summary[\"timestamp\"] = time.time()\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def save(self, filepath: Path):\n",
    "        \"\"\"Save statistics to file\"\"\"\n",
    "        with self.lock:\n",
    "            atomic_write(self.get_summary(), filepath)\n",
    "\n",
    "# Initialize statistics tracker\n",
    "STATISTICS = PipelineStatistics()\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZATION LOGGING\n",
    "# ============================================================\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"DATA TYPES & SCHEMAS INITIALIZED\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"CompletionState: {[s.value for s in CompletionState]}\")\n",
    "logger.info(f\"RejectionType: {[r.value for r in RejectionType]}\")\n",
    "logger.info(f\"ContentDensity: {[d.value for d in ContentDensity]}\")\n",
    "logger.info(f\"PromptStrategy: {[s.value for s in PromptStrategy]}\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade88a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: CONTENT PROCESSING & CLASSIFICATION (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(\"anki-pipeline.content\")\n",
    "\n",
    "# ============================================================\n",
    "# CONTENT INTENT PATTERNS\n",
    "# ============================================================\n",
    "CONTENT_INTENT_PATTERNS = {\n",
    "    ContentIntent.TUTORIAL: [\n",
    "        r\"step\\s+\\d+\", r\"first\\s+you\", r\"then\\s+you\", r\"next\\s+you\",\n",
    "        r\"let.*s\\s+walk\", r\"walk.*through\", r\"follow\\s+along\",\n",
    "        r\"tutorial\", r\"guide\", r\"how\\s+to\", r\"implement\"\n",
    "    ],\n",
    "    ContentIntent.PROBLEM_SOLVING: [\n",
    "        r\"problem\", r\"solution\", r\"solve\", r\"issue\",\n",
    "        r\"challenge\", r\"debug\", r\"fix\", r\"error\",\n",
    "        r\"bug\", r\"exception\", r\"troubleshoot\"\n",
    "    ],\n",
    "    ContentIntent.OVERVIEW: [\n",
    "        r\"overview\", r\"introduction\", r\"what\\s+is\",\n",
    "        r\"basics\\s+of\", r\"fundamentals\", r\"explained\",\n",
    "        r\"understanding\", r\"about\\s+[a-z]+\\s+in\"\n",
    "    ],\n",
    "    ContentIntent.MOTIVATIONAL: [\n",
    "        r\"you\\s+must\", r\"trust\\s+me\", r\"important\\s+to\",\n",
    "        r\"should\\s+know\", r\"every\\s+developer\",\n",
    "        r\"stop\\s+scrolling\", r\"no\\s+one\\s+tells\",\n",
    "        r\"secret\", r\"hidden\", r\"underrated\"\n",
    "    ],\n",
    "    ContentIntent.COMPARISON: [\n",
    "        r\"vs\\.\", r\"versus\", r\"compared\\s+to\",\n",
    "        r\"difference\\s+between\", r\"vs\\s+\",\n",
    "        r\"rather\\s+than\", r\"instead\\s+of\",\n",
    "        r\"alternative\\s+to\", r\"choice\\s+between\"\n",
    "    ],\n",
    "    ContentIntent.WARNING: [\n",
    "        r\"warning\", r\"danger\", r\"avoid\", r\"never\",\n",
    "        r\"don.*t\\s+use\", r\"should.*t\", r\"mistake\",\n",
    "        r\"pitfall\", r\"gotcha\", r\"common\\s+error\"\n",
    "    ],\n",
    "    ContentIntent.BEST_PRACTICE: [\n",
    "        r\"best\\s+practice\", r\"recommended\", r\"should\\s+use\",\n",
    "        r\"proper\\s+way\", r\"correct\\s+approach\",\n",
    "        r\"industry\\s+standard\", r\"production\\s+ready\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# MECHANISM MARKERS FOR DEEP CONTENT DETECTION\n",
    "# ============================================================\n",
    "MECHANISM_MARKERS = [\n",
    "    # Contract/guarantee markers\n",
    "    \"contract\", \"invariant\", \"guarantee\", \"violates\", \"breaks\", \"ensures\",\n",
    "    \"depends on\", \"requires\", \"only if\", \"undefined behavior\", \"happens-before\",\n",
    "    \n",
    "    # Concurrency markers\n",
    "    \"race condition\", \"memory model\", \"visibility\", \"ordering\", \"consistency\",\n",
    "    \"deadlock\", \"livelock\", \"starvation\", \"thread safety\", \"atomicity\",\n",
    "    \n",
    "    # Architecture markers\n",
    "    \"semantics\", \"specification\", \"constraint\", \"precondition\", \"postcondition\",\n",
    "    \"internal\", \"mechanism\", \"how it works\", \"under the hood\", \"implementation\",\n",
    "    \n",
    "    # Performance markers\n",
    "    \"latency\", \"throughput\", \"bottleneck\", \"overhead\", \"scalability\",\n",
    "    \"optimization\", \"efficiency\", \"performance characteristics\",\n",
    "    \n",
    "    # System design markers\n",
    "    \"trade-off\", \"tradeoff\", \"compromise\", \"sacrifice\", \"balance\",\n",
    "    \"pros and cons\", \"advantages disadvantages\", \"strengths weaknesses\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# TOPIC CLASSIFICATION CONFIGURATION\n",
    "# ============================================================\n",
    "TOPIC_CLASSES = {\n",
    "    \"foundation\": {\n",
    "        \"keywords\": [\n",
    "            # Core Java\n",
    "            \"immutability\", \"string\", \"array\", \"oops\", \"collection\",\n",
    "            \"hashmap\", \"equals\", \"hashcode\", \"sorting\", \"basics\",\n",
    "            \"fundamentals\", \"introduction\", \"inheritance\", \"polymorphism\",\n",
    "            \"encapsulation\", \"abstraction\", \"list\", \"set\", \"queue\",\n",
    "            \n",
    "            # Basic concepts\n",
    "            \"variable\", \"method\", \"class\", \"object\", \"constructor\",\n",
    "            \"static\", \"final\", \"interface\", \"abstract class\",\n",
    "            \"exception\", \"try catch\", \"finally\", \"throw\",\n",
    "            \n",
    "            # Common patterns\n",
    "            \"singleton\", \"factory\", \"builder\", \"observer\"\n",
    "        ],\n",
    "        \"threshold_adjustment\": -3,\n",
    "        \"expected_depth\": \"intermediate\",\n",
    "        \"expected_cards\": 2,\n",
    "        \"confidence_boost\": 1.2,\n",
    "        \"min_transcript_words\": 80\n",
    "    },\n",
    "    \"intermediate\": {\n",
    "        \"keywords\": [\n",
    "            # Concurrency\n",
    "            \"concurrency\", \"thread\", \"lock\", \"synchronization\", \"volatile\",\n",
    "            \"atomic\", \"concurrent\", \"deadlock\", \"race condition\",\n",
    "            \"memory model\", \"jvm\", \"garbage collection\", \"classloader\",\n",
    "            \n",
    "            # Advanced Java\n",
    "            \"generics\", \"annotations\", \"reflection\", \"serialization\",\n",
    "            \"stream api\", \"lambda\", \"functional interface\", \"optional\",\n",
    "            \n",
    "            # Spring basics\n",
    "            \"spring\", \"@autowired\", \"@component\", \"@service\", \"@repository\",\n",
    "            \"dependency injection\", \"bean\", \"application context\"\n",
    "        ],\n",
    "        \"threshold_adjustment\": -1,\n",
    "        \"expected_depth\": \"deep\",\n",
    "        \"expected_cards\": 3,\n",
    "        \"confidence_boost\": 1.0,\n",
    "        \"min_transcript_words\": 120\n",
    "    },\n",
    "    \"advanced\": {\n",
    "        \"keywords\": [\n",
    "            # Distributed systems\n",
    "            \"distributed\", \"saga\", \"circuit breaker\", \"event sourcing\",\n",
    "            \"cqrs\", \"microservice\", \"kubernetes\", \"docker\", \"kafka\",\n",
    "            \"consistency model\", \"partition tolerance\", \"cap theorem\",\n",
    "            \n",
    "            # System design\n",
    "            \"load balancing\", \"caching strategy\", \"database sharding\",\n",
    "            \"message queue\", \"api gateway\", \"service mesh\",\n",
    "            \n",
    "            # Cloud/DevOps\n",
    "            \"azure\", \"aws\", \"gcp\", \"terraform\", \"helm\", \"jenkins\",\n",
    "            \"ci/cd\", \"monitoring\", \"observability\"\n",
    "        ],\n",
    "        \"threshold_adjustment\": 0,\n",
    "        \"expected_depth\": \"very_deep\",\n",
    "        \"expected_cards\": 4,\n",
    "        \"confidence_boost\": 0.9,\n",
    "        \"min_transcript_words\": 200\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# TECHNICAL KEYWORDS FOR SCORING (ENHANCED)\n",
    "# ============================================================\n",
    "TECHNICAL_KEYWORDS = {\n",
    "    \"java\": [\n",
    "        \"heap\", \"stack\", \"garbage collection\", \"jvm\", \"bytecode\",\n",
    "        \"classloader\", \"reflection\", \"synchronized\", \"volatile\",\n",
    "        \"thread\", \"immutable\", \"serialization\", \"generics\",\n",
    "        \"annotation\", \"enum\", \"interface\", \"abstract class\",\n",
    "        \"try-with-resources\", \"autocloseable\", \"stream\",\n",
    "        \"optional\", \"lambda\", \"method reference\"\n",
    "    ],\n",
    "    \"spring\": [\n",
    "        \"dependency injection\", \"bean\", \"autowired\", \"aop\",\n",
    "        \"transactional\", \"repository\", \"service\", \"controller\",\n",
    "        \"configuration\", \"component scan\", \"spring boot\",\n",
    "        \"starter\", \"actuator\", \"security\", \"jpa\", \"hibernate\"\n",
    "    ],\n",
    "    \"microservices\": [\n",
    "        \"saga\", \"event sourcing\", \"cqrs\", \"api gateway\",\n",
    "        \"service mesh\", \"circuit breaker\", \"bulkhead\", \"rate limiting\",\n",
    "        \"idempotency\", \"distributed transaction\", \"event bus\",\n",
    "        \"service discovery\", \"config server\"\n",
    "    ],\n",
    "    \"algorithms\": [\n",
    "        \"time complexity\", \"space complexity\", \"big o\", \"recursion\",\n",
    "        \"dynamic programming\", \"backtracking\", \"greedy\", \"divide and conquer\",\n",
    "        \"sorting algorithm\", \"search algorithm\", \"graph algorithm\",\n",
    "        \"tree traversal\", \"hash table\", \"linked list\"\n",
    "    ],\n",
    "    \"system_design\": [\n",
    "        \"scalability\", \"availability\", \"consistency\", \"partition tolerance\",\n",
    "        \"load balancer\", \"caching\", \"sharding\", \"replication\",\n",
    "        \"eventual consistency\", \"cap theorem\", \"acid\", \"base\",\n",
    "        \"vertical scaling\", \"horizontal scaling\"\n",
    "    ],\n",
    "    \"databases\": [\n",
    "        \"index\", \"transaction\", \"isolation level\", \"acid compliance\",\n",
    "        \"normalization\", \"denormalization\", \"join\", \"foreign key\",\n",
    "        \"primary key\", \"nosql\", \"sql\", \"orm\", \"migration\"\n",
    "    ],\n",
    "    \"testing\": [\n",
    "        \"unit test\", \"integration test\", \"mock\", \"stub\", \"spy\",\n",
    "        \"test coverage\", \"tdd\", \"bdd\", \"junit\", \"testng\",\n",
    "        \"mockito\", \"assertj\", \"hamcrest\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Foundation-specific keywords for better scoring\n",
    "FOUNDATION_ENHANCEMENT_KEYWORDS = [\n",
    "    \"string\", \"array\", \"list\", \"map\", \"set\", \"queue\", \"stack\",\n",
    "    \"equals\", \"hashcode\", \"immutable\", \"mutable\", \"inheritance\",\n",
    "    \"polymorphism\", \"encapsulation\", \"abstraction\", \"interface\",\n",
    "    \"abstract class\", \"constructor\", \"static\", \"final\", \"volatile\",\n",
    "    \"transient\", \"synchronized\", \"exception\", \"try\", \"catch\", \"finally\",\n",
    "    \"stream\", \"lambda\", \"functional interface\", \"optional\", \"generic\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# PREPROCESSING UTILITIES\n",
    "# ============================================================\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing utilities with caching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        self._cache_size = 10000\n",
    "    \n",
    "    def _normalize_cache_key(self, text: str) -> str:\n",
    "        \"\"\"Create cache key for text\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into words\"\"\"\n",
    "        cache_key = self._normalize_cache_key(f\"tokenize:{text}\")\n",
    "        if cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        # Convert to lowercase and split\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        \n",
    "        # Cache result\n",
    "        if len(self._cache) >= self._cache_size:\n",
    "            # Remove oldest entry (FIFO)\n",
    "            self._cache.pop(next(iter(self._cache)))\n",
    "        \n",
    "        self._cache[cache_key] = tokens\n",
    "        return tokens\n",
    "    \n",
    "    def remove_fillers(self, text: str) -> str:\n",
    "        \"\"\"Remove conversational fillers\"\"\"\n",
    "        fillers = [\n",
    "            r'\\bso\\b', r'\\bwell\\b', r'\\bright\\b', r'\\bokay\\b',\n",
    "            r'\\byou know\\b', r'\\bi mean\\b', r'\\blike\\b',\n",
    "            r'\\bactually\\b', r'\\bbasically\\b', r'\\bliterally\\b',\n",
    "            r'\\bjust\\b', r'\\breally\\b', r'\\bvery\\b', r'\\bquite\\b'\n",
    "        ]\n",
    "        \n",
    "        for filler in fillers:\n",
    "            text = re.sub(filler, ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def extract_code_snippets(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract code snippets from text\"\"\"\n",
    "        # Look for code blocks\n",
    "        code_blocks = re.findall(r'```(?:[a-z]+)?\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        \n",
    "        # Look for inline code\n",
    "        inline_code = re.findall(r'`([^`]+)`', text)\n",
    "        \n",
    "        return code_blocks + inline_code\n",
    "    \n",
    "    def has_code_example(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains code examples\"\"\"\n",
    "        code_snippets = self.extract_code_snippets(text)\n",
    "        return len(code_snippets) > 0\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# ============================================================\n",
    "# CONTENT INTENT CLASSIFICATION\n",
    "# ============================================================\n",
    "def detect_content_intent(caption: str, transcript: str) -> List[Tuple[ContentIntent, float]]:\n",
    "    \"\"\"\n",
    "    Detect content intent with confidence scores.\n",
    "    Returns list of (intent, confidence) sorted by confidence.\n",
    "    \"\"\"\n",
    "    combined_text = f\"{caption} {transcript}\".lower()\n",
    "    tokens = set(preprocessor.tokenize(combined_text))\n",
    "    \n",
    "    intent_scores = []\n",
    "    \n",
    "    for intent, patterns in CONTENT_INTENT_PATTERNS.items():\n",
    "        score = 0.0\n",
    "        \n",
    "        # Pattern matching\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, combined_text, re.IGNORECASE):\n",
    "                score += 0.3\n",
    "        \n",
    "        # Exact keyword matching (stronger signal)\n",
    "        intent_keywords = {\n",
    "            ContentIntent.TUTORIAL: [\"tutorial\", \"guide\", \"walkthrough\", \"step-by-step\"],\n",
    "            ContentIntent.PROBLEM_SOLVING: [\"problem\", \"solution\", \"solve\", \"fix\"],\n",
    "            ContentIntent.OVERVIEW: [\"overview\", \"introduction\", \"basics\", \"fundamentals\"],\n",
    "            ContentIntent.MOTIVATIONAL: [\"must know\", \"important\", \"secret\", \"hidden\"],\n",
    "            ContentIntent.COMPARISON: [\"vs\", \"versus\", \"compared\", \"difference\"],\n",
    "            ContentIntent.WARNING: [\"warning\", \"avoid\", \"never\", \"danger\"],\n",
    "            ContentIntent.BEST_PRACTICE: [\"best practice\", \"recommended\", \"should use\"]\n",
    "        }\n",
    "        \n",
    "        if intent in intent_keywords:\n",
    "            for keyword in intent_keywords[intent]:\n",
    "                if keyword in combined_text:\n",
    "                    score += 0.5\n",
    "        \n",
    "        # Length-based adjustments\n",
    "        word_count = len(transcript.split())\n",
    "        if intent == ContentIntent.TUTORIAL and word_count > 200:\n",
    "            score += 0.2\n",
    "        elif intent == ContentIntent.OVERVIEW and word_count < 150:\n",
    "            score += 0.2\n",
    "        \n",
    "        if score > 0:\n",
    "            # Normalize to 0-1 range\n",
    "            score = min(score, 1.0)\n",
    "            intent_scores.append((intent, score))\n",
    "    \n",
    "    # Sort by confidence\n",
    "    intent_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # If no strong intent detected, default to OVERVIEW\n",
    "    if not intent_scores or intent_scores[0][1] < 0.3:\n",
    "        intent_scores.append((ContentIntent.OVERVIEW, 0.2))\n",
    "    \n",
    "    return intent_scores\n",
    "\n",
    "# ============================================================\n",
    "# TOPIC CLASSIFICATION\n",
    "# ============================================================\n",
    "def classify_topic(caption: str, category: str, transcript: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Classify topic depth based on keywords and context.\n",
    "    Returns: 'foundation', 'intermediate', or 'advanced'\n",
    "    \"\"\"\n",
    "    text = f\"{caption} {transcript}\".lower()\n",
    "    tokens = set(preprocessor.tokenize(text))\n",
    "    \n",
    "    # Check each class in reverse order (advanced first)\n",
    "    for topic_class, config in reversed(list(TOPIC_CLASSES.items())):\n",
    "        keywords = config[\"keywords\"]\n",
    "        \n",
    "        # Count keyword matches\n",
    "        matches = sum(1 for kw in keywords if kw in text)\n",
    "        \n",
    "        # For foundation topics, require stronger signal\n",
    "        if topic_class == \"foundation\":\n",
    "            if matches >= 2:\n",
    "                return topic_class\n",
    "        elif matches >= 1:\n",
    "            return topic_class\n",
    "    \n",
    "    # Enhanced: Check for decision/comparison/tradeoff signals\n",
    "    tradeoff_signals = [\"vs\", \"versus\", \"instead\", \"trade-off\", \"tradeoff\",\n",
    "                       \"compare\", \"comparison\", \"difference between\", \"when to\", \"choose\"]\n",
    "    decision_signals = [\"decide\", \"decision\", \"which\", \"what if\", \"scenario\",\n",
    "                       \"use case\", \"depends on\", \"context\", \"situation\"]\n",
    "    \n",
    "    has_tradeoff = any(signal in text for signal in tradeoff_signals)\n",
    "    has_decision = any(signal in text for signal in decision_signals)\n",
    "    \n",
    "    # Count technical depth markers\n",
    "    depth_markers = [\"internal\", \"mechanism\", \"how it works\", \"under the hood\",\n",
    "                    \"implementation\", \"architecture\", \"design pattern\", \"best practice\"]\n",
    "    depth_count = sum(1 for marker in depth_markers if marker in text)\n",
    "    \n",
    "    # Don't default to foundation - use multiple signals\n",
    "    transcript_len = len(transcript.strip())\n",
    "    \n",
    "    # Advanced signals: long transcript + tradeoffs/decisions + depth\n",
    "    if transcript_len > 400 and (has_tradeoff or depth_count >= 2):\n",
    "        return \"advanced\"\n",
    "    \n",
    "    # Intermediate signals: medium length + decisions OR good depth\n",
    "    if transcript_len > 200 and (has_decision or has_tradeoff or depth_count >= 1):\n",
    "        return \"intermediate\"\n",
    "    \n",
    "    # Long transcripts without keywords default to intermediate (not foundation)\n",
    "    if transcript_len > 500:\n",
    "        return \"intermediate\"\n",
    "    elif transcript_len > 250:\n",
    "        return \"intermediate\"\n",
    "    else:\n",
    "        return \"foundation\"\n",
    "\n",
    "def classify_topic_with_confidence(caption: str, category: str, transcript: str = \"\") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Classify topic with confidence score.\n",
    "    \"\"\"\n",
    "    topic_class = classify_topic(caption, category, transcript)\n",
    "    \n",
    "    # Calculate confidence based on signal strength\n",
    "    text = f\"{caption} {transcript}\".lower()\n",
    "    config = TOPIC_CLASSES.get(topic_class, {})\n",
    "    keywords = config.get(\"keywords\", [])\n",
    "    \n",
    "    # Count keyword matches\n",
    "    matches = sum(1 for kw in keywords if kw in text)\n",
    "    \n",
    "    # Confidence calculation\n",
    "    if topic_class == \"advanced\":\n",
    "        base_confidence = 0.7 if matches >= 2 else 0.5\n",
    "    elif topic_class == \"intermediate\":\n",
    "        base_confidence = 0.8 if matches >= 2 else 0.6\n",
    "    else:  # foundation\n",
    "        base_confidence = 0.9 if matches >= 2 else 0.7\n",
    "    \n",
    "    # Adjust based on transcript length\n",
    "    word_count = len(transcript.split())\n",
    "    expected_min = config.get(\"min_transcript_words\", 100)\n",
    "    \n",
    "    if word_count >= expected_min * 1.5:\n",
    "        base_confidence = min(base_confidence + 0.1, 0.95)\n",
    "    elif word_count < expected_min * 0.5:\n",
    "        base_confidence = max(base_confidence - 0.2, 0.3)\n",
    "    \n",
    "    return topic_class, round(base_confidence, 2)\n",
    "\n",
    "# ============================================================\n",
    "# CONTENT DENSITY CLASSIFICATION\n",
    "# ============================================================\n",
    "def classify_content_density(caption: str, transcript: str, category: str = \"\") -> ContentDensity:\n",
    "    \"\"\"\n",
    "    PHASE 3: Classify content density for multi-track routing.\n",
    "    \n",
    "    Returns:\n",
    "    - DENSE: Tutorial/problem-solving with concrete examples â†’ Full pipeline\n",
    "    - LIGHT: Foundation/motivational content â†’ Reference-only cards  \n",
    "    - SKIP: Pure CTA/promotional â†’ Don't process\n",
    "    \"\"\"\n",
    "    text = f\"{caption} {transcript}\".lower()\n",
    "    word_count = len(transcript.split())\n",
    "    \n",
    "    # Dense indicators (tutorial/problem-solving content)\n",
    "    dense_signals = {\n",
    "        \"code_present\": preprocessor.has_code_example(text) or any(marker in text for marker in [\n",
    "            \"code\", \"function\", \"method\", \"class\", \"variable\", \n",
    "            \"algorithm\", \"implementation\", \"syntax\", \"example\"\n",
    "        ]),\n",
    "        \"problem_solving\": any(marker in text for marker in [\n",
    "            \"question\", \"problem\", \"solve\", \"solution\", \"approach\",\n",
    "            \"step 1\", \"step 2\", \"algorithm\", \"complexity\"\n",
    "        ]),\n",
    "        \"concrete_example\": any(marker in text for marker in [\n",
    "            \"for example\", \"let's say\", \"consider\", \"here is\",\n",
    "            \"output\", \"result\", \"returns\"\n",
    "        ]),\n",
    "        \"mechanism\": any(marker in text for marker in [\n",
    "            \"how it works\", \"internally\", \"mechanism\", \"under the hood\",\n",
    "            \"behind the scenes\", \"what happens\"\n",
    "        ]),\n",
    "        \"structured\": \"step\" in text and word_count > 150,\n",
    "        \"long_form\": word_count > 300,\n",
    "        \"has_numbers\": bool(re.search(r'\\b\\d+\\b', text))  # Contains numbers\n",
    "    }\n",
    "    \n",
    "    # Light indicators (overview/motivational content)\n",
    "    light_signals = {\n",
    "        \"overview\": any(marker in text for marker in [\n",
    "            \"introduction\", \"overview\", \"basics\", \"fundamentals\",\n",
    "            \"what is\", \"definition\", \"concept\"\n",
    "        ]),\n",
    "        \"motivational\": any(marker in text for marker in [\n",
    "            \"you must know\", \"important to\", \"should learn\",\n",
    "            \"every developer\", \"trust me\", \"secret\"\n",
    "        ]),\n",
    "        \"list_based\": any(marker in text for marker in [\n",
    "            \"top 10\", \"top 5\", \"things to\", \"tips\", \"must know\",\n",
    "            \"reasons why\", \"benefits of\"\n",
    "        ]),\n",
    "        \"short_form\": word_count < 150,\n",
    "        \"cta_present\": any(marker in text for marker in [\n",
    "            \"comment link\", \"link in bio\", \"check out\", \"dm for\",\n",
    "            \"go ahead\", \"share with\", \"follow me\"\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Calculate density score\n",
    "    dense_score = sum(1 for v in dense_signals.values() if v)\n",
    "    light_score = sum(1 for v in light_signals.values() if v)\n",
    "    \n",
    "    # Track metrics\n",
    "    STATISTICS.record_content_density(\"analyzed\")\n",
    "    \n",
    "    # Decision logic with thresholds\n",
    "    if light_signals[\"cta_present\"] and word_count < 100:\n",
    "        logger.debug(f\"Content density: SKIP (promotional CTA)\")\n",
    "        STATISTICS.record_content_density(\"skip\")\n",
    "        return ContentDensity.SKIP\n",
    "    \n",
    "    if dense_score >= 4:\n",
    "        logger.debug(f\"Content density: DENSE (score: {dense_score})\")\n",
    "        STATISTICS.record_content_density(\"dense\")\n",
    "        return ContentDensity.DENSE\n",
    "    elif dense_score >= 3 and light_score <= 2:\n",
    "        logger.debug(f\"Content density: DENSE (score: {dense_score}, light: {light_score})\")\n",
    "        STATISTICS.record_content_density(\"dense\")\n",
    "        return ContentDensity.DENSE\n",
    "    elif light_score >= 3 or word_count < CONFIG.get(\"LIGHT_CONTENT_MAX_WORDS\", 100):\n",
    "        logger.debug(f\"Content density: LIGHT (score: {light_score}, words: {word_count})\")\n",
    "        STATISTICS.record_content_density(\"light\")\n",
    "        return ContentDensity.LIGHT\n",
    "    elif dense_score >= 2:\n",
    "        # Borderline case - check intent\n",
    "        intent_scores = detect_content_intent(caption, transcript)\n",
    "        if intent_scores and intent_scores[0][0] in [ContentIntent.TUTORIAL, ContentIntent.PROBLEM_SOLVING]:\n",
    "            logger.debug(f\"Content density: DENSE (intent: {intent_scores[0][0].value})\")\n",
    "            STATISTICS.record_content_density(\"dense\")\n",
    "            return ContentDensity.DENSE\n",
    "    \n",
    "    # Default: if unclear, treat as LIGHT (safer)\n",
    "    logger.debug(f\"Content density: LIGHT (default)\")\n",
    "    STATISTICS.record_content_density(\"light\")\n",
    "    return ContentDensity.LIGHT\n",
    "\n",
    "def classify_content_density_with_score(caption: str, transcript: str, category: str = \"\") -> Tuple[ContentDensity, Dict]:\n",
    "    \"\"\"\n",
    "    Classify content density with detailed scoring.\n",
    "    \"\"\"\n",
    "    text = f\"{caption} {transcript}\".lower()\n",
    "    word_count = len(transcript.split())\n",
    "    \n",
    "    # Calculate signals\n",
    "    dense_signals = {\n",
    "        \"code_present\": preprocessor.has_code_example(text),\n",
    "        \"problem_solving\": any(marker in text for marker in [\"problem\", \"solution\", \"solve\"]),\n",
    "        \"concrete_example\": any(marker in text for marker in [\"for example\", \"example\"]),\n",
    "        \"mechanism\": any(marker in text for marker in [\"how it works\", \"internally\"]),\n",
    "        \"structured\": \"step\" in text and word_count > 150,\n",
    "        \"has_numbers\": bool(re.search(r'\\b\\d+\\b', text))\n",
    "    }\n",
    "    \n",
    "    light_signals = {\n",
    "        \"overview\": any(marker in text for marker in [\"overview\", \"introduction\"]),\n",
    "        \"motivational\": any(marker in text for marker in [\"must know\", \"important\"]),\n",
    "        \"list_based\": any(marker in text for marker in [\"top 10\", \"tips\"]),\n",
    "        \"short_form\": word_count < 150,\n",
    "        \"cta_present\": any(marker in text for marker in [\"link in bio\", \"dm for\"])\n",
    "    }\n",
    "    \n",
    "    dense_score = sum(dense_signals.values())\n",
    "    light_score = sum(light_signals.values())\n",
    "    \n",
    "    # Classify\n",
    "    density = classify_content_density(caption, transcript, category)\n",
    "    \n",
    "    # Calculate confidence\n",
    "    total_signals = dense_score + light_score\n",
    "    if total_signals == 0:\n",
    "        confidence = 0.5  # Default confidence\n",
    "    else:\n",
    "        if density == ContentDensity.DENSE:\n",
    "            confidence = dense_score / max(total_signals, 1)\n",
    "        elif density == ContentDensity.LIGHT:\n",
    "            confidence = light_score / max(total_signals, 1)\n",
    "        else:\n",
    "            confidence = 0.8  # High confidence for SKIP\n",
    "    \n",
    "    return density, {\n",
    "        \"dense_score\": dense_score,\n",
    "        \"light_score\": light_score,\n",
    "        \"confidence\": round(confidence, 2),\n",
    "        \"word_count\": word_count,\n",
    "        \"dense_signals\": {k: v for k, v in dense_signals.items() if v},\n",
    "        \"light_signals\": {k: v for k, v in light_signals.items() if v}\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# TEXT NORMALIZATION\n",
    "# ============================================================\n",
    "def normalize_learning_key(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Robust caption normalization to prevent memory fragmentation.\n",
    "    \n",
    "    Without this, memory fragments across:\n",
    "    - punctuation variants (\"HashMap!\" vs \"HashMap\")\n",
    "    - emoji presence\n",
    "    - trailing hashtags\n",
    "    - \"Part 1 / Part 2\" suffixes\n",
    "    \n",
    "    This ensures learning convergence.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'#\\w+', '', text)              # Remove hashtags\n",
    "    text = re.sub(r'\\bpart\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Remove \"Part 1\", \"Part 2\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)          # Remove punctuation, keep words and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()      # Normalize whitespace\n",
    "    \n",
    "    # Remove common filler words\n",
    "    filler_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in filler_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def create_concept_signature(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a unique signature for a concept for deduplication.\n",
    "    More aggressive than normalize_learning_key.\n",
    "    \"\"\"\n",
    "    normalized = normalize_learning_key(text)\n",
    "    \n",
    "    # Remove common words\n",
    "    common_words = {\n",
    "        'what', 'why', 'how', 'when', 'where', 'which',\n",
    "        'java', 'spring', 'microservice', 'system', 'design',\n",
    "        'interview', 'question', 'answer', 'explain', 'difference'\n",
    "    }\n",
    "    \n",
    "    words = normalized.split()\n",
    "    signature_words = [w for w in words if w not in common_words and len(w) > 2]\n",
    "    \n",
    "    # Sort words to be order-independent\n",
    "    signature_words.sort()\n",
    "    \n",
    "    # Take first 5 words or all if less\n",
    "    signature_words = signature_words[:5]\n",
    "    \n",
    "    return '_'.join(signature_words) if signature_words else 'empty'\n",
    "\n",
    "# ============================================================\n",
    "# TECHNICAL SCORING\n",
    "# ============================================================\n",
    "@lru_cache(maxsize=10000)\n",
    "def technical_score(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate technical density score (0-10).\n",
    "    Cached with LRU for 50k+ reel nightly processing.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        text = json.dumps(text, ensure_ascii=False)\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    score = 0\n",
    "    \n",
    "    # Keyword matching (base score)\n",
    "    for category, keywords in TECHNICAL_KEYWORDS.items():\n",
    "        matches = sum(1 for kw in keywords if kw in text_lower)\n",
    "        score += min(matches, 3)  # Cap per category\n",
    "    \n",
    "    # Enhanced: Foundation-specific keyword bonus\n",
    "    foundation_matches = sum(1 for kw in FOUNDATION_ENHANCEMENT_KEYWORDS if kw in text_lower)\n",
    "    score += min(foundation_matches, 4)  # Up to +4 for foundation content\n",
    "    \n",
    "    # Mechanism marker bonus\n",
    "    mechanism_count = sum(1 for marker in MECHANISM_MARKERS if marker in text_lower)\n",
    "    if mechanism_count > 0:\n",
    "        score += min(mechanism_count * 2, 4)  # Up to +4 for deep content\n",
    "    \n",
    "    # Code presence bonus\n",
    "    if preprocessor.has_code_example(text):\n",
    "        score += 3\n",
    "    \n",
    "    # Length normalization\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 50:\n",
    "        score = int(score * 0.7)\n",
    "    elif word_count > 200:\n",
    "        score = int(score * 1.2)\n",
    "    \n",
    "    # Cap score\n",
    "    return min(max(score, 1), 10)\n",
    "\n",
    "def technical_score_detailed(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate technical score with breakdown.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\"total\": 0, \"breakdown\": {}}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    breakdown = {}\n",
    "    \n",
    "    # Keyword matching\n",
    "    for category, keywords in TECHNICAL_KEYWORDS.items():\n",
    "        matches = sum(1 for kw in keywords if kw in text_lower)\n",
    "        if matches > 0:\n",
    "            breakdown[category] = min(matches, 3)\n",
    "    \n",
    "    # Foundation keywords\n",
    "    foundation_matches = sum(1 for kw in FOUNDATION_ENHANCEMENT_KEYWORDS if kw in text_lower)\n",
    "    if foundation_matches > 0:\n",
    "        breakdown[\"foundation\"] = min(foundation_matches, 4)\n",
    "    \n",
    "    # Mechanism markers\n",
    "    mechanism_count = sum(1 for marker in MECHANISM_MARKERS if marker in text_lower)\n",
    "    if mechanism_count > 0:\n",
    "        breakdown[\"mechanism\"] = min(mechanism_count * 2, 4)\n",
    "    \n",
    "    # Code presence\n",
    "    if preprocessor.has_code_example(text):\n",
    "        breakdown[\"code\"] = 3\n",
    "    \n",
    "    # Calculate total\n",
    "    total = sum(breakdown.values())\n",
    "    \n",
    "    # Length adjustment\n",
    "    word_count = len(text.split())\n",
    "    length_factor = 1.0\n",
    "    if word_count < 50:\n",
    "        length_factor = 0.7\n",
    "    elif word_count > 200:\n",
    "        length_factor = 1.2\n",
    "    \n",
    "    total = int(total * length_factor)\n",
    "    total = min(max(total, 1), 10)\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"breakdown\": breakdown,\n",
    "        \"word_count\": word_count,\n",
    "        \"length_factor\": length_factor\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# CONTENT QUALITY ASSESSMENT\n",
    "# ============================================================\n",
    "def assess_content_quality(caption: str, transcript: str, category: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive content quality assessment.\n",
    "    \"\"\"\n",
    "    # Classify everything\n",
    "    topic_class, topic_confidence = classify_topic_with_confidence(caption, category, transcript)\n",
    "    density, density_details = classify_content_density_with_score(caption, transcript, category)\n",
    "    intent_scores = detect_content_intent(caption, transcript)\n",
    "    tech_score_details = technical_score_detailed(f\"{caption} {transcript}\")\n",
    "    \n",
    "    # Calculate overall quality score (0-100)\n",
    "    quality_score = 0\n",
    "    \n",
    "    # Technical depth contribution (40%)\n",
    "    quality_score += (tech_score_details[\"total\"] / 10) * 40\n",
    "    \n",
    "    # Content density contribution (30%)\n",
    "    if density == ContentDensity.DENSE:\n",
    "        quality_score += 30\n",
    "    elif density == ContentDensity.LIGHT:\n",
    "        quality_score += 15\n",
    "    # SKIP gets 0\n",
    "    \n",
    "    # Intent contribution (20%)\n",
    "    if intent_scores:\n",
    "        primary_intent, intent_confidence = intent_scores[0]\n",
    "        if primary_intent in [ContentIntent.TUTORIAL, ContentIntent.PROBLEM_SOLVING]:\n",
    "            quality_score += 20 * intent_confidence\n",
    "        elif primary_intent in [ContentIntent.BEST_PRACTICE, ContentIntent.COMPARISON]:\n",
    "            quality_score += 15 * intent_confidence\n",
    "        else:\n",
    "            quality_score += 10 * intent_confidence\n",
    "    \n",
    "    # Transcript length contribution (10%)\n",
    "    word_count = len(transcript.split())\n",
    "    if word_count > 300:\n",
    "        quality_score += 10\n",
    "    elif word_count > 150:\n",
    "        quality_score += 7\n",
    "    elif word_count > 80:\n",
    "        quality_score += 5\n",
    "    else:\n",
    "        quality_score += 2\n",
    "    \n",
    "    # Cap at 100\n",
    "    quality_score = min(quality_score, 100)\n",
    "    \n",
    "    return {\n",
    "        \"quality_score\": round(quality_score, 1),\n",
    "        \"topic_class\": topic_class,\n",
    "        \"topic_confidence\": topic_confidence,\n",
    "        \"content_density\": density.value,\n",
    "        \"density_details\": density_details,\n",
    "        \"primary_intent\": intent_scores[0][0].value if intent_scores else \"unknown\",\n",
    "        \"intent_confidence\": intent_scores[0][1] if intent_scores else 0.0,\n",
    "        \"technical_score\": tech_score_details[\"total\"],\n",
    "        \"technical_breakdown\": tech_score_details[\"breakdown\"],\n",
    "        \"transcript_word_count\": word_count,\n",
    "        \"has_code\": preprocessor.has_code_example(transcript),\n",
    "        \"assessment_timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# CONTENT FILTERING\n",
    "# ============================================================\n",
    "def should_process_reel(reel: Dict) -> Tuple[bool, str, Dict]:\n",
    "    \"\"\"\n",
    "    PHASE 1: Pre-filter to identify reels that are worth processing.\n",
    "    Returns (should_process, reason, quality_assessment)\n",
    "    \"\"\"\n",
    "    transcript = reel.get(\"transcript\", \"\").strip()\n",
    "    caption = reel.get(\"caption\", \"\").strip()\n",
    "    category_confidence = reel.get(\"category_confidence\", 0)\n",
    "    \n",
    "    # Convert category_confidence to int\n",
    "    try:\n",
    "        category_confidence = int(category_confidence) if category_confidence else 0\n",
    "    except:\n",
    "        category_confidence = 0\n",
    "    \n",
    "    # Quick quality assessment\n",
    "    quality = assess_content_quality(caption, transcript, reel.get(\"category\", \"\"))\n",
    "    \n",
    "    # Filter 1: Content density SKIP\n",
    "    if quality[\"content_density\"] == \"skip\":\n",
    "        return False, RejectionReason.PROMOTIONAL_CTA.value, quality\n",
    "    \n",
    "    # Filter 2: Too short\n",
    "    if quality[\"transcript_word_count\"] < CONFIG.get(\"MIN_TRANSCRIPT_LENGTH\", 80):\n",
    "        return False, RejectionReason.TOO_SHORT.value, quality\n",
    "    \n",
    "    # Filter 3: Low category confidence\n",
    "    if category_confidence < CONFIG.get(\"MIN_CATEGORY_CONFIDENCE\", 70):\n",
    "        return False, RejectionReason.LOW_CONFIDENCE_CATEGORY.value, quality\n",
    "    \n",
    "    # Filter 4: Poor quality score\n",
    "    if quality[\"quality_score\"] < 30:\n",
    "        return False, RejectionReason.LOW_TECHNICAL_SIGNAL.value, quality\n",
    "    \n",
    "    # Filter 5: Pure motivational with no technical content\n",
    "    if (quality[\"primary_intent\"] == \"motivational\" and \n",
    "        quality[\"technical_score\"] < 3 and\n",
    "        not quality[\"has_code\"]):\n",
    "        return False, RejectionReason.MOTIVATIONAL_CONTENT.value, quality\n",
    "    \n",
    "    # All checks passed\n",
    "    return True, \"\", quality\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZATION LOGGING\n",
    "# ============================================================\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"CONTENT PROCESSING & CLASSIFICATION INITIALIZED\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Technical keyword categories: {list(TECHNICAL_KEYWORDS.keys())}\")\n",
    "logger.info(f\"Mechanism markers: {len(MECHANISM_MARKERS)} markers\")\n",
    "logger.info(f\"Topic classes: {list(TOPIC_CLASSES.keys())}\")\n",
    "logger.info(f\"Content intents: {[i.value for i in ContentIntent]}\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c1d04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: MEMORY & LEARNING SYSTEMS (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Optional, Tuple, Any\n",
    "from threading import Lock\n",
    "import re\n",
    "\n",
    "logger = logging.getLogger(\"anki-pipeline.memory\")\n",
    "\n",
    "# ============================================================\n",
    "# BLOOM FILTER ENHANCEMENTS\n",
    "# ============================================================\n",
    "class EnhancedBloomFilter:\n",
    "    \"\"\"\n",
    "    Enhanced bloom filter with statistics and auto-rebuild.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_elements: int = 2_000_000, error_rate: float = 0.001):\n",
    "        if not BLOOM_AVAILABLE:\n",
    "            self.available = False\n",
    "            self.filter = None\n",
    "            logger.warning(\"Bloom filter not available - using fallback\")\n",
    "            return\n",
    "        \n",
    "        self.available = True\n",
    "        self.max_elements = max_elements\n",
    "        self.error_rate = error_rate\n",
    "        self.filter = BloomFilter(max_elements=max_elements, error_rate=error_rate)\n",
    "        \n",
    "        # Statistics\n",
    "        self.checks = 0\n",
    "        self.hits = 0\n",
    "        self.false_positives_suspected = 0\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Capacity tracking\n",
    "        self.estimated_count = 0\n",
    "        self.last_rebuild = time.time()\n",
    "        \n",
    "        logger.info(f\"Bloom filter initialized: max={max_elements:,}, error_rate={error_rate}\")\n",
    "    \n",
    "    def add(self, item: str):\n",
    "        \"\"\"Add item to bloom filter\"\"\"\n",
    "        if not self.available or not self.filter:\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            self.filter.add(item)\n",
    "            self.estimated_count += 1\n",
    "    \n",
    "    def __contains__(self, item: str) -> bool:\n",
    "        \"\"\"Check if item might be in bloom filter\"\"\"\n",
    "        if not self.available or not self.filter:\n",
    "            return False\n",
    "        \n",
    "        with self.lock:\n",
    "            self.checks += 1\n",
    "            result = item in self.filter\n",
    "            if result:\n",
    "                self.hits += 1\n",
    "            return result\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get bloom filter statistics\"\"\"\n",
    "        if not self.available:\n",
    "            return {\n",
    "                \"available\": False,\n",
    "                \"estimated_count\": 0,\n",
    "                \"checks\": 0,\n",
    "                \"hit_rate\": 0.0,\n",
    "                \"false_positive_rate\": 0.0,\n",
    "                \"occupancy\": 0.0,\n",
    "                \"needs_rebuild\": False\n",
    "            }\n",
    "        \n",
    "        with self.lock:\n",
    "            hit_rate = self.hits / self.checks if self.checks > 0 else 0.0\n",
    "            fp_rate = self.false_positives_suspected / self.checks if self.checks > 0 else 0.0\n",
    "            occupancy = self.estimated_count / self.max_elements\n",
    "            \n",
    "            needs_rebuild = (\n",
    "                occupancy > 0.8 or  # High occupancy\n",
    "                fp_rate > 0.01 or   # High false positive rate\n",
    "                (time.time() - self.last_rebuild > 30 * 24 * 3600)  # 30 days old\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"available\": True,\n",
    "                \"estimated_count\": self.estimated_count,\n",
    "                \"max_elements\": self.max_elements,\n",
    "                \"checks\": self.checks,\n",
    "                \"hits\": self.hits,\n",
    "                \"hit_rate\": round(hit_rate, 4),\n",
    "                \"false_positive_rate\": round(fp_rate, 4),\n",
    "                \"occupancy\": round(occupancy, 4),\n",
    "                \"needs_rebuild\": needs_rebuild,\n",
    "                \"days_since_rebuild\": round((time.time() - self.last_rebuild) / (24 * 3600), 1)\n",
    "            }\n",
    "    \n",
    "    def record_false_positive(self):\n",
    "        \"\"\"Record suspected false positive\"\"\"\n",
    "        if not self.available:\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            self.false_positives_suspected += 1\n",
    "    \n",
    "    def rebuild_if_needed(self) -> bool:\n",
    "        \"\"\"Rebuild bloom filter if needed, returns True if rebuilt\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        if not stats[\"needs_rebuild\"]:\n",
    "            return False\n",
    "        \n",
    "        logger.warning(f\"Bloom filter needs rebuild: occupancy={stats['occupancy']:.1%}, \"\n",
    "                      f\"FP rate={stats['false_positive_rate']:.3%}\")\n",
    "        \n",
    "        # In production, we would create a new filter and migrate\n",
    "        # For now, just reset statistics\n",
    "        with self.lock:\n",
    "            self.estimated_count = int(self.estimated_count * 0.9)  # Estimate\n",
    "            self.last_rebuild = time.time()\n",
    "            self.false_positives_suspected = 0\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ============================================================\n",
    "# HYBRID DUPLICATE DETECTOR (COMPLETE)\n",
    "# ============================================================\n",
    "class HybridDuplicateDetector:\n",
    "    \"\"\"\n",
    "    Hybrid duplicate detection:\n",
    "    - Bloom filter for O(1) probabilistic check\n",
    "    - Exact set for final verification\n",
    "    - Batch persistence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fingerprints_file: Path, bloom_file: Path):\n",
    "        self.fingerprints_file = fingerprints_file\n",
    "        self.bloom_file = bloom_file\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Load fingerprints\n",
    "        self.fingerprints: Set[str] = self._load_fingerprints()\n",
    "        \n",
    "        # Initialize enhanced bloom filter\n",
    "        self.bloom = EnhancedBloomFilter()\n",
    "        self._load_bloom()\n",
    "        \n",
    "        # Batch persistence\n",
    "        self._dirty = False\n",
    "        self._dirty_count = 0\n",
    "        self._last_save = time.time()\n",
    "        \n",
    "        # Statistics\n",
    "        self.duplicates_filtered = 0\n",
    "        self.total_checks = 0\n",
    "        \n",
    "        logger.info(f\"Duplicate detector initialized: {len(self.fingerprints):,} fingerprints loaded\")\n",
    "    \n",
    "    def _load_fingerprints(self) -> Set[str]:\n",
    "        \"\"\"Load fingerprints from JSON file\"\"\"\n",
    "        if not self.fingerprints_file.exists():\n",
    "            return set()\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(self.fingerprints_file, default=[])\n",
    "            if isinstance(data, list):\n",
    "                return set(data)\n",
    "            elif isinstance(data, dict) and \"fingerprints\" in data:\n",
    "                return set(data[\"fingerprints\"])\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected fingerprint format in {self.fingerprints_file}\")\n",
    "                return set()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading fingerprints: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def _load_bloom(self):\n",
    "        \"\"\"Load bloom filter from disk\"\"\"\n",
    "        if not self.bloom.available or not self.bloom_file.exists():\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(self.bloom_file, \"rb\") as f:\n",
    "                loaded = pickle.load(f)\n",
    "                if isinstance(loaded, BloomFilter):\n",
    "                    self.bloom.filter = loaded\n",
    "                    # Estimate count (BloomFilter doesn't track exact count)\n",
    "                    self.bloom.estimated_count = len(self.fingerprints)\n",
    "                    logger.info(f\"Bloom filter loaded from disk\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading bloom filter: {e}\")\n",
    "    \n",
    "    def _normalize_text(self, text: Any) -> str:\n",
    "        \"\"\"Normalize text for fingerprinting\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            try:\n",
    "                text = json.dumps(text, ensure_ascii=False)\n",
    "            except:\n",
    "                text = str(text)\n",
    "        \n",
    "        # Remove punctuation and normalize\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def _create_fingerprint(self, card: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Create deterministic fingerprint for a card.\n",
    "        Consistent across different representations of same content.\n",
    "        \"\"\"\n",
    "        card_type = card.get(\"type\", \"basic\")\n",
    "        \n",
    "        if card_type == \"basic\":\n",
    "            front = self._normalize_text(card.get(\"front\", \"\"))\n",
    "            back = self._normalize_text(card.get(\"back\", \"\"))\n",
    "            \n",
    "            # Extract key phrases (first 100 chars of each)\n",
    "            front_key = ' '.join(front.split()[:20])  # First 20 words\n",
    "            back_key = ' '.join(back.split()[:50])    # First 50 words\n",
    "            \n",
    "            content = f\"basic:{front_key}:{back_key}\"\n",
    "            \n",
    "        elif card_type == \"cloze\":\n",
    "            cloze = card.get(\"cloze\", \"\")\n",
    "            # Remove cloze markers for content\n",
    "            clean = re.sub(r'\\{\\{c\\d+::(.*?)\\}\\}', r'\\1', cloze)\n",
    "            normalized = self._normalize_text(clean)\n",
    "            \n",
    "            # Take key part\n",
    "            content_key = ' '.join(normalized.split()[:30])\n",
    "            content = f\"cloze:{content_key}\"\n",
    "            \n",
    "        elif card_type == \"tradeoff\":\n",
    "            front = self._normalize_text(card.get(\"front\", \"\"))\n",
    "            tradeoffs = card.get(\"tradeoffs\", [])\n",
    "            \n",
    "            # Extract approach names\n",
    "            approaches = []\n",
    "            for t in tradeoffs[:3]:  # First 3 approaches\n",
    "                approach = self._normalize_text(t.get(\"approach\", \"\"))\n",
    "                if approach:\n",
    "                    approaches.append(approach[:50])  # First 50 chars\n",
    "            \n",
    "            content = f\"tradeoff:{front}:{':'.join(approaches)}\"\n",
    "            \n",
    "        else:\n",
    "            content = f\"unknown:{json.dumps(card, sort_keys=True)}\"\n",
    "        \n",
    "        # Create MD5 hash\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def _create_semantic_fingerprint(self, card: Dict) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Create semantic fingerprint for similarity detection.\n",
    "        Uses key terms and structure instead of exact content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            card_type = card.get(\"type\", \"basic\")\n",
    "            \n",
    "            if card_type == \"basic\":\n",
    "                text = f\"{card.get('front', '')} {card.get('back', '')}\"\n",
    "            elif card_type == \"cloze\":\n",
    "                cloze = card.get(\"cloze\", \"\")\n",
    "                # Extract content from cloze\n",
    "                text = re.sub(r'\\{\\{c\\d+::(.*?)\\}\\}', r'\\1', cloze)\n",
    "            elif card_type == \"tradeoff\":\n",
    "                text = card.get(\"front\", \"\")\n",
    "                for t in card.get(\"tradeoffs\", []):\n",
    "                    text += f\" {t.get('approach', '')}\"\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Normalize\n",
    "            normalized = self._normalize_text(text)\n",
    "            \n",
    "            # Extract key terms (nouns and verbs)\n",
    "            words = normalized.split()\n",
    "            \n",
    "            # Remove stop words\n",
    "            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', \n",
    "                         'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "            content_words = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "            \n",
    "            # Take top 10 content words\n",
    "            word_counter = Counter(content_words)\n",
    "            top_words = [word for word, _ in word_counter.most_common(10)]\n",
    "            \n",
    "            if not top_words:\n",
    "                return None\n",
    "            \n",
    "            # Sort for consistency\n",
    "            top_words.sort()\n",
    "            semantic_key = f\"{card_type}:\" + \":\".join(top_words)\n",
    "            \n",
    "            return hashlib.md5(semantic_key.encode()).hexdigest()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Semantic fingerprint failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def is_duplicate(self, card: Dict) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Check if card is a duplicate.\n",
    "        Returns (is_duplicate, fingerprint_type)\n",
    "        \"\"\"\n",
    "        self.total_checks += 1\n",
    "        \n",
    "        # Create fingerprints\n",
    "        exact_fp = self._create_fingerprint(card)\n",
    "        semantic_fp = self._create_semantic_fingerprint(card)\n",
    "        \n",
    "        # Track metrics\n",
    "        PROCESSING_METRICS.increment(\"duplicate_checks\")\n",
    "        \n",
    "        # First check: Exact fingerprint\n",
    "        with self.lock:\n",
    "            if exact_fp in self.fingerprints:\n",
    "                self.duplicates_filtered += 1\n",
    "                PROCESSING_METRICS.increment(\"exact_duplicates_found\")\n",
    "                return True, \"exact\"\n",
    "        \n",
    "        # Second check: Bloom filter (fast probabilistic)\n",
    "        if self.bloom.available and exact_fp in self.bloom:\n",
    "            # Might be a false positive, check exact set again\n",
    "            with self.lock:\n",
    "                if exact_fp in self.fingerprints:\n",
    "                    self.duplicates_filtered += 1\n",
    "                    PROCESSING_METRICS.increment(\"bloom_confirmed_duplicates\")\n",
    "                    return True, \"bloom_confirmed\"\n",
    "                else:\n",
    "                    # False positive suspected\n",
    "                    self.bloom.record_false_positive()\n",
    "                    PROCESSING_METRICS.increment(\"bloom_false_positives\")\n",
    "        \n",
    "        # Third check: Semantic similarity (if enabled)\n",
    "        if semantic_fp and CONFIG.get(\"ENABLE_SEMANTIC_DEDUP\", True):\n",
    "            with self.lock:\n",
    "                # Check if we've seen similar content before\n",
    "                semantic_key = f\"semantic:{semantic_fp}\"\n",
    "                if semantic_key in self.fingerprints:\n",
    "                    self.duplicates_filtered += 1\n",
    "                    PROCESSING_METRICS.increment(\"semantic_duplicates_found\")\n",
    "                    return True, \"semantic\"\n",
    "        \n",
    "        return False, None\n",
    "    \n",
    "    def add_fingerprint(self, fingerprint: str, semantic_fp: Optional[str] = None):\n",
    "        \"\"\"Add fingerprint to tracking systems\"\"\"\n",
    "        with self.lock:\n",
    "            self.fingerprints.add(fingerprint)\n",
    "            self.bloom.add(fingerprint)\n",
    "            \n",
    "            if semantic_fp:\n",
    "                semantic_key = f\"semantic:{semantic_fp}\"\n",
    "                self.fingerprints.add(semantic_key)\n",
    "                self.bloom.add(semantic_key)\n",
    "            \n",
    "            self._dirty = True\n",
    "            self._dirty_count += 1\n",
    "    \n",
    "    def save_if_dirty(self, force: bool = False):\n",
    "        \"\"\"Save fingerprints if dirty or forced\"\"\"\n",
    "        if not self._dirty and not force:\n",
    "            return\n",
    "        \n",
    "        if not force and self._dirty_count < CONFIG.get(\"FINGERPRINT_BATCH_SIZE\", 100):\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            try:\n",
    "                # Save fingerprints\n",
    "                data = {\n",
    "                    \"fingerprints\": list(self.fingerprints),\n",
    "                    \"count\": len(self.fingerprints),\n",
    "                    \"duplicates_filtered\": self.duplicates_filtered,\n",
    "                    \"total_checks\": self.total_checks,\n",
    "                    \"last_updated\": time.time(),\n",
    "                    \"version\": \"v2\"\n",
    "                }\n",
    "                atomic_write(data, self.fingerprints_file)\n",
    "                \n",
    "                # Save bloom filter if available\n",
    "                if self.bloom.available and self.bloom.filter:\n",
    "                    with open(self.bloom_file, \"wb\") as f:\n",
    "                        pickle.dump(self.bloom.filter, f)\n",
    "                \n",
    "                # Rebuild bloom filter if needed\n",
    "                if self.bloom.rebuild_if_needed():\n",
    "                    logger.info(\"Bloom filter rebuilt\")\n",
    "                \n",
    "                self._dirty = False\n",
    "                self._dirty_count = 0\n",
    "                self._last_save = time.time()\n",
    "                \n",
    "                logger.debug(f\"Saved {len(self.fingerprints):,} fingerprints\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving fingerprints: {e}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get detector statistics\"\"\"\n",
    "        bloom_stats = self.bloom.get_stats() if self.bloom else {}\n",
    "        \n",
    "        return {\n",
    "            \"fingerprints_count\": len(self.fingerprints),\n",
    "            \"duplicates_filtered\": self.duplicates_filtered,\n",
    "            \"total_checks\": self.total_checks,\n",
    "            \"duplicate_rate\": self.duplicates_filtered / self.total_checks if self.total_checks > 0 else 0,\n",
    "            \"bloom_stats\": bloom_stats,\n",
    "            \"last_save\": self._last_save,\n",
    "            \"dirty\": self._dirty,\n",
    "            \"dirty_count\": self._dirty_count\n",
    "        }\n",
    "    \n",
    "    def cleanup_old_fingerprints(self, max_age_days: int = 90):\n",
    "        \"\"\"Cleanup old fingerprints (not implemented for bloom filter)\"\"\"\n",
    "        # Note: Bloom filters don't support deletion\n",
    "        # In production, we would periodically create a new filter\n",
    "        logger.info(\"Fingerprint cleanup not implemented for bloom filters\")\n",
    "\n",
    "# ============================================================\n",
    "# TERMINAL REJECTION TRACKER (COMPLETE)\n",
    "# ============================================================\n",
    "class TerminalRejectionTracker:\n",
    "    \"\"\"\n",
    "    Track reels that are terminally rejected by logic.\n",
    "    Once a reel is marked terminal, it NEVER re-enters pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "        self.terminal_reels: Dict[str, Dict] = self._load()\n",
    "        self.lock = Lock()\n",
    "        self._dirty = False\n",
    "        \n",
    "        logger.info(f\"Terminal rejection tracker: {len(self.terminal_reels):,} reels tracked\")\n",
    "    \n",
    "    def _load(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load terminal rejections from disk\"\"\"\n",
    "        if not self.file_path.exists():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(self.file_path, default={})\n",
    "            if isinstance(data, dict) and \"terminal_reels\" in data:\n",
    "                return data[\"terminal_reels\"]\n",
    "            elif isinstance(data, dict):\n",
    "                return data  # Old format\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected terminal rejections format\")\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading terminal rejections: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def mark_terminal(self, reel_id: str, reason: str = \"\", \n",
    "                     stage: str = \"stage1\", rejection_type: str = \"STRUCTURAL\",\n",
    "                     details: Dict = None):\n",
    "        \"\"\"\n",
    "        Mark a reel as terminally rejected.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            self.terminal_reels[reel_id] = {\n",
    "                \"reason\": reason,\n",
    "                \"stage\": stage,\n",
    "                \"rejection_type\": rejection_type,\n",
    "                \"details\": details or {},\n",
    "                \"timestamp\": time.time(),\n",
    "                \"iso_timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            self._dirty = True\n",
    "            \n",
    "            # Track metrics\n",
    "            ROUTING_METRICS.increment(f\"terminal_rejection_{rejection_type}\")\n",
    "            ROUTING_METRICS.increment(f\"terminal_rejection_stage_{stage}\")\n",
    "            \n",
    "            logger.debug(f\"Reel {reel_id} marked terminal: {reason}\")\n",
    "    \n",
    "    def is_terminal(self, reel_id: str) -> bool:\n",
    "        \"\"\"Check if reel is terminally rejected\"\"\"\n",
    "        with self.lock:\n",
    "            return reel_id in self.terminal_reels\n",
    "    \n",
    "    def get_rejection_details(self, reel_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Get rejection details for a reel\"\"\"\n",
    "        with self.lock:\n",
    "            return self.terminal_reels.get(reel_id)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about terminal rejections\"\"\"\n",
    "        with self.lock:\n",
    "            reasons = defaultdict(int)\n",
    "            stages = defaultdict(int)\n",
    "            types = defaultdict(int)\n",
    "            \n",
    "            now = time.time()\n",
    "            recent_count = 0\n",
    "            old_count = 0\n",
    "            \n",
    "            for entry in self.terminal_reels.values():\n",
    "                reason = entry.get(\"reason\", \"unknown\")\n",
    "                stage = entry.get(\"stage\", \"unknown\")\n",
    "                rejection_type = entry.get(\"rejection_type\", \"unknown\")\n",
    "                timestamp = entry.get(\"timestamp\", now)\n",
    "                \n",
    "                reasons[reason] += 1\n",
    "                stages[stage] += 1\n",
    "                types[rejection_type] += 1\n",
    "                \n",
    "                # Age analysis\n",
    "                age_hours = (now - timestamp) / 3600\n",
    "                if age_hours < 24:\n",
    "                    recent_count += 1\n",
    "                elif age_hours > 30 * 24:  # 30 days\n",
    "                    old_count += 1\n",
    "            \n",
    "            return {\n",
    "                \"total\": len(self.terminal_reels),\n",
    "                \"recent_24h\": recent_count,\n",
    "                \"old_30d\": old_count,\n",
    "                \"by_reason\": dict(sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]),\n",
    "                \"by_stage\": dict(stages),\n",
    "                \"by_type\": dict(types)\n",
    "            }\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save terminal rejections to disk\"\"\"\n",
    "        if not self._dirty:\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            try:\n",
    "                data = {\n",
    "                    \"terminal_reels\": self.terminal_reels,\n",
    "                    \"count\": len(self.terminal_reels),\n",
    "                    \"stats\": self.get_stats(),\n",
    "                    \"last_updated\": time.time(),\n",
    "                    \"version\": \"v2\"\n",
    "                }\n",
    "                atomic_write(data, self.file_path)\n",
    "                self._dirty = False\n",
    "                logger.debug(f\"Saved {len(self.terminal_reels):,} terminal rejections\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save terminal rejections: {e}\")\n",
    "    \n",
    "    def cleanup_old_rejections(self, max_age_days: int = 90):\n",
    "        \"\"\"Cleanup rejections older than max_age_days\"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 3600\n",
    "            \n",
    "            to_remove = []\n",
    "            for reel_id, entry in self.terminal_reels.items():\n",
    "                timestamp = entry.get(\"timestamp\", 0)\n",
    "                if now - timestamp > max_age_seconds:\n",
    "                    to_remove.append(reel_id)\n",
    "            \n",
    "            for reel_id in to_remove:\n",
    "                del self.terminal_reels[reel_id]\n",
    "            \n",
    "            if to_remove:\n",
    "                self._dirty = True\n",
    "                logger.info(f\"Cleaned up {len(to_remove)} old terminal rejections (> {max_age_days} days)\")\n",
    "    \n",
    "    def export_for_analysis(self) -> List[Dict]:\n",
    "        \"\"\"Export rejections for analysis\"\"\"\n",
    "        with self.lock:\n",
    "            return [\n",
    "                {\n",
    "                    \"reel_id\": reel_id,\n",
    "                    **details\n",
    "                }\n",
    "                for reel_id, details in self.terminal_reels.items()\n",
    "            ]\n",
    "\n",
    "# ============================================================\n",
    "# REJECTION MEMORY WITH LEARNING (COMPLETE)\n",
    "# ============================================================\n",
    "class RejectionMemory:\n",
    "    \"\"\"\n",
    "    Learning system that remembers rejection patterns and successful strategies.\n",
    "    Key format: concept::category::topic_class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, memory_file: Path):\n",
    "        self.memory_file = memory_file\n",
    "        self.memory = self._load()\n",
    "        self.lock = Lock()\n",
    "        self._dirty = False\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.min_attempts_for_learning = 3\n",
    "        self.success_decay_factor = 0.95  # Decay old successes\n",
    "        self.rejection_decay_factor = 0.9  # Decay old rejections\n",
    "        \n",
    "        logger.info(f\"Rejection memory: {len(self.memory):,} concepts tracked\")\n",
    "    \n",
    "    def _load(self) -> Dict:\n",
    "        \"\"\"Load rejection memory from disk\"\"\"\n",
    "        if not self.memory_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(self.memory_file, default={})\n",
    "            if isinstance(data, dict):\n",
    "                return data\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected memory format\")\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading rejection memory: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _make_key(self, concept: str, category: str = \"\", topic_class: str = \"\") -> str:\n",
    "        \"\"\"Create memory key\"\"\"\n",
    "        return f\"{concept}::{category}::{topic_class}\"\n",
    "    \n",
    "    def _make_canonical_key(self, concept: str, category: str = \"\") -> str:\n",
    "        \"\"\"Create canonical key (without topic class)\"\"\"\n",
    "        return f\"{concept}::{category or 'unknown'}\"\n",
    "    \n",
    "    def _get_or_create_entry(self, concept: str, category: str = \"\", topic_class: str = \"\") -> Dict:\n",
    "        \"\"\"Get or create memory entry\"\"\"\n",
    "        key = self._make_key(concept, category, topic_class)\n",
    "        \n",
    "        if key not in self.memory:\n",
    "            self.memory[key] = {\n",
    "                \"concept\": concept,\n",
    "                \"category\": category,\n",
    "                \"topic_class\": topic_class,\n",
    "                \"rejections\": [],\n",
    "                \"successful_strategies\": [],  # Now a list to track multiple successes\n",
    "                \"first_seen_ts\": time.time(),\n",
    "                \"first_success_ts\": None,\n",
    "                \"last_success_ts\": None,\n",
    "                \"attempts_until_first_success\": None,\n",
    "                \"total_attempts\": 0,\n",
    "                \"success_count\": 0,\n",
    "                \"rejection_count\": 0,\n",
    "                \"success_by_strategy\": defaultdict(int),\n",
    "                \"rejection_by_reason\": defaultdict(int),\n",
    "                \"topic_class_history\": [],  # Track topic class changes\n",
    "                \"confidence_history\": [],  # Track confidence over time\n",
    "                \"metadata\": {}\n",
    "            }\n",
    "            self._dirty = True\n",
    "        \n",
    "        return self.memory[key]\n",
    "    \n",
    "    def record_rejection(self, concept: str, score: int, reason: str,\n",
    "                        category: str = \"\", topic_class: str = \"\",\n",
    "                        details: Dict = None):\n",
    "        \"\"\"\n",
    "        Record a rejection with details.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            entry = self._get_or_create_entry(concept, category, topic_class)\n",
    "            \n",
    "            rejection_record = {\n",
    "                \"score\": score,\n",
    "                \"reason\": reason,\n",
    "                \"details\": details or {},\n",
    "                \"timestamp\": time.time(),\n",
    "                \"topic_class\": topic_class\n",
    "            }\n",
    "            \n",
    "            entry[\"rejections\"].append(rejection_record)\n",
    "            entry[\"rejection_count\"] += 1\n",
    "            entry[\"total_attempts\"] += 1\n",
    "            entry[\"rejection_by_reason\"][reason] += 1\n",
    "            \n",
    "            # Track topic class history\n",
    "            if topic_class and (not entry[\"topic_class_history\"] or \n",
    "                              entry[\"topic_class_history\"][-1][\"topic_class\"] != topic_class):\n",
    "                entry[\"topic_class_history\"].append({\n",
    "                    \"topic_class\": topic_class,\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "            \n",
    "            self._dirty = True\n",
    "            \n",
    "            # Apply decay to old records\n",
    "            self._apply_decay(entry)\n",
    "            \n",
    "            # Log for debugging\n",
    "            if entry[\"rejection_count\"] % 10 == 0:\n",
    "                logger.debug(f\"Concept '{concept}' has {entry['rejection_count']} rejections\")\n",
    "    \n",
    "    def record_success(self, concept: str, strategy: str, confidence: float,\n",
    "                      category: str = \"\", topic_class: str = \"\",\n",
    "                      delta_score: int = 0, details: Dict = None):\n",
    "        \"\"\"\n",
    "        Record a successful processing.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            entry = self._get_or_create_entry(concept, category, topic_class)\n",
    "            \n",
    "            success_record = {\n",
    "                \"strategy\": strategy,\n",
    "                \"confidence\": confidence,\n",
    "                \"delta_score\": delta_score,\n",
    "                \"topic_class\": topic_class,\n",
    "                \"details\": details or {},\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            entry[\"successful_strategies\"].append(success_record)\n",
    "            entry[\"success_count\"] += 1\n",
    "            entry[\"total_attempts\"] += 1\n",
    "            entry[\"last_success_ts\"] = time.time()\n",
    "            entry[\"success_by_strategy\"][strategy] += 1\n",
    "            \n",
    "            # Track confidence history\n",
    "            entry[\"confidence_history\"].append({\n",
    "                \"confidence\": confidence,\n",
    "                \"strategy\": strategy,\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "            \n",
    "            # Keep only recent confidence history\n",
    "            if len(entry[\"confidence_history\"]) > 20:\n",
    "                entry[\"confidence_history\"] = entry[\"confidence_history\"][-20:]\n",
    "            \n",
    "            # Record first success\n",
    "            if entry[\"first_success_ts\"] is None:\n",
    "                entry[\"first_success_ts\"] = time.time()\n",
    "                entry[\"attempts_until_first_success\"] = entry[\"rejection_count\"]\n",
    "                logger.info(f\"Concept '{concept}' succeeded after {entry['attempts_until_first_success']} attempts\")\n",
    "            \n",
    "            self._dirty = True\n",
    "            \n",
    "            # Apply decay to old records\n",
    "            self._apply_decay(entry)\n",
    "    \n",
    "    def _apply_decay(self, entry: Dict):\n",
    "        \"\"\"Apply decay to old records to give more weight to recent data\"\"\"\n",
    "        now = time.time()\n",
    "        decay_window = 30 * 24 * 3600  # 30 days\n",
    "        \n",
    "        # Decay old successes\n",
    "        recent_successes = []\n",
    "        for success in entry.get(\"successful_strategies\", []):\n",
    "            age = now - success.get(\"timestamp\", 0)\n",
    "            if age < decay_window:\n",
    "                recent_successes.append(success)\n",
    "        \n",
    "        if len(recent_successes) < len(entry.get(\"successful_strategies\", [])):\n",
    "            entry[\"successful_strategies\"] = recent_successes\n",
    "        \n",
    "        # Decay old rejections\n",
    "        recent_rejections = []\n",
    "        for rejection in entry.get(\"rejections\", []):\n",
    "            age = now - rejection.get(\"timestamp\", 0)\n",
    "            if age < decay_window:\n",
    "                recent_rejections.append(rejection)\n",
    "        \n",
    "        if len(recent_rejections) < len(entry.get(\"rejections\", [])):\n",
    "            entry[\"rejections\"] = recent_rejections\n",
    "    \n",
    "    def get_best_strategy(self, concept: str, category: str = \"\", \n",
    "                         topic_class: str = \"\") -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get the best strategy for a concept based on historical success.\n",
    "        Returns strategy details or None if no successful strategy.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            key = self._make_key(concept, category, topic_class)\n",
    "            entry = self.memory.get(key)\n",
    "            \n",
    "            if not entry or not entry.get(\"successful_strategies\"):\n",
    "                # Try canonical key (without topic class)\n",
    "                canonical_key = self._make_canonical_key(concept, category)\n",
    "                for stored_key, stored_entry in self.memory.items():\n",
    "                    if stored_key.startswith(canonical_key + \"::\"):\n",
    "                        if stored_entry.get(\"successful_strategies\"):\n",
    "                            entry = stored_entry\n",
    "                            break\n",
    "            \n",
    "            if not entry or not entry.get(\"successful_strategies\"):\n",
    "                return None\n",
    "            \n",
    "            # Find most recent successful strategy\n",
    "            strategies = entry.get(\"successful_strategies\", [])\n",
    "            if not strategies:\n",
    "                return None\n",
    "            \n",
    "            # Sort by recency and confidence\n",
    "            strategies.sort(key=lambda x: (\n",
    "                x.get(\"timestamp\", 0),\n",
    "                x.get(\"confidence\", 0)\n",
    "            ), reverse=True)\n",
    "            \n",
    "            best_strategy = strategies[0]\n",
    "            \n",
    "            # Calculate success rate for this strategy\n",
    "            strategy_name = best_strategy.get(\"strategy\", \"\")\n",
    "            total_with_strategy = entry[\"success_by_strategy\"].get(strategy_name, 0)\n",
    "            total_attempts = entry.get(\"total_attempts\", 0)\n",
    "            \n",
    "            success_rate = total_with_strategy / total_attempts if total_attempts > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                \"strategy\": strategy_name,\n",
    "                \"confidence\": best_strategy.get(\"confidence\", 0),\n",
    "                \"success_rate\": success_rate,\n",
    "                \"last_used\": best_strategy.get(\"timestamp\", 0),\n",
    "                \"total_uses\": total_with_strategy,\n",
    "                \"topic_class\": best_strategy.get(\"topic_class\", \"\"),\n",
    "                \"is_recent\": (time.time() - best_strategy.get(\"timestamp\", 0)) < (7 * 24 * 3600)\n",
    "            }\n",
    "    \n",
    "    def should_skip(self, concept: str, category: str = \"\", \n",
    "                   topic_class: str = \"\", max_rejections: int = 5) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if a concept should be skipped due to too many rejections.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            key = self._make_key(concept, category, topic_class)\n",
    "            entry = self.memory.get(key)\n",
    "            \n",
    "            if not entry:\n",
    "                return False\n",
    "            \n",
    "            rejection_count = entry.get(\"rejection_count\", 0)\n",
    "            has_success = entry.get(\"success_count\", 0) > 0\n",
    "            \n",
    "            # If too many rejections and no success, skip\n",
    "            if rejection_count >= max_rejections and not has_success:\n",
    "                return True\n",
    "            \n",
    "            # If recent rejections are increasing\n",
    "            recent_rejections = 0\n",
    "            now = time.time()\n",
    "            for rejection in entry.get(\"rejections\", []):\n",
    "                if now - rejection.get(\"timestamp\", 0) < 24 * 3600:  # Last 24 hours\n",
    "                    recent_rejections += 1\n",
    "            \n",
    "            if recent_rejections >= 3 and not has_success:\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "    \n",
    "    def get_concept_stats(self, concept: str, category: str = \"\", \n",
    "                         topic_class: str = \"\") -> Optional[Dict]:\n",
    "        \"\"\"Get statistics for a specific concept\"\"\"\n",
    "        with self.lock:\n",
    "            key = self._make_key(concept, category, topic_class)\n",
    "            entry = self.memory.get(key)\n",
    "            \n",
    "            if not entry:\n",
    "                return None\n",
    "            \n",
    "            # Calculate success rate\n",
    "            success_rate = entry[\"success_count\"] / entry[\"total_attempts\"] if entry[\"total_attempts\"] > 0 else 0\n",
    "            \n",
    "            # Most common rejection reason\n",
    "            rejection_reasons = entry.get(\"rejection_by_reason\", {})\n",
    "            most_common_reason = max(rejection_reasons.items(), key=lambda x: x[1])[0] if rejection_reasons else \"none\"\n",
    "            \n",
    "            # Most successful strategy\n",
    "            success_strategies = entry.get(\"success_by_strategy\", {})\n",
    "            most_successful_strategy = max(success_strategies.items(), key=lambda x: x[1])[0] if success_strategies else \"none\"\n",
    "            \n",
    "            return {\n",
    "                \"concept\": concept,\n",
    "                \"category\": category,\n",
    "                \"topic_class\": topic_class,\n",
    "                \"total_attempts\": entry[\"total_attempts\"],\n",
    "                \"success_count\": entry[\"success_count\"],\n",
    "                \"rejection_count\": entry[\"rejection_count\"],\n",
    "                \"success_rate\": success_rate,\n",
    "                \"first_seen\": entry[\"first_seen_ts\"],\n",
    "                \"first_success\": entry[\"first_success_ts\"],\n",
    "                \"last_success\": entry[\"last_success_ts\"],\n",
    "                \"attempts_until_first_success\": entry[\"attempts_until_first_success\"],\n",
    "                \"most_common_rejection_reason\": most_common_reason,\n",
    "                \"most_successful_strategy\": most_successful_strategy,\n",
    "                \"rejection_by_reason\": dict(rejection_reasons),\n",
    "                \"success_by_strategy\": dict(success_strategies)\n",
    "            }\n",
    "    \n",
    "    def get_learning_velocity(self) -> Dict:\n",
    "        \"\"\"Calculate learning velocity metrics\"\"\"\n",
    "        with self.lock:\n",
    "            concepts_with_success = []\n",
    "            concepts_learning = []\n",
    "            \n",
    "            for entry in self.memory.values():\n",
    "                if entry.get(\"success_count\", 0) > 0:\n",
    "                    concepts_with_success.append(entry)\n",
    "                elif entry.get(\"rejection_count\", 0) > 0:\n",
    "                    concepts_learning.append(entry)\n",
    "            \n",
    "            # Calculate average attempts until first success\n",
    "            attempts_until_success = []\n",
    "            for entry in concepts_with_success:\n",
    "                if entry.get(\"attempts_until_first_success\"):\n",
    "                    attempts_until_success.append(entry[\"attempts_until_first_success\"])\n",
    "            \n",
    "            avg_attempts = sum(attempts_until_success) / len(attempts_until_success) if attempts_until_success else 0\n",
    "            \n",
    "            # Calculate success rate by topic class\n",
    "            success_by_topic = defaultdict(lambda: {\"success\": 0, \"total\": 0})\n",
    "            for entry in self.memory.values():\n",
    "                topic = entry.get(\"topic_class\", \"unknown\")\n",
    "                success_by_topic[topic][\"success\"] += entry.get(\"success_count\", 0)\n",
    "                success_by_topic[topic][\"total\"] += entry.get(\"total_attempts\", 0)\n",
    "            \n",
    "            # Calculate success rates\n",
    "            success_rates = {}\n",
    "            for topic, stats in success_by_topic.items():\n",
    "                if stats[\"total\"] > 0:\n",
    "                    success_rates[topic] = stats[\"success\"] / stats[\"total\"]\n",
    "            \n",
    "            return {\n",
    "                \"total_concepts\": len(self.memory),\n",
    "                \"concepts_learned\": len(concepts_with_success),\n",
    "                \"concepts_learning\": len(concepts_learning),\n",
    "                \"avg_attempts_until_success\": round(avg_attempts, 2),\n",
    "                \"success_rates_by_topic\": success_rates,\n",
    "                \"learning_efficiency\": len(concepts_with_success) / len(self.memory) if self.memory else 0\n",
    "            }\n",
    "    \n",
    "    def save(self, prune: bool = True):\n",
    "        \"\"\"Save rejection memory to disk\"\"\"\n",
    "        if not self._dirty:\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            try:\n",
    "                # Prune old entries if requested\n",
    "                if prune:\n",
    "                    self._prune_old_entries()\n",
    "                \n",
    "                data = {\n",
    "                    \"memory\": self.memory,\n",
    "                    \"count\": len(self.memory),\n",
    "                    \"learning_stats\": self.get_learning_velocity(),\n",
    "                    \"last_updated\": time.time(),\n",
    "                    \"version\": \"v2\"\n",
    "                }\n",
    "                atomic_write(data, self.memory_file)\n",
    "                self._dirty = False\n",
    "                logger.debug(f\"Saved rejection memory: {len(self.memory):,} concepts\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save rejection memory: {e}\")\n",
    "    \n",
    "    def _prune_old_entries(self, ttl_days: int = 90):\n",
    "        \"\"\"Prune old entries that haven't been successful\"\"\"\n",
    "        now = time.time()\n",
    "        ttl_seconds = ttl_days * 24 * 60 * 60\n",
    "        \n",
    "        keys_to_remove = []\n",
    "        for key, entry in self.memory.items():\n",
    "            first_seen = entry.get(\"first_seen_ts\", now)\n",
    "            age_seconds = now - first_seen\n",
    "            has_success = entry.get(\"success_count\", 0) > 0\n",
    "            \n",
    "            if age_seconds > ttl_seconds and not has_success:\n",
    "                keys_to_remove.append(key)\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            del self.memory[key]\n",
    "        \n",
    "        if keys_to_remove:\n",
    "            logger.info(f\"Pruned {len(keys_to_remove)} stale memory entries (>{ttl_days} days old, no success)\")\n",
    "            self._dirty = True\n",
    "    \n",
    "    def export_for_analysis(self) -> List[Dict]:\n",
    "        \"\"\"Export memory for analysis\"\"\"\n",
    "        with self.lock:\n",
    "            return [\n",
    "                self.get_concept_stats(\n",
    "                    entry[\"concept\"],\n",
    "                    entry[\"category\"],\n",
    "                    entry[\"topic_class\"]\n",
    "                )\n",
    "                for entry in self.memory.values()\n",
    "            ]\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE GLOBAL INSTANCES\n",
    "# ============================================================\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"INITIALIZING MEMORY & LEARNING SYSTEMS\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Initialize duplicate detector\n",
    "duplicate_detector = HybridDuplicateDetector(CARD_FINGERPRINTS_FILE, BLOOM_FILE)\n",
    "\n",
    "# Initialize terminal rejection tracker\n",
    "terminal_rejections = TerminalRejectionTracker(TERMINAL_REJECTIONS_FILE)\n",
    "\n",
    "# Initialize rejection memory if enabled\n",
    "rejection_memory = None\n",
    "if CONFIG.get(\"ENABLE_REJECTION_LEARNING\", True):\n",
    "    rejection_memory = RejectionMemory(REJECTION_MEMORY_FILE)\n",
    "    logger.info(\"Rejection memory enabled\")\n",
    "else:\n",
    "    logger.info(\"Rejection memory disabled\")\n",
    "\n",
    "# Initialize from existing state\n",
    "logger.info(f\"Duplicate detector: {duplicate_detector.get_stats()['fingerprints_count']:,} fingerprints\")\n",
    "logger.info(f\"Terminal rejections: {terminal_rejections.get_stats()['total']:,} reels\")\n",
    "if rejection_memory:\n",
    "    stats = rejection_memory.get_learning_velocity()\n",
    "    logger.info(f\"Rejection memory: {stats['total_concepts']:,} concepts, \"\n",
    "                f\"{stats['concepts_learned']:,} learned\")\n",
    "\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beddf38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: LLM & MODEL MANAGEMENT (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(\"anki-pipeline.llm\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Primary model configurations\n",
    "PIPELINE_MODELS = {\n",
    "    'extract': ModelConfig(\n",
    "        name='mistral:7b-instruct-v0.2',\n",
    "        temperature=0.1,\n",
    "        num_predict=2000,\n",
    "        timeout=300\n",
    "    ),\n",
    "    'extract_retry': ModelConfig(\n",
    "        name='mistral:7b-instruct-v0.2',\n",
    "        temperature=0.15,\n",
    "        num_predict=1500,\n",
    "        timeout=180\n",
    "    ),\n",
    "    'generate_basic': ModelConfig(\n",
    "        name='mistral:7b-instruct',\n",
    "        temperature=0.2,\n",
    "        num_predict=1500,\n",
    "        timeout=180\n",
    "    ),\n",
    "    'generate_cloze': ModelConfig(\n",
    "        name='mistral:7b-instruct',\n",
    "        temperature=0.2,\n",
    "        num_predict=800,\n",
    "        timeout=120\n",
    "    ),\n",
    "    'generate_tradeoff': ModelConfig(\n",
    "        name='qwen2.5:7b-instruct',\n",
    "        temperature=0.2,\n",
    "        num_predict=1200,\n",
    "        timeout=150\n",
    "    ),\n",
    "    'validate': ModelConfig(\n",
    "        name='qwen2.5:7b-instruct',\n",
    "        temperature=0.05,\n",
    "        num_predict=500,\n",
    "        timeout=60\n",
    "    ),\n",
    "    'normalize': ModelConfig(\n",
    "        name='mistral:7b-instruct-v0.2',\n",
    "        temperature=0.1,\n",
    "        num_predict=1000,\n",
    "        timeout=120\n",
    "    )\n",
    "}\n",
    "\n",
    "# Model fallback chains\n",
    "MODEL_FALLBACKS = {\n",
    "    'mistral:7b-instruct-v0.2': ['mistral:7b-instruct', 'mistral:latest'],\n",
    "    'llama3.1:8b-instruct-q8_0': [\n",
    "        'mistral:7b-instruct',\n",
    "        'qwen2.5:7b-instruct',\n",
    "        'llama3.1:8b-instruct-q4_0',\n",
    "        'llama3.1:latest'\n",
    "    ],\n",
    "    'qwen2.5:7b-instruct-q4_K_M': [\n",
    "        'qwen2.5:7b-instruct',\n",
    "        'mistral:7b-instruct',\n",
    "        'llama3.1:8b-instruct-q4_0'\n",
    "    ],\n",
    "    'qwen2.5:7b-instruct': [\n",
    "        'qwen2.5:latest',\n",
    "        'mistral:7b-instruct',\n",
    "        'phi3:mini'\n",
    "    ],\n",
    "    'mistral:7b-instruct': [\n",
    "        'mistral:latest',\n",
    "        'qwen2.5:7b-instruct',\n",
    "        'llama3.1:8b-instruct-q4_0'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Model capability profiles\n",
    "MODEL_CAPABILITIES = {\n",
    "    'mistral': {\n",
    "        'context_window': 32768,\n",
    "        'supports_json': True,\n",
    "        'max_tokens': 8000,\n",
    "        'good_at': ['reasoning', 'instruction_following']\n",
    "    },\n",
    "    'qwen': {\n",
    "        'context_window': 32768,\n",
    "        'supports_json': True,\n",
    "        'max_tokens': 8000,\n",
    "        'good_at': ['coding', 'technical_content']\n",
    "    },\n",
    "    'llama': {\n",
    "        'context_window': 8192,\n",
    "        'supports_json': True,\n",
    "        'max_tokens': 4000,\n",
    "        'good_at': ['general_knowledge', 'summarization']\n",
    "    },\n",
    "    'phi': {\n",
    "        'context_window': 4096,\n",
    "        'supports_json': True,\n",
    "        'max_tokens': 2000,\n",
    "        'good_at': ['fast_inference', 'small_tasks']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# HTTP SESSION MANAGEMENT\n",
    "# ============================================================\n",
    "\n",
    "class ResilientSession:\n",
    "    \"\"\"HTTP session with retry and circuit breaker support\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"User-Agent\": \"Anki-Pipeline/1.0\"\n",
    "        })\n",
    "        \n",
    "        # Retry configuration\n",
    "        self.max_retries = 3\n",
    "        self.retry_delays = [1, 2, 5, 10]  # seconds\n",
    "        \n",
    "        # Statistics\n",
    "        self.requests_made = 0\n",
    "        self.failures = 0\n",
    "        self.total_latency = 0.0\n",
    "        \n",
    "        logger.info(\"Resilient HTTP session initialized\")\n",
    "    \n",
    "    def request_with_retry(self, method: str, url: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make HTTP request with exponential backoff\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Add timeout if not specified\n",
    "                if 'timeout' not in kwargs:\n",
    "                    kwargs['timeout'] = 30\n",
    "                \n",
    "                response = self.session.request(method, url, **kwargs)\n",
    "                latency = time.time() - start_time\n",
    "                \n",
    "                # Track statistics\n",
    "                self.requests_made += 1\n",
    "                self.total_latency += latency\n",
    "                \n",
    "                # Check for HTTP errors\n",
    "                if response.status_code >= 500:\n",
    "                    raise requests.exceptions.HTTPError(f\"Server error: {response.status_code}\")\n",
    "                \n",
    "                # Success\n",
    "                PROCESSING_METRICS.record_timing(\"http_request\", latency)\n",
    "                return response\n",
    "                \n",
    "            except (requests.exceptions.Timeout, \n",
    "                    requests.exceptions.ConnectionError,\n",
    "                    requests.exceptions.HTTPError) as e:\n",
    "                \n",
    "                last_error = e\n",
    "                self.failures += 1\n",
    "                \n",
    "                if attempt < self.max_retries:\n",
    "                    delay = self.retry_delays[min(attempt, len(self.retry_delays) - 1)]\n",
    "                    logger.warning(f\"Request failed (attempt {attempt + 1}/{self.max_retries + 1}): {e}. Retrying in {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(f\"Request failed after {self.max_retries + 1} attempts: {e}\")\n",
    "        \n",
    "        # All retries failed\n",
    "        error_tracker.record(\n",
    "            \"http_request_failed\",\n",
    "            f\"Failed after {self.max_retries + 1} attempts\",\n",
    "            {\"url\": url, \"method\": method, \"last_error\": str(last_error)}\n",
    "        )\n",
    "        \n",
    "        raise last_error or requests.exceptions.RequestException(\"Request failed\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get session statistics\"\"\"\n",
    "        avg_latency = self.total_latency / self.requests_made if self.requests_made > 0 else 0\n",
    "        success_rate = (self.requests_made - self.failures) / self.requests_made if self.requests_made > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"requests_made\": self.requests_made,\n",
    "            \"failures\": self.failures,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"total_latency\": self.total_latency\n",
    "        }\n",
    "\n",
    "# Create global session\n",
    "session = ResilientSession()\n",
    "\n",
    "# ============================================================\n",
    "# GPU & HARDWARE DETECTION\n",
    "# ============================================================\n",
    "\n",
    "def detect_hardware() -> Dict:\n",
    "    \"\"\"Detect available hardware for model optimization\"\"\"\n",
    "    hardware_info = {\n",
    "        \"gpu_available\": False,\n",
    "        \"gpu_memory_gb\": 0,\n",
    "        \"cpu_cores\": os.cpu_count() or 1,\n",
    "        \"total_memory_gb\": 0,\n",
    "        \"platform\": sys.platform\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check for NVIDIA GPU\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.total\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            hardware_info[\"gpu_available\"] = True\n",
    "            memory_mb = int(result.stdout.strip().split('\\n')[0])\n",
    "            hardware_info[\"gpu_memory_gb\"] = memory_mb / 1024\n",
    "            \n",
    "            # Get GPU name\n",
    "            result = subprocess.run(\n",
    "                [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=5\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                hardware_info[\"gpu_name\"] = result.stdout.strip()\n",
    "    \n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError, ValueError):\n",
    "        # No GPU or nvidia-smi not available\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Get system memory (Linux/Mac)\n",
    "        if sys.platform == \"linux\":\n",
    "            with open('/proc/meminfo', 'r') as f:\n",
    "                for line in f:\n",
    "                    if 'MemTotal' in line:\n",
    "                        kb = int(line.split()[1])\n",
    "                        hardware_info[\"total_memory_gb\"] = kb / (1024 * 1024)\n",
    "                        break\n",
    "        elif sys.platform == \"darwin\":  # macOS\n",
    "            result = subprocess.run(\n",
    "                [\"sysctl\", \"-n\", \"hw.memsize\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                bytes_mem = int(result.stdout.strip())\n",
    "                hardware_info[\"total_memory_gb\"] = bytes_mem / (1024 ** 3)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Could not detect system memory: {e}\")\n",
    "    \n",
    "    logger.info(f\"Hardware detected: {json.dumps(hardware_info, indent=2)}\")\n",
    "    return hardware_info\n",
    "\n",
    "# Detect hardware\n",
    "HARDWARE_INFO = detect_hardware()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL MANAGEMENT\n",
    "# ============================================================\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Manage LLM models with health checks and fallbacks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_models_cache = None\n",
    "        self.cache_timestamp = 0\n",
    "        self.cache_ttl = 30  # seconds\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Model health tracking\n",
    "        self.model_health = defaultdict(lambda: {\"success\": 0, \"failures\": 0})\n",
    "        self.unhealthy_models = set()\n",
    "        self.health_lock = Lock()\n",
    "        \n",
    "        logger.info(\"Model manager initialized\")\n",
    "    \n",
    "    def get_available_models(self, force_refresh: bool = False) -> List[str]:\n",
    "        \"\"\"Get list of available models from Ollama\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        with self.lock:\n",
    "            if (not force_refresh and \n",
    "                self.available_models_cache and \n",
    "                now - self.cache_timestamp < self.cache_ttl):\n",
    "                return self.available_models_cache\n",
    "        \n",
    "        try:\n",
    "            response = session.request_with_retry(\n",
    "                \"GET\",\n",
    "                f\"{CONFIG['OLLAMA_URL']}/api/tags\",\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            models_data = response.json()\n",
    "            models = [m[\"name\"] for m in models_data.get(\"models\", [])]\n",
    "            \n",
    "            with self.lock:\n",
    "                self.available_models_cache = models\n",
    "                self.cache_timestamp = now\n",
    "            \n",
    "            logger.debug(f\"Found {len(models)} available models\")\n",
    "            return models\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get available models: {e}\")\n",
    "            \n",
    "            # Return cache if available, even if stale\n",
    "            with self.lock:\n",
    "                if self.available_models_cache:\n",
    "                    logger.warning(\"Using stale model cache\")\n",
    "                    return self.available_models_cache\n",
    "            \n",
    "            return []\n",
    "    \n",
    "    def check_model_health(self, model_name: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if a model is healthy and responsive\"\"\"\n",
    "        # Skip if marked unhealthy recently\n",
    "        with self.health_lock:\n",
    "            if model_name in self.unhealthy_models:\n",
    "                return False, \"Marked as unhealthy\"\n",
    "        \n",
    "        try:\n",
    "            # Simple test prompt\n",
    "            test_payload = {\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": \"test\",\n",
    "                \"stream\": False,\n",
    "                \"options\": {\"num_predict\": 1}\n",
    "            }\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = session.request_with_retry(\n",
    "                \"POST\",\n",
    "                f\"{CONFIG['OLLAMA_URL']}/api/generate\",\n",
    "                json=test_payload,\n",
    "                timeout=15\n",
    "            )\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with self.health_lock:\n",
    "                    self.model_health[model_name][\"success\"] += 1\n",
    "                \n",
    "                logger.debug(f\"Model {model_name} health check passed ({latency:.2f}s)\")\n",
    "                return True, None\n",
    "            else:\n",
    "                error_msg = response.text\n",
    "                logger.warning(f\"Model {model_name} health check failed: {response.status_code} - {error_msg}\")\n",
    "                \n",
    "                with self.health_lock:\n",
    "                    self.model_health[model_name][\"failures\"] += 1\n",
    "                \n",
    "                return False, f\"HTTP {response.status_code}: {error_msg}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Model {model_name} health check error: {e}\")\n",
    "            \n",
    "            with self.health_lock:\n",
    "                self.model_health[model_name][\"failures\"] += 1\n",
    "                if self.model_health[model_name][\"failures\"] >= 3:\n",
    "                    self.unhealthy_models.add(model_name)\n",
    "                    logger.error(f\"Model {model_name} marked as unhealthy\")\n",
    "            \n",
    "            return False, str(e)\n",
    "    \n",
    "    def get_model_capability(self, model_name: str) -> Dict:\n",
    "        \"\"\"Get capabilities of a specific model\"\"\"\n",
    "        model_lower = model_name.lower()\n",
    "        capabilities = {\n",
    "            \"supports_generate\": True,\n",
    "            \"supports_json\": True,\n",
    "            \"estimated_context\": 4096,\n",
    "            \"estimated_max_tokens\": 2000,\n",
    "            \"family\": \"unknown\"\n",
    "        }\n",
    "        \n",
    "        # Determine model family\n",
    "        for family, profile in MODEL_CAPABILITIES.items():\n",
    "            if family in model_lower:\n",
    "                capabilities.update(profile)\n",
    "                capabilities[\"family\"] = family\n",
    "                break\n",
    "        \n",
    "        # Check for chat-only models\n",
    "        chat_hints = [\"chat\", \"instruct-chat\", \"assistant\"]\n",
    "        if any(hint in model_lower for hint in chat_hints):\n",
    "            capabilities[\"supports_generate\"] = True  # Most instruct models support generate\n",
    "        \n",
    "        # Check for embedding models\n",
    "        if \"embed\" in model_lower:\n",
    "            capabilities[\"supports_generate\"] = False\n",
    "        \n",
    "        # Adjust based on model size hints\n",
    "        if \"7b\" in model_lower:\n",
    "            capabilities[\"estimated_context\"] = min(capabilities[\"estimated_context\"], 32768)\n",
    "            capabilities[\"estimated_max_tokens\"] = min(capabilities[\"estimated_max_tokens\"], 8000)\n",
    "        elif \"13b\" in model_lower:\n",
    "            capabilities[\"estimated_context\"] = min(capabilities[\"estimated_context\"], 16384)\n",
    "            capabilities[\"estimated_max_tokens\"] = min(capabilities[\"estimated_max_tokens\"], 4000)\n",
    "        \n",
    "        # Cache the capability\n",
    "        with MODEL_CAPABILITY_LOCK:\n",
    "            MODEL_CAPABILITY_CACHE[model_name] = capabilities\n",
    "        \n",
    "        return capabilities\n",
    "    \n",
    "    def select_best_model(self, preferred: str, fallbacks: List[str], \n",
    "                         purpose: str = \"general\") -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Select the best available model for a specific purpose.\n",
    "        \"\"\"\n",
    "        available_models = set(self.get_available_models())\n",
    "        \n",
    "        # Check preferred model first\n",
    "        if preferred in available_models:\n",
    "            is_healthy, error = self.check_model_health(preferred)\n",
    "            if is_healthy:\n",
    "                capabilities = self.get_model_capability(preferred)\n",
    "                \n",
    "                # Check if model supports the purpose\n",
    "                if purpose == \"extract\" and capabilities.get(\"supports_json\", True):\n",
    "                    logger.debug(f\"Selected preferred model: {preferred}\")\n",
    "                    return preferred\n",
    "                elif purpose == \"generate\" and capabilities.get(\"supports_generate\", True):\n",
    "                    logger.debug(f\"Selected preferred model: {preferred}\")\n",
    "                    return preferred\n",
    "                else:\n",
    "                    logger.warning(f\"Preferred model {preferred} doesn't support {purpose}\")\n",
    "        \n",
    "        # Try fallbacks\n",
    "        for fallback in fallbacks:\n",
    "            if fallback in available_models:\n",
    "                is_healthy, error = self.check_model_health(fallback)\n",
    "                if is_healthy:\n",
    "                    capabilities = self.get_model_capability(fallback)\n",
    "                    \n",
    "                    # Check capability for purpose\n",
    "                    if (purpose == \"extract\" and capabilities.get(\"supports_json\", True)) or \\\n",
    "                       (purpose == \"generate\" and capabilities.get(\"supports_generate\", True)):\n",
    "                        logger.info(f\"Using fallback model: {fallback} (instead of {preferred})\")\n",
    "                        return fallback\n",
    "        \n",
    "        logger.error(f\"No suitable model found for {purpose}. Preferred: {preferred}, Fallbacks: {fallbacks}\")\n",
    "        return None\n",
    "    \n",
    "    def warmup_models(self, models_to_warm: Optional[List[str]] = None):\n",
    "        \"\"\"Warm up models in parallel\"\"\"\n",
    "        if models_to_warm is None:\n",
    "            models_to_warm = list(set(cfg.name for cfg in PIPELINE_MODELS.values()))\n",
    "        \n",
    "        logger.info(f\"Warming up {len(models_to_warm)} models...\")\n",
    "        \n",
    "        warmup_results = []\n",
    "        with ThreadPoolExecutor(max_workers=min(4, len(models_to_warm))) as executor:\n",
    "            future_to_model = {\n",
    "                executor.submit(self.check_model_health, model): model \n",
    "                for model in models_to_warm\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_model):\n",
    "                model = future_to_model[future]\n",
    "                try:\n",
    "                    healthy, error = future.result(timeout=30)\n",
    "                    warmup_results.append((model, healthy, error))\n",
    "                except Exception as e:\n",
    "                    warmup_results.append((model, False, str(e)))\n",
    "        \n",
    "        # Log results\n",
    "        healthy_count = sum(1 for _, healthy, _ in warmup_results if healthy)\n",
    "        logger.info(f\"Warmup complete: {healthy_count}/{len(warmup_results)} models healthy\")\n",
    "        \n",
    "        for model, healthy, error in warmup_results:\n",
    "            if healthy:\n",
    "                logger.info(f\"  âœ… {model}\")\n",
    "            else:\n",
    "                logger.warning(f\"  âŒ {model}: {error}\")\n",
    "        \n",
    "        return warmup_results\n",
    "    \n",
    "    def get_model_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about model usage and health\"\"\"\n",
    "        with self.health_lock:\n",
    "            stats = {}\n",
    "            for model, health in self.model_health.items():\n",
    "                total = health[\"success\"] + health[\"failures\"]\n",
    "                success_rate = health[\"success\"] / total if total > 0 else 0\n",
    "                stats[model] = {\n",
    "                    \"success\": health[\"success\"],\n",
    "                    \"failures\": health[\"failures\"],\n",
    "                    \"success_rate\": success_rate,\n",
    "                    \"unhealthy\": model in self.unhealthy_models\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"model_stats\": stats,\n",
    "                \"unhealthy_models\": list(self.unhealthy_models),\n",
    "                \"cache_size\": len(self.available_models_cache) if self.available_models_cache else 0\n",
    "            }\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# ============================================================\n",
    "# LLM INTERFACE WITH CIRCUIT BREAKER\n",
    "# ============================================================\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Interface to LLM with circuit breaker protection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.circuit_breaker = OLLAMA_CIRCUIT_BREAKER\n",
    "        self.session = session\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Token tracking\n",
    "        self.total_tokens = 0\n",
    "        self.total_requests = 0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.response_times = []\n",
    "        self.max_response_time_history = 1000\n",
    "        \n",
    "        logger.info(\"LLM interface initialized with circuit breaker\")\n",
    "    \n",
    "    def _truncate_prompt_if_needed(self, prompt: str, model_config: ModelConfig) -> str:\n",
    "        \"\"\"Truncate prompt if too long for model context\"\"\"\n",
    "        prompt_len = len(prompt)\n",
    "        \n",
    "        # Get model capabilities\n",
    "        capabilities = model_manager.get_model_capability(model_config.name)\n",
    "        max_context = capabilities.get(\"estimated_context\", 4096)\n",
    "        \n",
    "        # Rough estimate: 1 token â‰ˆ 4 chars for English\n",
    "        estimated_tokens = prompt_len // 4\n",
    "        \n",
    "        if estimated_tokens > max_context * 0.8:  # 80% of context\n",
    "            logger.warning(f\"Prompt may be too long: {estimated_tokens} tokens \"\n",
    "                          f\"(model context: {max_context})\")\n",
    "            \n",
    "            # Calculate how much to truncate\n",
    "            target_chars = int(max_context * 0.7 * 4)  # 70% of context\n",
    "            if prompt_len > target_chars:\n",
    "                logger.warning(f\"Truncating prompt from {prompt_len} to {target_chars} chars\")\n",
    "                \n",
    "                # Try to truncate at a paragraph boundary\n",
    "                truncated = prompt[:target_chars]\n",
    "                last_period = truncated.rfind('.')\n",
    "                last_newline = truncated.rfind('\\n')\n",
    "                \n",
    "                if last_period > target_chars * 0.9:  # Good break point\n",
    "                    prompt = truncated[:last_period + 1]\n",
    "                elif last_newline > target_chars * 0.9:\n",
    "                    prompt = truncated[:last_newline]\n",
    "                else:\n",
    "                    prompt = truncated\n",
    "                \n",
    "                # Add instruction\n",
    "                prompt += \"\\n\\n[Content truncated due to length. Provide concise answer.]\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _prepare_payload(self, prompt: str, model_config: ModelConfig) -> Dict:\n",
    "        \"\"\"Prepare payload for Ollama API\"\"\"\n",
    "        # Truncate if needed\n",
    "        prompt = self._truncate_prompt_if_needed(prompt, model_config)\n",
    "        \n",
    "        # Create payload\n",
    "        payload = model_config.to_ollama_payload(prompt)\n",
    "        \n",
    "        # Add GPU layers if available\n",
    "        if HARDWARE_INFO[\"gpu_available\"] and model_config.gpu_layers == -1:\n",
    "            # Auto-configure GPU layers based on available memory\n",
    "            gpu_memory_gb = HARDWARE_INFO[\"gpu_memory_gb\"]\n",
    "            if gpu_memory_gb >= 16:\n",
    "                payload[\"options\"][\"num_gpu\"] = 100  # Use all layers\n",
    "            elif gpu_memory_gb >= 8:\n",
    "                payload[\"options\"][\"num_gpu\"] = 50   # Half layers\n",
    "            elif gpu_memory_gb >= 4:\n",
    "                payload[\"options\"][\"num_gpu\"] = 25   # Quarter layers\n",
    "        \n",
    "        return payload\n",
    "    \n",
    "    def generate(self, prompt: str, model_config: ModelConfig, \n",
    "                max_retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Generate text using LLM with circuit breaker protection.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to send\n",
    "            model_config: Model configuration\n",
    "            max_retries: Maximum number of retries\n",
    "            \n",
    "        Returns:\n",
    "            Generated text\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If generation fails after retries\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        def _call_ollama():\n",
    "            \"\"\"Inner function for circuit breaker protection\"\"\"\n",
    "            payload = self._prepare_payload(prompt, model_config)\n",
    "            \n",
    "            try:\n",
    "                response = self.session.request_with_retry(\n",
    "                    \"POST\",\n",
    "                    f\"{CONFIG['OLLAMA_URL']}/api/generate\",\n",
    "                    json=payload,\n",
    "                    timeout=model_config.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                result = response.json()\n",
    "                \n",
    "                # Track tokens\n",
    "                if \"prompt_eval_count\" in result:\n",
    "                    self.total_tokens += result.get(\"prompt_eval_count\", 0)\n",
    "                if \"eval_count\" in result:\n",
    "                    self.total_tokens += result.get(\"eval_count\", 0)\n",
    "                \n",
    "                self.total_requests += 1\n",
    "                \n",
    "                return result.get(\"response\", \"\")\n",
    "                \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 400:\n",
    "                    error_detail = e.response.text\n",
    "                    logger.error(f\"HTTP 400 from Ollama: {error_detail}\")\n",
    "                    \n",
    "                    # Check for common errors\n",
    "                    if \"context\" in error_detail.lower() or \"too long\" in error_detail.lower():\n",
    "                        raise RuntimeError(f\"Prompt too long for model {model_config.name}\")\n",
    "                    elif \"model not found\" in error_detail.lower():\n",
    "                        raise RuntimeError(f\"Model {model_config.name} not found. \"\n",
    "                                          f\"Pull it with: ollama pull {model_config.name}\")\n",
    "                \n",
    "                raise\n",
    "        \n",
    "        # Use circuit breaker\n",
    "        try:\n",
    "            response_text = self.circuit_breaker.call(_call_ollama)\n",
    "            \n",
    "            # Record success\n",
    "            latency = time.time() - start_time\n",
    "            self.response_times.append(latency)\n",
    "            if len(self.response_times) > self.max_response_time_history:\n",
    "                self.response_times = self.response_times[-self.max_response_time_history:]\n",
    "            \n",
    "            PROCESSING_METRICS.record_timing(\"llm_generation\", latency)\n",
    "            PROCESSING_METRICS.increment(\"llm_requests\")\n",
    "            \n",
    "            logger.debug(f\"LLM call succeeded: {model_config.name}, {latency:.2f}s\")\n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            PROCESSING_METRICS.increment(\"llm_failures\")\n",
    "            \n",
    "            error_tracker.record(\n",
    "                \"llm_generation_failed\",\n",
    "                str(e),\n",
    "                {\n",
    "                    \"model\": model_config.name,\n",
    "                    \"prompt_length\": len(prompt),\n",
    "                    \"latency\": latency,\n",
    "                    \"retries\": max_retries\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            logger.error(f\"LLM generation failed after {latency:.2f}s: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def batch_generate(self, prompts: List[str], model_config: ModelConfig,\n",
    "                      max_concurrent: int = 3) -> List[str]:\n",
    "        \"\"\"Generate multiple prompts concurrently\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n",
    "            future_to_idx = {\n",
    "                executor.submit(self.generate, prompt, model_config): i\n",
    "                for i, prompt in enumerate(prompts)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append((idx, result))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Batch generation failed for prompt {idx}: {e}\")\n",
    "                    results.append((idx, f\"ERROR: {str(e)}\"))\n",
    "        \n",
    "        # Sort by original index\n",
    "        results.sort(key=lambda x: x[0])\n",
    "        return [result for _, result in results]\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get LLM interface statistics\"\"\"\n",
    "        avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0\n",
    "        p95_response_time = sorted(self.response_times)[int(len(self.response_times) * 0.95)] if len(self.response_times) > 1 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"avg_response_time\": avg_response_time,\n",
    "            \"p95_response_time\": p95_response_time,\n",
    "            \"response_time_samples\": len(self.response_times),\n",
    "            \"circuit_breaker_state\": self.circuit_breaker.state\n",
    "        }\n",
    "\n",
    "# Initialize LLM interface\n",
    "llm_interface = LLMInterface()\n",
    "\n",
    "# ============================================================\n",
    "# JSON EXTRACTION UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def extract_json(text: str, lenient: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract JSON from LLM response with robust error handling.\n",
    "    \n",
    "    Args:\n",
    "        text: Text potentially containing JSON\n",
    "        lenient: Whether to try lenient parsing if strict fails\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON as dictionary, or empty dict if failed\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {}\n",
    "    \n",
    "    # Clean the text\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Try to find JSON object or array\n",
    "    json_match = None\n",
    "    \n",
    "    # Look for JSON object\n",
    "    obj_match = re.search(r'\\{[^{}]*\\}', text, re.DOTALL)\n",
    "    if obj_match:\n",
    "        # Try to expand to include nested objects\n",
    "        start = obj_match.start()\n",
    "        end = obj_match.end()\n",
    "        \n",
    "        # Count braces to find complete object\n",
    "        brace_count = 0\n",
    "        in_string = False\n",
    "        escape = False\n",
    "        \n",
    "        for i, char in enumerate(text[start:]):\n",
    "            if escape:\n",
    "                escape = False\n",
    "                continue\n",
    "            \n",
    "            if char == '\\\\':\n",
    "                escape = True\n",
    "            elif char == '\"' and not escape:\n",
    "                in_string = not in_string\n",
    "            elif not in_string:\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        end = start + i + 1\n",
    "                        json_match = text[start:end]\n",
    "                        break\n",
    "        \n",
    "        if not json_match and obj_match:\n",
    "            json_match = obj_match.group()\n",
    "    \n",
    "    # If no object found, look for array\n",
    "    if not json_match:\n",
    "        array_match = re.search(r'\\[[^\\[\\]]*\\]', text, re.DOTALL)\n",
    "        if array_match:\n",
    "            json_match = array_match.group()\n",
    "    \n",
    "    # If still no match, use the whole text\n",
    "    if not json_match:\n",
    "        json_match = text\n",
    "    \n",
    "    # Clean up common JSON issues\n",
    "    cleaned = json_match\n",
    "    \n",
    "    # Remove trailing commas before } or ]\n",
    "    cleaned = re.sub(r',(\\s*[}\\]])', r'\\1', cleaned)\n",
    "    \n",
    "    # Remove control characters\n",
    "    cleaned = re.sub(r'[\\x00-\\x1F\\x7F]', '', cleaned)\n",
    "    \n",
    "    # Fix unescaped quotes\n",
    "    lines = cleaned.split('\\n')\n",
    "    fixed_lines = []\n",
    "    for line in lines:\n",
    "        # Count quotes in line\n",
    "        quote_count = line.count('\"')\n",
    "        if quote_count % 2 != 0:\n",
    "            # Odd number of quotes, try to fix\n",
    "            line = line.replace('\"', \"'\")\n",
    "        fixed_lines.append(line)\n",
    "    cleaned = '\\n'.join(fixed_lines)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError as e:\n",
    "        if lenient:\n",
    "            try:\n",
    "                # Try more aggressive cleaning\n",
    "                cleaned = re.sub(r'([{\\[,])\\s*,', r'\\1', cleaned)  # Remove leading commas\n",
    "                cleaned = re.sub(r'\":\\s*,', '\": null,', cleaned)   # Fix empty values\n",
    "                cleaned = re.sub(r',\\s*\"}', '\"}', cleaned)         # Remove trailing commas\n",
    "                \n",
    "                return json.loads(cleaned)\n",
    "            except:\n",
    "                logger.warning(f\"JSON parsing failed even with lenient mode: {e}\")\n",
    "                logger.debug(f\"Problematic text: {text[:500]}...\")\n",
    "                return {}\n",
    "        else:\n",
    "            logger.warning(f\"JSON parsing failed: {e}\")\n",
    "            logger.debug(f\"Problematic text: {text[:500]}...\")\n",
    "            return {}\n",
    "\n",
    "def validate_json_structure(data: Dict, expected_structure: Dict) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate JSON structure against expected format.\n",
    "    \n",
    "    Args:\n",
    "        data: JSON data to validate\n",
    "        expected_structure: Dictionary mapping keys to expected types\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, list_of_errors)\n",
    "    \"\"\"\n",
    "    if not isinstance(data, dict):\n",
    "        return False, [\"Data is not a dictionary\"]\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for key, expected_type in expected_structure.items():\n",
    "        if key not in data:\n",
    "            errors.append(f\"Missing required key: {key}\")\n",
    "        elif expected_type == \"list\" and not isinstance(data[key], list):\n",
    "            errors.append(f\"Key '{key}' should be a list, got {type(data[key]).__name__}\")\n",
    "        elif expected_type == \"str\" and not isinstance(data[key], str):\n",
    "            errors.append(f\"Key '{key}' should be a string, got {type(data[key]).__name__}\")\n",
    "        elif expected_type == \"bool\" and not isinstance(data[key], bool):\n",
    "            errors.append(f\"Key '{key}' should be a boolean, got {type(data[key]).__name__}\")\n",
    "        elif expected_type == \"int\" and not isinstance(data[key], int):\n",
    "            errors.append(f\"Key '{key}' should be an integer, got {type(data[key]).__name__}\")\n",
    "        elif expected_type == \"float\" and not isinstance(data[key], (int, float)):\n",
    "            errors.append(f\"Key '{key}' should be a number, got {type(data[key]).__name__}\")\n",
    "        elif expected_type == \"dict\" and not isinstance(data[key], dict):\n",
    "            errors.append(f\"Key '{key}' should be a dictionary, got {type(data[key]).__name__}\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "# ============================================================\n",
    "# MODEL VALIDATION & FALLBACK\n",
    "# ============================================================\n",
    "\n",
    "def validate_and_fix_models() -> Tuple[List[str], Dict]:\n",
    "    \"\"\"\n",
    "    Validate all pipeline models and apply fallbacks if needed.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (missing_models, fixed_configs)\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating pipeline models...\")\n",
    "    \n",
    "    missing_models = []\n",
    "    fixed_configs = {}\n",
    "    \n",
    "    for stage, config in PIPELINE_MODELS.items():\n",
    "        original_model = config.name\n",
    "        purpose = \"extract\" if \"extract\" in stage else \"generate\"\n",
    "        \n",
    "        # Get best available model\n",
    "        fallbacks = MODEL_FALLBACKS.get(original_model, [])\n",
    "        selected_model = model_manager.select_best_model(original_model, fallbacks, purpose)\n",
    "        \n",
    "        if selected_model and selected_model != original_model:\n",
    "            # Update config\n",
    "            config.name = selected_model\n",
    "            fixed_configs[stage] = {\n",
    "                \"original\": original_model,\n",
    "                \"selected\": selected_model,\n",
    "                \"reason\": \"fallback_used\"\n",
    "            }\n",
    "            logger.info(f\"  {stage}: {original_model} â†’ {selected_model}\")\n",
    "        elif not selected_model:\n",
    "            missing_models.append((stage, original_model))\n",
    "            logger.error(f\"  âŒ {stage}: {original_model} - NO SUITABLE MODEL FOUND\")\n",
    "        else:\n",
    "            logger.info(f\"  âœ… {stage}: {original_model}\")\n",
    "    \n",
    "    if missing_models:\n",
    "        logger.error(f\"Missing models for stages: {missing_models}\")\n",
    "    \n",
    "    return missing_models, fixed_configs\n",
    "\n",
    "def check_ollama_health() -> bool:\n",
    "    \"\"\"Check if Ollama server is healthy\"\"\"\n",
    "    try:\n",
    "        response = session.request_with_retry(\n",
    "            \"GET\",\n",
    "            f\"{CONFIG['OLLAMA_URL']}/api/tags\",\n",
    "            timeout=5\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ollama health check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def initialize_llm_system():\n",
    "    \"\"\"Initialize the complete LLM system\"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"INITIALIZING LLM SYSTEM\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Check Ollama health\n",
    "    if not check_ollama_health():\n",
    "        logger.error(\"âŒ Ollama server not responding!\")\n",
    "        raise RuntimeError(\"Ollama server not available\")\n",
    "    \n",
    "    logger.info(\"âœ… Ollama server is healthy\")\n",
    "    \n",
    "    # Validate and fix models\n",
    "    missing_models, fixes = validate_and_fix_models()\n",
    "    \n",
    "    if missing_models:\n",
    "        logger.error(f\"âŒ Missing models: {missing_models}\")\n",
    "        # Don't fail immediately, try to continue\n",
    "    \n",
    "    # Warm up models\n",
    "    warmup_results = model_manager.warmup_models()\n",
    "    \n",
    "    # Check for critical failures\n",
    "    critical_models = [\"extract\", \"generate_basic\"]\n",
    "    critical_failed = []\n",
    "    \n",
    "    for stage in critical_models:\n",
    "        if stage in PIPELINE_MODELS:\n",
    "            model = PIPELINE_MODELS[stage].name\n",
    "            is_healthy = any(r[0] == model and r[1] for r in warmup_results)\n",
    "            if not is_healthy:\n",
    "                critical_failed.append((stage, model))\n",
    "    \n",
    "    if critical_failed:\n",
    "        logger.error(f\"âŒ Critical models failed warmup: {critical_failed}\")\n",
    "        logger.error(\"Pipeline may not function correctly\")\n",
    "    \n",
    "    # Log hardware info\n",
    "    logger.info(f\"Hardware: GPU={HARDWARE_INFO.get('gpu_available', False)}, \"\n",
    "                f\"Memory={HARDWARE_INFO.get('gpu_memory_gb', 0):.1f}GB GPU, \"\n",
    "                f\"{HARDWARE_INFO.get('total_memory_gb', 0):.1f}GB System\")\n",
    "    \n",
    "    # Log circuit breaker status\n",
    "    logger.info(f\"Circuit breaker: {OLLAMA_CIRCUIT_BREAKER.state}\")\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"LLM SYSTEM READY\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "# Run initialization\n",
    "try:\n",
    "    initialize_llm_system()\n",
    "except Exception as e:\n",
    "    logger.error(f\"LLM system initialization failed: {e}\")\n",
    "    # Continue anyway, some features may work\n",
    "\n",
    "# ============================================================\n",
    "# CONVENIENCE FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def llm(prompt: str, model_config: ModelConfig, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function for LLM generation.\n",
    "    \n",
    "    This is the main function other cells should use.\n",
    "    \"\"\"\n",
    "    return llm_interface.generate(prompt, model_config, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d69e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: PROMPT MANAGEMENT & GENERATION (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, deque\n",
    "import hashlib\n",
    "\n",
    "logger = logging.getLogger(\"anki-pipeline.prompts\")\n",
    "\n",
    "# ============================================================\n",
    "# PROGRESS TRACKER (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    Track processed reels with atomic file operations and buffer management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, progress_file: Path):\n",
    "        self.progress_file = progress_file\n",
    "        self.processed: Set[str] = self._load()\n",
    "        self.lock = Lock()\n",
    "        self._buffer: List[str] = []\n",
    "        self._buffer_size = CONFIG.get(\"PROGRESS_BUFFER_SIZE\", 50)\n",
    "        self._last_flush = time.time()\n",
    "        self.flush_interval = 30  # seconds\n",
    "        \n",
    "        # Statistics\n",
    "        self.loaded_count = len(self.processed)\n",
    "        logger.info(f\"Progress tracker initialized: {self.loaded_count:,} reels already processed\")\n",
    "    \n",
    "    def _load(self) -> Set[str]:\n",
    "        \"\"\"Load processed reels from file\"\"\"\n",
    "        if not self.progress_file.exists():\n",
    "            return set()\n",
    "        \n",
    "        try:\n",
    "            # Read with error handling\n",
    "            with open(self.progress_file, 'r', encoding='utf-8') as f:\n",
    "                lines = []\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        if ',' in line:  # Handle CSV-style if needed\n",
    "                            parts = line.split(',')\n",
    "                            if parts:\n",
    "                                lines.append(parts[0].strip())\n",
    "                        else:\n",
    "                            lines.append(line)\n",
    "                \n",
    "                loaded_set = set(lines)\n",
    "                logger.debug(f\"Loaded {len(loaded_set):,} processed reels from {self.progress_file}\")\n",
    "                return loaded_set\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading progress file {self.progress_file}: {e}\")\n",
    "            \n",
    "            # Try backup if available\n",
    "            backup_file = self.progress_file.with_suffix('.progress.bak')\n",
    "            if backup_file.exists():\n",
    "                logger.info(f\"Trying backup file: {backup_file}\")\n",
    "                try:\n",
    "                    with open(backup_file, 'r') as f:\n",
    "                        loaded_set = set(line.strip() for line in f if line.strip())\n",
    "                    logger.info(f\"Recovered {len(loaded_set):,} reels from backup\")\n",
    "                    return loaded_set\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return set()\n",
    "    \n",
    "    def mark_processed(self, reel_id: str):\n",
    "        \"\"\"Mark a reel as processed (buffered write)\"\"\"\n",
    "        with self.lock:\n",
    "            if reel_id and reel_id not in self.processed:\n",
    "                self.processed.add(reel_id)\n",
    "                self._buffer.append(reel_id)\n",
    "                \n",
    "                # Check if we should flush\n",
    "                should_flush = (\n",
    "                    len(self._buffer) >= self._buffer_size or\n",
    "                    (time.time() - self._last_flush) >= self.flush_interval\n",
    "                )\n",
    "                \n",
    "                if should_flush:\n",
    "                    self.flush()\n",
    "    \n",
    "    def flush(self):\n",
    "        \"\"\"Flush buffer to disk\"\"\"\n",
    "        if not self._buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create backup first\n",
    "            if self.progress_file.exists():\n",
    "                backup_file = self.progress_file.with_suffix('.progress.bak')\n",
    "                self.progress_file.rename(backup_file)\n",
    "            \n",
    "            # Append new entries\n",
    "            with open(self.progress_file, 'a', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(self._buffer) + '\\n')\n",
    "            \n",
    "            self._buffer.clear()\n",
    "            self._last_flush = time.time()\n",
    "            logger.debug(f\"Progress flushed: {len(self._buffer)} entries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error flushing progress: {e}\")\n",
    "            \n",
    "            # Try to restore backup\n",
    "            backup_file = self.progress_file.with_suffix('.progress.bak')\n",
    "            if backup_file.exists():\n",
    "                try:\n",
    "                    backup_file.rename(self.progress_file)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    def is_processed(self, reel_id: str) -> bool:\n",
    "        \"\"\"Check if reel has been processed\"\"\"\n",
    "        with self.lock:\n",
    "            return reel_id in self.processed\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get progress statistics\"\"\"\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"total_processed\": len(self.processed),\n",
    "                \"buffer_size\": len(self._buffer),\n",
    "                \"last_flush\": self._last_flush,\n",
    "                \"loaded_count\": self.loaded_count\n",
    "            }\n",
    "    \n",
    "    def save_backup(self):\n",
    "        \"\"\"Create a backup of progress file\"\"\"\n",
    "        try:\n",
    "            if self.progress_file.exists():\n",
    "                backup_file = self.progress_file.with_suffix(f'.progress.{int(time.time())}.bak')\n",
    "                import shutil\n",
    "                shutil.copy2(self.progress_file, backup_file)\n",
    "                logger.debug(f\"Progress backup created: {backup_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating progress backup: {e}\")\n",
    "\n",
    "# Initialize progress tracker\n",
    "progress_tracker = ProgressTracker(PROGRESS_FILE)\n",
    "\n",
    "# ============================================================\n",
    "# CACHE MANAGEMENT\n",
    "# ============================================================\n",
    "\n",
    "class PipelineCache:\n",
    "    \"\"\"\n",
    "    Cache for intermediate pipeline results with versioning and invalidation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Cache statistics\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.writes = 0\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Version for cache invalidation\n",
    "        self.cache_version = CONFIG.get(\"CACHE_VERSION\", \"v1\")\n",
    "        \n",
    "        logger.info(f\"Pipeline cache initialized at {cache_dir}, version: {self.cache_version}\")\n",
    "    \n",
    "    def _get_cache_key(self, reel_id: str, stage: str) -> str:\n",
    "        \"\"\"Generate cache key with version\"\"\"\n",
    "        # Create deterministic key\n",
    "        key_data = f\"{reel_id}::{stage}::{self.cache_version}\"\n",
    "        return hashlib.md5(key_data.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file(self, reel_id: str, stage: str) -> Path:\n",
    "        \"\"\"Get cache file path\"\"\"\n",
    "        cache_key = self._get_cache_key(reel_id, stage)\n",
    "        return self.cache_dir / f\"{cache_key}.json\"\n",
    "    \n",
    "    def get(self, reel_id: str, stage: str) -> Optional[Dict]:\n",
    "        \"\"\"Get cached result\"\"\"\n",
    "        cache_file = self._get_cache_file(reel_id, stage)\n",
    "        \n",
    "        if not cache_file.exists():\n",
    "            with self.lock:\n",
    "                self.misses += 1\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(cache_file, default=None)\n",
    "            if data:\n",
    "                # Check if cache is still valid\n",
    "                cache_timestamp = data.get(\"_cache_timestamp\", 0)\n",
    "                cache_ttl = data.get(\"_cache_ttl\", 7 * 24 * 3600)  # Default 7 days\n",
    "                \n",
    "                if time.time() - cache_timestamp < cache_ttl:\n",
    "                    with self.lock:\n",
    "                        self.hits += 1\n",
    "                    return data\n",
    "                else:\n",
    "                    logger.debug(f\"Cache expired for {reel_id}::{stage}\")\n",
    "                    cache_file.unlink(missing_ok=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error reading cache {cache_file}: {e}\")\n",
    "        \n",
    "        with self.lock:\n",
    "            self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, reel_id: str, stage: str, data: Dict, ttl: int = 7 * 24 * 3600):\n",
    "        \"\"\"Set cached result with TTL\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Add cache metadata\n",
    "            data_with_meta = data.copy()\n",
    "            data_with_meta[\"_cache_timestamp\"] = time.time()\n",
    "            data_with_meta[\"_cache_ttl\"] = ttl\n",
    "            data_with_meta[\"_cache_version\"] = self.cache_version\n",
    "            data_with_meta[\"_reel_id\"] = reel_id\n",
    "            data_with_meta[\"_stage\"] = stage\n",
    "            \n",
    "            cache_file = self._get_cache_file(reel_id, stage)\n",
    "            atomic_write(data_with_meta, cache_file)\n",
    "            \n",
    "            with self.lock:\n",
    "                self.writes += 1\n",
    "            \n",
    "            logger.debug(f\"Cached {reel_id}::{stage}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing cache for {reel_id}::{stage}: {e}\")\n",
    "    \n",
    "    def invalidate(self, reel_id: str = None, stage: str = None):\n",
    "        \"\"\"Invalidate cache entries\"\"\"\n",
    "        try:\n",
    "            if reel_id and stage:\n",
    "                # Invalidate specific entry\n",
    "                cache_file = self._get_cache_file(reel_id, stage)\n",
    "                cache_file.unlink(missing_ok=True)\n",
    "                logger.debug(f\"Invalidated cache for {reel_id}::{stage}\")\n",
    "            \n",
    "            elif reel_id:\n",
    "                # Invalidate all entries for reel\n",
    "                pattern = f\"*{reel_id}*\" if \"*\" not in reel_id else reel_id\n",
    "                for cache_file in self.cache_dir.glob(pattern):\n",
    "                    cache_file.unlink(missing_ok=True)\n",
    "                logger.debug(f\"Invalidated all cache for reel {reel_id}\")\n",
    "            \n",
    "            elif stage:\n",
    "                # Invalidate all entries for stage\n",
    "                pattern = f\"*::{stage}::*\"\n",
    "                for cache_file in self.cache_dir.glob(\"*\"):\n",
    "                    try:\n",
    "                        if stage in cache_file.name:\n",
    "                            cache_file.unlink(missing_ok=True)\n",
    "                    except:\n",
    "                        pass\n",
    "                logger.debug(f\"Invalidated all cache for stage {stage}\")\n",
    "            \n",
    "            else:\n",
    "                # Invalidate all cache\n",
    "                for cache_file in self.cache_dir.glob(\"*\"):\n",
    "                    cache_file.unlink(missing_ok=True)\n",
    "                logger.info(\"Invalidated all cache\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error invalidating cache: {e}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        with self.lock:\n",
    "            total = self.hits + self.misses\n",
    "            hit_rate = self.hits / total if total > 0 else 0\n",
    "            \n",
    "            # Count cache files\n",
    "            cache_files = list(self.cache_dir.glob(\"*.json\"))\n",
    "            cache_size_mb = sum(f.stat().st_size for f in cache_files) / (1024 * 1024)\n",
    "            \n",
    "            return {\n",
    "                \"hits\": self.hits,\n",
    "                \"misses\": self.misses,\n",
    "                \"writes\": self.writes,\n",
    "                \"hit_rate\": hit_rate,\n",
    "                \"cache_files\": len(cache_files),\n",
    "                \"cache_size_mb\": round(cache_size_mb, 2),\n",
    "                \"cache_dir\": str(self.cache_dir),\n",
    "                \"cache_version\": self.cache_version\n",
    "            }\n",
    "    \n",
    "    def cleanup_old_entries(self, max_age_days: int = 30):\n",
    "        \"\"\"Cleanup old cache entries\"\"\"\n",
    "        try:\n",
    "            now = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 3600\n",
    "            deleted = 0\n",
    "            \n",
    "            for cache_file in self.cache_dir.glob(\"*.json\"):\n",
    "                try:\n",
    "                    # Read cache timestamp without loading entire file\n",
    "                    data = atomic_read(cache_file, default={})\n",
    "                    cache_timestamp = data.get(\"_cache_timestamp\", 0)\n",
    "                    \n",
    "                    if now - cache_timestamp > max_age_seconds:\n",
    "                        cache_file.unlink()\n",
    "                        deleted += 1\n",
    "                except:\n",
    "                    # If can't read, delete anyway\n",
    "                    cache_file.unlink(missing_ok=True)\n",
    "                    deleted += 1\n",
    "            \n",
    "            if deleted > 0:\n",
    "                logger.info(f\"Cleaned up {deleted} old cache entries (> {max_age_days} days)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning up cache: {e}\")\n",
    "\n",
    "# Initialize cache\n",
    "pipeline_cache = PipelineCache(CONFIG[\"CACHE_DIR\"])\n",
    "\n",
    "# Convenience functions\n",
    "def get_cached_result(reel_id: str, stage: str) -> Optional[Dict]:\n",
    "    \"\"\"Get cached result (convenience wrapper)\"\"\"\n",
    "    return pipeline_cache.get(reel_id, stage)\n",
    "\n",
    "def save_cached_result(reel_id: str, stage: str, data: Dict, ttl: int = 7 * 24 * 3600):\n",
    "    \"\"\"Save result to cache (convenience wrapper)\"\"\"\n",
    "    pipeline_cache.set(reel_id, stage, data, ttl)\n",
    "\n",
    "# ============================================================\n",
    "# PROMPT VERSION LIFECYCLE MANAGEMENT\n",
    "# ============================================================\n",
    "\n",
    "class PromptVersionManager:\n",
    "    \"\"\"\n",
    "    Manage prompt versions with auto-deprecation and success tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stats_file: Path):\n",
    "        self.stats_file = stats_file\n",
    "        self.stats = self._load_stats()\n",
    "        self.lock = Lock()\n",
    "        self._dirty = False\n",
    "        \n",
    "        # Deprecation thresholds\n",
    "        self.min_attempts_for_deprecation = 8\n",
    "        self.deprecation_threshold = 0.35  # 35% success rate\n",
    "        self.recovery_threshold = 0.45     # 45% success rate to recover\n",
    "        self.recovery_check_interval = 50   # Check every 50 attempts\n",
    "        \n",
    "        logger.info(f\"Prompt version manager initialized: {len(self.stats):,} versions tracked\")\n",
    "    \n",
    "    def _load_stats(self) -> Dict:\n",
    "        \"\"\"Load prompt version statistics\"\"\"\n",
    "        if not self.stats_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(self.stats_file, default={})\n",
    "            return data.get(\"versions\", {})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading prompt version stats: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_stats(self):\n",
    "        \"\"\"Save prompt version statistics\"\"\"\n",
    "        if not self._dirty:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data = {\n",
    "                \"versions\": self.stats,\n",
    "                \"last_updated\": time.time(),\n",
    "                \"total_versions\": len(self.stats)\n",
    "            }\n",
    "            atomic_write(data, self.stats_file)\n",
    "            self._dirty = False\n",
    "            logger.debug(f\"Saved prompt version stats: {len(self.stats):,} versions\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving prompt version stats: {e}\")\n",
    "    \n",
    "    def record_result(self, version: str, success: bool, stage: str = \"unknown\", \n",
    "                     details: Dict = None):\n",
    "        \"\"\"\n",
    "        Record prompt version result.\n",
    "        \n",
    "        Args:\n",
    "            version: Prompt version identifier\n",
    "            success: Whether the prompt was successful\n",
    "            stage: Which pipeline stage\n",
    "            details: Additional details about the attempt\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if version not in self.stats:\n",
    "                self.stats[version] = {\n",
    "                    \"attempts\": 0,\n",
    "                    \"successes\": 0,\n",
    "                    \"mechanical_failures\": 0,\n",
    "                    \"deprecated\": False,\n",
    "                    \"success_rate\": 0.0,\n",
    "                    \"last_failure_stage\": None,\n",
    "                    \"last_success_timestamp\": None,\n",
    "                    \"first_seen\": time.time(),\n",
    "                    \"stage_stats\": defaultdict(lambda: {\"attempts\": 0, \"successes\": 0}),\n",
    "                    \"recent_attempts\": deque(maxlen=20),  # Track recent attempts\n",
    "                    \"metadata\": details or {}\n",
    "                }\n",
    "            \n",
    "            stats = self.stats[version]\n",
    "            \n",
    "            # Mechanical failures don't count against prompt quality\n",
    "            if stage == \"mechanical\":\n",
    "                stats[\"mechanical_failures\"] += 1\n",
    "                self._dirty = True\n",
    "                return\n",
    "            \n",
    "            # Record attempt\n",
    "            stats[\"attempts\"] += 1\n",
    "            stats[\"recent_attempts\"].append((success, time.time()))\n",
    "            \n",
    "            # Update stage statistics\n",
    "            stats[\"stage_stats\"][stage][\"attempts\"] += 1\n",
    "            if success:\n",
    "                stats[\"successes\"] += 1\n",
    "                stats[\"stage_stats\"][stage][\"successes\"] += 1\n",
    "                stats[\"last_success_timestamp\"] = time.time()\n",
    "            else:\n",
    "                stats[\"last_failure_stage\"] = stage\n",
    "            \n",
    "            # Calculate success rate\n",
    "            if stats[\"attempts\"] > 0:\n",
    "                stats[\"success_rate\"] = stats[\"successes\"] / stats[\"attempts\"]\n",
    "            \n",
    "            # Auto-deprecation logic\n",
    "            if (stats[\"attempts\"] >= self.min_attempts_for_deprecation and\n",
    "                stats[\"success_rate\"] < self.deprecation_threshold and\n",
    "                not stats[\"deprecated\"]):\n",
    "                \n",
    "                stats[\"deprecated\"] = True\n",
    "                logger.warning(f\"ðŸš« Auto-deprecated prompt version '{version}' \"\n",
    "                              f\"(success rate: {stats['success_rate']:.1%} after {stats['attempts']} attempts)\")\n",
    "            \n",
    "            # Recovery logic for deprecated prompts\n",
    "            if (stats[\"deprecated\"] and \n",
    "                stats[\"attempts\"] % self.recovery_check_interval == 0):\n",
    "                \n",
    "                if stats[\"success_rate\"] >= self.recovery_threshold:\n",
    "                    stats[\"deprecated\"] = False\n",
    "                    logger.info(f\"â™»ï¸  Un-deprecated prompt version '{version}' \"\n",
    "                               f\"(success rate recovered: {stats['success_rate']:.1%})\")\n",
    "            \n",
    "            # Periodic save\n",
    "            if stats[\"attempts\"] % 10 == 0:\n",
    "                self._dirty = True\n",
    "            \n",
    "            self._dirty = True\n",
    "    \n",
    "    def is_deprecated(self, version: str) -> bool:\n",
    "        \"\"\"Check if a prompt version is deprecated\"\"\"\n",
    "        with self.lock:\n",
    "            if version not in self.stats:\n",
    "                return False\n",
    "            \n",
    "            return self.stats[version].get(\"deprecated\", False)\n",
    "    \n",
    "    def get_version_stats(self, version: str) -> Optional[Dict]:\n",
    "        \"\"\"Get statistics for a specific version\"\"\"\n",
    "        with self.lock:\n",
    "            if version not in self.stats:\n",
    "                return None\n",
    "            \n",
    "            stats = self.stats[version].copy()\n",
    "            \n",
    "            # Calculate recent success rate (last 10 attempts)\n",
    "            recent_attempts = list(stats.get(\"recent_attempts\", []))\n",
    "            if recent_attempts:\n",
    "                recent_successes = sum(1 for success, _ in recent_attempts if success)\n",
    "                stats[\"recent_success_rate\"] = recent_successes / len(recent_attempts)\n",
    "            \n",
    "            # Convert stage_stats defaultdict to regular dict\n",
    "            stats[\"stage_stats\"] = dict(stats[\"stage_stats\"])\n",
    "            \n",
    "            return stats\n",
    "    \n",
    "    def get_best_version_for_stage(self, stage: str, min_attempts: int = 10) -> Optional[str]:\n",
    "        \"\"\"Get the best prompt version for a specific stage\"\"\"\n",
    "        with self.lock:\n",
    "            best_version = None\n",
    "            best_success_rate = -1\n",
    "            \n",
    "            for version, stats in self.stats.items():\n",
    "                stage_stat = stats[\"stage_stats\"].get(stage, {\"attempts\": 0, \"successes\": 0})\n",
    "                \n",
    "                if (stage_stat[\"attempts\"] >= min_attempts and\n",
    "                    not stats[\"deprecated\"]):\n",
    "                    \n",
    "                    success_rate = stage_stat[\"successes\"] / stage_stat[\"attempts\"]\n",
    "                    \n",
    "                    if success_rate > best_success_rate:\n",
    "                        best_success_rate = success_rate\n",
    "                        best_version = version\n",
    "            \n",
    "            if best_version:\n",
    "                logger.debug(f\"Best version for {stage}: {best_version} ({best_success_rate:.1%})\")\n",
    "            \n",
    "            return best_version\n",
    "    \n",
    "    def get_overall_stats(self) -> Dict:\n",
    "        \"\"\"Get overall statistics\"\"\"\n",
    "        with self.lock:\n",
    "            total_versions = len(self.stats)\n",
    "            active_versions = sum(1 for v in self.stats.values() if not v[\"deprecated\"])\n",
    "            deprecated_versions = total_versions - active_versions\n",
    "            \n",
    "            # Calculate aggregate success rate\n",
    "            total_attempts = sum(v[\"attempts\"] for v in self.stats.values())\n",
    "            total_successes = sum(v[\"successes\"] for v in self.stats.values())\n",
    "            overall_success_rate = total_successes / total_attempts if total_attempts > 0 else 0\n",
    "            \n",
    "            # Success rate by stage\n",
    "            stage_stats = defaultdict(lambda: {\"attempts\": 0, \"successes\": 0})\n",
    "            for version_stats in self.stats.values():\n",
    "                for stage, stats in version_stats[\"stage_stats\"].items():\n",
    "                    stage_stats[stage][\"attempts\"] += stats[\"attempts\"]\n",
    "                    stage_stats[stage][\"successes\"] += stats[\"successes\"]\n",
    "            \n",
    "            # Calculate success rates\n",
    "            stage_success_rates = {}\n",
    "            for stage, stats in stage_stats.items():\n",
    "                if stats[\"attempts\"] > 0:\n",
    "                    stage_success_rates[stage] = stats[\"successes\"] / stats[\"attempts\"]\n",
    "            \n",
    "            return {\n",
    "                \"total_versions\": total_versions,\n",
    "                \"active_versions\": active_versions,\n",
    "                \"deprecated_versions\": deprecated_versions,\n",
    "                \"overall_success_rate\": overall_success_rate,\n",
    "                \"total_attempts\": total_attempts,\n",
    "                \"total_successes\": total_successes,\n",
    "                \"stage_success_rates\": stage_success_rates,\n",
    "                \"top_versions\": self._get_top_versions(5)\n",
    "            }\n",
    "    \n",
    "    def _get_top_versions(self, n: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get top n versions by success rate\"\"\"\n",
    "        versions_with_stats = []\n",
    "        \n",
    "        for version, stats in self.stats.items():\n",
    "            if stats[\"attempts\"] >= 10:  # Only consider versions with enough attempts\n",
    "                versions_with_stats.append({\n",
    "                    \"version\": version,\n",
    "                    \"success_rate\": stats[\"success_rate\"],\n",
    "                    \"attempts\": stats[\"attempts\"],\n",
    "                    \"deprecated\": stats[\"deprecated\"],\n",
    "                    \"last_success\": stats.get(\"last_success_timestamp\")\n",
    "                })\n",
    "        \n",
    "        # Sort by success rate, then by attempts\n",
    "        versions_with_stats.sort(key=lambda x: (x[\"success_rate\"], x[\"attempts\"]), reverse=True)\n",
    "        \n",
    "        return versions_with_stats[:n]\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save statistics to disk\"\"\"\n",
    "        with self.lock:\n",
    "            self._save_stats()\n",
    "\n",
    "# Initialize prompt version manager\n",
    "prompt_version_manager = PromptVersionManager(PROMPT_VERSION_FILE)\n",
    "\n",
    "# Convenience functions\n",
    "def record_prompt_version_result(version: str, success: bool, stage: str = \"unknown\", details: Dict = None):\n",
    "    \"\"\"Record prompt version result (convenience wrapper)\"\"\"\n",
    "    prompt_version_manager.record_result(version, success, stage, details)\n",
    "\n",
    "def is_prompt_version_deprecated(version: str) -> bool:\n",
    "    \"\"\"Check if prompt version is deprecated (convenience wrapper)\"\"\"\n",
    "    return prompt_version_manager.is_deprecated(version)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIDENCE CALIBRATION SYSTEM\n",
    "# ============================================================\n",
    "\n",
    "class ConfidenceCalibrator:\n",
    "    \"\"\"\n",
    "    Calibrate confidence scores based on historical accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, calibration_file: Path):\n",
    "        self.calibration_file = calibration_file\n",
    "        self.calibration_data = self._load_calibration()\n",
    "        self.lock = Lock()\n",
    "        self._dirty = False\n",
    "        \n",
    "        # Calibration parameters\n",
    "        self.min_samples_per_bucket = 10\n",
    "        self.calibration_update_interval = 10  # Update every 10 samples\n",
    "        \n",
    "        logger.info(\"Confidence calibrator initialized\")\n",
    "    \n",
    "    def _load_calibration(self) -> Dict:\n",
    "        \"\"\"Load calibration data\"\"\"\n",
    "        if not self.calibration_file.exists():\n",
    "            return CONFIDENCE_CALIBRATION  # Default from CELL 2\n",
    "        \n",
    "        try:\n",
    "            data = atomic_read(self.calibration_file, default={})\n",
    "            return data.get(\"calibration\", CONFIDENCE_CALIBRATION)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading confidence calibration: {e}\")\n",
    "            return CONFIDENCE_CALIBRATION\n",
    "    \n",
    "    def _save_calibration(self):\n",
    "        \"\"\"Save calibration data\"\"\"\n",
    "        if not self._dirty:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data = {\n",
    "                \"calibration\": self.calibration_data,\n",
    "                \"last_updated\": time.time(),\n",
    "                \"version\": \"v2\"\n",
    "            }\n",
    "            atomic_write(data, self.calibration_file)\n",
    "            self._dirty = False\n",
    "            logger.debug(\"Saved confidence calibration\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving confidence calibration: {e}\")\n",
    "    \n",
    "    def _get_bucket(self, confidence: float) -> Optional[str]:\n",
    "        \"\"\"Get bucket for confidence score\"\"\"\n",
    "        if 0.5 <= confidence < 0.6:\n",
    "            return \"0.5-0.6\"\n",
    "        elif 0.6 <= confidence < 0.7:\n",
    "            return \"0.6-0.7\"\n",
    "        elif 0.7 <= confidence < 0.8:\n",
    "            return \"0.7-0.8\"\n",
    "        elif 0.8 <= confidence < 0.9:\n",
    "            return \"0.8-0.9\"\n",
    "        elif 0.9 <= confidence <= 1.0:\n",
    "            return \"0.9-1.0\"\n",
    "        return None\n",
    "    \n",
    "    def track_outcome(self, confidence: float, accepted: bool, \n",
    "                     topic_class: str = \"\", details: Dict = None):\n",
    "        \"\"\"\n",
    "        Track confidence outcome for calibration.\n",
    "        \n",
    "        Args:\n",
    "            confidence: Predicted confidence score\n",
    "            accepted: Whether the card was actually accepted\n",
    "            topic_class: Topic classification for stratified calibration\n",
    "            details: Additional context\n",
    "        \"\"\"\n",
    "        bucket = self._get_bucket(confidence)\n",
    "        if not bucket:\n",
    "            return\n",
    "        \n",
    "        with self.lock:\n",
    "            # Initialize bucket if needed\n",
    "            if \"buckets\" not in self.calibration_data:\n",
    "                self.calibration_data[\"buckets\"] = {}\n",
    "            \n",
    "            if bucket not in self.calibration_data[\"buckets\"]:\n",
    "                self.calibration_data[\"buckets\"][bucket] = {\n",
    "                    \"total\": 0,\n",
    "                    \"accepted\": 0,\n",
    "                    \"by_topic_class\": defaultdict(lambda: {\"total\": 0, \"accepted\": 0})\n",
    "                }\n",
    "            \n",
    "            bucket_data = self.calibration_data[\"buckets\"][bucket]\n",
    "            bucket_data[\"total\"] += 1\n",
    "            if accepted:\n",
    "                bucket_data[\"accepted\"] += 1\n",
    "            \n",
    "            # Track by topic class if provided\n",
    "            if topic_class:\n",
    "                if \"by_topic_class\" not in bucket_data:\n",
    "                    bucket_data[\"by_topic_class\"] = defaultdict(lambda: {\"total\": 0, \"accepted\": 0})\n",
    "                \n",
    "                topic_stats = bucket_data[\"by_topic_class\"][topic_class]\n",
    "                topic_stats[\"total\"] += 1\n",
    "                if accepted:\n",
    "                    topic_stats[\"accepted\"] += 1\n",
    "            \n",
    "            # Track overall statistics\n",
    "            if \"overall\" not in self.calibration_data:\n",
    "                self.calibration_data[\"overall\"] = {\"total\": 0, \"accepted\": 0}\n",
    "            \n",
    "            self.calibration_data[\"overall\"][\"total\"] += 1\n",
    "            if accepted:\n",
    "                self.calibration_data[\"overall\"][\"accepted\"] += 1\n",
    "            \n",
    "            # Update calibration factor periodically\n",
    "            if bucket_data[\"total\"] % self.calibration_update_interval == 0:\n",
    "                self._update_calibration_factor()\n",
    "            \n",
    "            self._dirty = True\n",
    "    \n",
    "    def _update_calibration_factor(self):\n",
    "        \"\"\"Update global calibration factor based on all buckets\"\"\"\n",
    "        if \"buckets\" not in self.calibration_data:\n",
    "            return\n",
    "        \n",
    "        total_samples = 0\n",
    "        total_discrepancy = 0.0\n",
    "        \n",
    "        for bucket_name, bucket_data in self.calibration_data[\"buckets\"].items():\n",
    "            if bucket_data[\"total\"] >= self.min_samples_per_bucket:\n",
    "                # Parse bucket range\n",
    "                low, high = map(float, bucket_name.split('-'))\n",
    "                expected_accuracy = (low + high) / 2\n",
    "                \n",
    "                actual_accuracy = bucket_data[\"accepted\"] / bucket_data[\"total\"]\n",
    "                \n",
    "                # Weight discrepancy by number of samples\n",
    "                weight = bucket_data[\"total\"]\n",
    "                discrepancy = (actual_accuracy - expected_accuracy) * weight\n",
    "                \n",
    "                total_discrepancy += discrepancy\n",
    "                total_samples += weight\n",
    "        \n",
    "        if total_samples > 0:\n",
    "            avg_discrepancy = total_discrepancy / total_samples\n",
    "            self.calibration_data[\"calibration_factor\"] = 1.0 - avg_discrepancy * 0.5\n",
    "            \n",
    "            logger.debug(f\"Updated calibration factor: {self.calibration_data['calibration_factor']:.3f}\")\n",
    "    \n",
    "    def calibrate_confidence(self, raw_confidence: float, topic_class: str = \"\") -> float:\n",
    "        \"\"\"\n",
    "        Calibrate confidence score based on historical accuracy.\n",
    "        \n",
    "        Args:\n",
    "            raw_confidence: Raw confidence score (0-1)\n",
    "            topic_class: Optional topic class for stratified calibration\n",
    "            \n",
    "        Returns:\n",
    "            Calibrated confidence score\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            # Apply global calibration factor\n",
    "            calibration_factor = self.calibration_data.get(\"calibration_factor\", 1.0)\n",
    "            calibrated = raw_confidence * calibration_factor\n",
    "            \n",
    "            # Apply topic-specific adjustment if available\n",
    "            if topic_class and \"buckets\" in self.calibration_data:\n",
    "                bucket = self._get_bucket(raw_confidence)\n",
    "                if bucket and bucket in self.calibration_data[\"buckets\"]:\n",
    "                    bucket_data = self.calibration_data[\"buckets\"][bucket]\n",
    "                    \n",
    "                    if (\"by_topic_class\" in bucket_data and \n",
    "                        topic_class in bucket_data[\"by_topic_class\"]):\n",
    "                        \n",
    "                        topic_stats = bucket_data[\"by_topic_class\"][topic_class]\n",
    "                        if topic_stats[\"total\"] >= 5:\n",
    "                            topic_accuracy = topic_stats[\"accepted\"] / topic_stats[\"total\"]\n",
    "                            \n",
    "                            # Adjust based on topic accuracy\n",
    "                            low, high = map(float, bucket.split('-'))\n",
    "                            expected_accuracy = (low + high) / 2\n",
    "                            \n",
    "                            adjustment = (topic_accuracy - expected_accuracy) * 0.3\n",
    "                            calibrated = min(max(calibrated + adjustment, 0), 1)\n",
    "            \n",
    "            return min(max(calibrated, 0), 1)  # Clamp to [0, 1]\n",
    "    \n",
    "    def get_calibration_stats(self) -> Dict:\n",
    "        \"\"\"Get calibration statistics\"\"\"\n",
    "        with self.lock:\n",
    "            stats = self.calibration_data.copy()\n",
    "            \n",
    "            # Calculate calibration quality\n",
    "            if \"buckets\" in stats:\n",
    "                calibration_quality = []\n",
    "                for bucket_name, bucket_data in stats[\"buckets\"].items():\n",
    "                    if bucket_data[\"total\"] >= self.min_samples_per_bucket:\n",
    "                        low, high = map(float, bucket_name.split('-'))\n",
    "                        expected = (low + high) / 2\n",
    "                        actual = bucket_data[\"accepted\"] / bucket_data[\"total\"]\n",
    "                        calibration_quality.append(abs(actual - expected))\n",
    "                \n",
    "                if calibration_quality:\n",
    "                    stats[\"avg_calibration_error\"] = sum(calibration_quality) / len(calibration_quality)\n",
    "            \n",
    "            # Add metadata\n",
    "            stats[\"calibration_factor\"] = stats.get(\"calibration_factor\", 1.0)\n",
    "            stats[\"total_samples\"] = stats.get(\"overall\", {}).get(\"total\", 0)\n",
    "            \n",
    "            return stats\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save calibration data\"\"\"\n",
    "        with self.lock:\n",
    "            self._save_calibration()\n",
    "\n",
    "# Initialize confidence calibrator\n",
    "confidence_calibrator = ConfidenceCalibrator(CONFIDENCE_CALIBRATION_FILE)\n",
    "\n",
    "# Convenience functions\n",
    "def track_confidence_outcome(confidence: float, accepted: bool, topic_class: str = \"\", details: Dict = None):\n",
    "    \"\"\"Track confidence outcome (convenience wrapper)\"\"\"\n",
    "    confidence_calibrator.track_outcome(confidence, accepted, topic_class, details)\n",
    "\n",
    "def calibrate_confidence(raw_confidence: float, topic_class: str = \"\") -> float:\n",
    "    \"\"\"Calibrate confidence score (convenience wrapper)\"\"\"\n",
    "    return confidence_calibrator.calibrate_confidence(raw_confidence, topic_class)\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 0: TRANSCRIPT NORMALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def normalize_transcript(transcript: str, caption: str) -> str:\n",
    "    \"\"\"\n",
    "    PHASE 2: Transform spoken Instagram Reel transcript into structured content.\n",
    "    Converts conversational/spoken content into declarative educational prose.\n",
    "    \n",
    "    Args:\n",
    "        transcript: Raw transcript text\n",
    "        caption: Video caption for context\n",
    "        \n",
    "    Returns:\n",
    "        Normalized transcript\n",
    "    \"\"\"\n",
    "    if not CONFIG.get(\"ENABLE_TRANSCRIPT_NORMALIZATION\", True):\n",
    "        return transcript\n",
    "    \n",
    "    if not transcript or len(transcript.strip()) < 50:\n",
    "        return transcript\n",
    "    \n",
    "    # Check if normalization is needed\n",
    "    filler_count = (\n",
    "        transcript.lower().count(\"so \") + \n",
    "        transcript.lower().count(\"well \") + \n",
    "        transcript.lower().count(\"let's \") + \n",
    "        transcript.lower().count(\"hello \") + \n",
    "        transcript.lower().count(\" follow \") + \n",
    "        transcript.lower().count(\" follow @\")\n",
    "    )\n",
    "    \n",
    "    # If already structured and minimal fillers, skip normalization\n",
    "    if filler_count < 2 and len(transcript.split()) > 100:\n",
    "        return transcript\n",
    "    \n",
    "    logger.debug(f\"Normalizing transcript ({len(transcript)} chars, {filler_count} fillers)\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a technical content normalizer for educational flashcards. Transform spoken Instagram Reel transcripts into structured technical prose.\n",
    "\n",
    "INPUT TRANSCRIPT (spoken/conversational):\n",
    "{transcript}\n",
    "\n",
    "TOPIC/CAPTION:\n",
    "{caption}\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Convert speech â†’ declarative statements\n",
    "2. Remove ALL non-technical content:\n",
    "   - Filler: \"so\", \"well\", \"hello\", \"let's see\", \"right?\", \"okay\"\n",
    "   - Self-reference: \"I have created\", \"I will show\", \"comment link\", \"DM for\"\n",
    "   - CTAs: \"go ahead\", \"link in bio\", \"check out\", \"share with your friends\"\n",
    "3. Extract ONLY what was explicitly said or clearly implied\n",
    "4. Preserve: ALL technical terms, code examples, specific numbers/metrics\n",
    "5. Format as neutral technical explanation\n",
    "\n",
    "OUTPUT FORMAT (JSON):\n",
    "{{\n",
    "  \"normalized_content\": \"Technical explanation in declarative prose\",\n",
    "  \"key_concepts\": [\"term1\", \"term2\"],\n",
    "  \"has_code_example\": true/false,\n",
    "  \"content_type\": \"explanation|warning|comparison|definition\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        raw = llm(prompt, PIPELINE_MODELS['normalize'])\n",
    "        result = extract_json(raw, lenient=True)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        PROCESSING_METRICS.record_timing(\"transcript_normalization\", latency)\n",
    "        ROUTING_METRICS.increment(\"transcript_normalization_attempts\")\n",
    "        \n",
    "        normalized = result.get(\"normalized_content\", \"\").strip()\n",
    "        content_type = result.get(\"content_type\", \"explanation\")\n",
    "        \n",
    "        if not normalized or len(normalized) < 50:\n",
    "            logger.debug(\"Normalization skipped: no technical content extracted\")\n",
    "            ROUTING_METRICS.increment(\"transcript_normalization_skipped\")\n",
    "            return transcript\n",
    "        \n",
    "        # Validate normalization\n",
    "        original_words = len(transcript.split())\n",
    "        normalized_words = len(normalized.split())\n",
    "        ratio = normalized_words / original_words if original_words > 0 else 0\n",
    "        \n",
    "        if ratio < 0.5:\n",
    "            logger.warning(f\"Normalization suspicious: {ratio:.1%} of original length\")\n",
    "            ROUTING_METRICS.increment(\"transcript_normalization_suspicious\")\n",
    "            return transcript\n",
    "        \n",
    "        if ratio > 2.0:\n",
    "            logger.warning(f\"Normalization rejected: {ratio:.1%} of original (hallucination risk)\")\n",
    "            ROUTING_METRICS.increment(\"transcript_normalization_rejected\")\n",
    "            return transcript\n",
    "        \n",
    "        if normalized.lower().strip() == transcript.lower().strip():\n",
    "            logger.debug(\"Normalization unchanged\")\n",
    "            return transcript\n",
    "        \n",
    "        logger.info(f\"âœ¨ Normalized: {original_words}w â†’ {normalized_words}w ({ratio:.0%}, {content_type}, {latency:.2f}s)\")\n",
    "        ROUTING_METRICS.increment(\"transcript_normalization_successful\")\n",
    "        \n",
    "        return normalized\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Normalization error: {e}\")\n",
    "        ROUTING_METRICS.increment(\"transcript_normalization_errors\")\n",
    "        return transcript\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZATION LOGGING\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"PROMPT MANAGEMENT INITIALIZED\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Progress tracker: {progress_tracker.get_stats()['total_processed']:,} reels\")\n",
    "logger.info(f\"Pipeline cache: {pipeline_cache.get_stats()['cache_files']:,} entries\")\n",
    "logger.info(f\"Prompt versions: {len(prompt_version_manager.stats):,} versions tracked\")\n",
    "logger.info(f\"Confidence calibrator: {confidence_calibrator.get_calibration_stats().get('total_samples', 0):,} samples\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c008f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: MAIN PIPELINE & ORCHESTRATION (COMPLETE)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "logger = logging.getLogger(\"anki-pipeline.orchestration\")\n",
    "\n",
    "# ============================================================\n",
    "# CORE PIPELINE COMPONENTS\n",
    "# ============================================================\n",
    "\n",
    "def compact_atoms_for_generation(atoms: Dict, max_chars: int = 2000) -> Dict:\n",
    "    \"\"\"\n",
    "    Compact atoms before Stage-2 to prevent context overflow.\n",
    "    \n",
    "    Args:\n",
    "        atoms: Full atoms dictionary\n",
    "        max_chars: Maximum character limit for compacted atoms\n",
    "        \n",
    "    Returns:\n",
    "        Compacted atoms for generation\n",
    "    \"\"\"\n",
    "    compacted = {\n",
    "        \"concept\": atoms.get(\"concept\", \"\")[:200],\n",
    "        \"category\": atoms.get(\"category\", \"\"),\n",
    "        \"definition\": atoms.get(\"definition\", \"\")[:300],\n",
    "        \"technical_points\": atoms.get(\"technical_points\", [])[:4],\n",
    "        \"solutions\": atoms.get(\"solutions\", [])[:2],\n",
    "        \"impact\": atoms.get(\"impact\", [])[:2],\n",
    "        \"has_tradeoffs\": atoms.get(\"has_tradeoffs\", False),\n",
    "        \"related_concepts\": [\n",
    "            {\n",
    "                \"name\": rc.get(\"name\", \"\")[:100],\n",
    "                \"why_relevant\": rc.get(\"why_relevant\", \"\")[:150],\n",
    "                \"key_points\": rc.get(\"key_points\", [])[:2]\n",
    "            }\n",
    "            for rc in atoms.get(\"related_concepts\", [])[:2]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add topic class if available\n",
    "    if \"topic_class\" in atoms:\n",
    "        compacted[\"topic_class\"] = atoms[\"topic_class\"]\n",
    "    \n",
    "    # Ensure we don't exceed max_chars\n",
    "    compacted_str = json.dumps(compacted)\n",
    "    if len(compacted_str) > max_chars:\n",
    "        # Further truncation\n",
    "        compacted[\"technical_points\"] = compacted[\"technical_points\"][:2]\n",
    "        compacted[\"related_concepts\"] = compacted[\"related_concepts\"][:1]\n",
    "        compacted_str = json.dumps(compacted)\n",
    "        \n",
    "        if len(compacted_str) > max_chars:\n",
    "            compacted[\"related_concepts\"] = []\n",
    "    \n",
    "    return compacted\n",
    "\n",
    "def validate_cloze(card: Dict, topic_class: str) -> bool:\n",
    "    \"\"\"\n",
    "    Topic-aware cloze validation.\n",
    "    \n",
    "    Foundation: 2 clozes minimum, 60 chars minimum\n",
    "    Intermediate/Advanced: 3 clozes minimum, 90 chars minimum\n",
    "    \"\"\"\n",
    "    cloze = card.get(\"cloze\", \"\")\n",
    "    \n",
    "    # Count actual cloze deletions\n",
    "    cloze_pattern = r'\\{\\{c\\d+::'\n",
    "    cloze_count = len(re.findall(cloze_pattern, cloze))\n",
    "    \n",
    "    if topic_class == \"foundation\":\n",
    "        return cloze_count >= 2 and len(cloze) >= 60\n",
    "    else:\n",
    "        return cloze_count >= 3 and len(cloze) >= 90\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2: CARD GENERATION PROMPTS\n",
    "# ============================================================\n",
    "\n",
    "def stage2a_basic_prompt(atoms: Dict) -> str:\n",
    "    \"\"\"Generate 2-3 BASIC interview cards.\"\"\"\n",
    "    clean_atoms = compact_atoms_for_generation(atoms)\n",
    "    \n",
    "    return f\"\"\"Generate 2-3 BASIC interview cards.\n",
    "\n",
    "Input:\n",
    "{json.dumps(clean_atoms, indent=2)}\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Generate 2â€“3 cards for the PRIMARY concept\n",
    "2. Each card must be interview-ready with clear front/back\n",
    "3. Use bullet points for clarity in back\n",
    "4. Include concrete examples or code snippets\n",
    "5. Focus on understanding, not just facts\n",
    "\n",
    "OUTPUT JSON:\n",
    "{{\n",
    "  \"cards\": [\n",
    "    {{\n",
    "      \"type\": \"basic\",\n",
    "      \"concept_source\": \"primary\",\n",
    "      \"front\": \"Clear question about {clean_atoms.get('concept', 'concept')}\",\n",
    "      \"back\": \"Detailed answer with bullet points and examples\",\n",
    "      \"tags\": [\"domain:{clean_atoms.get('category', 'java')}\", \"difficulty:medium\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "def stage2b_cloze_prompt(atoms: Dict) -> str:\n",
    "    \"\"\"Generate 1-2 CLOZE deletion cards.\"\"\"\n",
    "    clean_atoms = compact_atoms_for_generation(atoms)\n",
    "    \n",
    "    return f\"\"\"Generate 1-2 CLOZE deletion cards.\n",
    "\n",
    "Input:\n",
    "{json.dumps(clean_atoms, indent=2)}\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Each card MUST have at least 2-3 deletions (topic-aware)\n",
    "2. Prefer mechanisms, definitions, and key concepts for cloze\n",
    "3. Length 60â€“180 characters (topic-aware)\n",
    "4. Avoid filler text - focus on core technical content\n",
    "5. Make deletions meaningful (key terms, not obvious fillers)\n",
    "\n",
    "FORMAT:\n",
    "Use {{{{c1:: }}}}, {{{{c2:: }}}}, {{{{c3:: }}}} syntax\n",
    "\n",
    "OUTPUT JSON:\n",
    "{{\n",
    "  \"cards\": [\n",
    "    {{\n",
    "      \"type\": \"cloze\",\n",
    "      \"concept_source\": \"primary\",\n",
    "      \"cloze\": \"Text with {{{{c1::term}}}} and {{{{c2::another}}}} deletion...\",\n",
    "      \"tags\": [\"domain:{clean_atoms.get('category', 'java')}\", \"difficulty:medium\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "def stage2c_tradeoff_prompt(atoms: Dict) -> str:\n",
    "    \"\"\"Generate ONE TRADEOFF card.\"\"\"\n",
    "    if not atoms.get(\"has_tradeoffs\"):\n",
    "        return \"\"\n",
    "    \n",
    "    clean_atoms = compact_atoms_for_generation(atoms)\n",
    "    \n",
    "    return f\"\"\"Generate ONE TRADEOFF card.\n",
    "\n",
    "Input:\n",
    "{json.dumps(clean_atoms, indent=2)}\n",
    "\n",
    "RULES:\n",
    "1. Compare exactly 2â€“3 approaches\n",
    "2. Each approach MUST have 2 PROS and 2 CONS\n",
    "3. Focus on engineering tradeoffs (performance, complexity, maintainability)\n",
    "4. If no explicit alternative exists, infer the most common real-world alternative\n",
    "\n",
    "OUTPUT JSON:\n",
    "{{\n",
    "  \"cards\": [\n",
    "    {{\n",
    "      \"type\": \"tradeoff\",\n",
    "      \"concept_source\": \"primary\",\n",
    "      \"front\": \"What are the trade-offs when implementing {clean_atoms.get('concept', 'this concept')}?\",\n",
    "      \"tradeoffs\": [\n",
    "        {{\n",
    "          \"approach\": \"Approach name\",\n",
    "          \"pros\": [\"Performance benefit...\", \"Simplicity...\"],\n",
    "          \"cons\": [\"Memory overhead...\", \"Complexity...\"]\n",
    "        }}\n",
    "      ],\n",
    "      \"tags\": [\"domain:{clean_atoms.get('category', 'java')}\", \"difficulty:senior\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "def generate_basic_cards(atoms: Dict) -> List[Dict]:\n",
    "    \"\"\"Generate basic cards from atoms.\"\"\"\n",
    "    try:\n",
    "        prompt = stage2a_basic_prompt(atoms)\n",
    "        raw = llm(prompt, PIPELINE_MODELS['generate_basic'])\n",
    "        output = extract_json(raw, lenient=True)\n",
    "        \n",
    "        if not output:\n",
    "            logger.debug(\"Basic card generation returned no output\")\n",
    "            return []\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(output, list):\n",
    "            return output\n",
    "        elif isinstance(output, dict):\n",
    "            cards = output.get(\"cards\", [])\n",
    "            if cards:\n",
    "                logger.debug(f\"Generated {len(cards)} basic cards\")\n",
    "            return cards\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Basic generation failed: {e}\")\n",
    "        PROCESSING_METRICS.increment(\"card_generation_errors\")\n",
    "        return []\n",
    "\n",
    "def generate_cloze_cards(atoms: Dict) -> List[Dict]:\n",
    "    \"\"\"Generate cloze cards from atoms.\"\"\"\n",
    "    try:\n",
    "        topic_class = atoms.get(\"topic_class\", \"intermediate\")\n",
    "        tech_points = atoms.get(\"technical_points\", [])\n",
    "        definition = atoms.get(\"definition\", \"\")\n",
    "        \n",
    "        # Check if content is suitable for cloze\n",
    "        has_suitable_content = False\n",
    "        if topic_class == \"foundation\":\n",
    "            combined_text = f\"{definition} {' '.join(tech_points)}\"\n",
    "            has_definition = len(definition.split()) >= 5\n",
    "            has_is_statement = \" is \" in combined_text.lower() or \" are \" in combined_text.lower()\n",
    "            has_cause_effect = any(word in combined_text.lower() for word in [\"because\", \"therefore\", \"causes\", \"leads to\", \"results in\"])\n",
    "            has_suitable_content = has_definition or has_is_statement or has_cause_effect\n",
    "        else:\n",
    "            has_suitable_content = len(tech_points) >= 2\n",
    "        \n",
    "        if not has_suitable_content:\n",
    "            logger.debug(f\"Skipping cloze generation - unsuitable content for {topic_class}\")\n",
    "            return []\n",
    "        \n",
    "        prompt = stage2b_cloze_prompt(atoms)\n",
    "        raw = llm(prompt, PIPELINE_MODELS['generate_cloze'])\n",
    "        output = extract_json(raw, lenient=True)\n",
    "        \n",
    "        if not output:\n",
    "            logger.debug(\"Cloze card generation returned no output\")\n",
    "            return []\n",
    "        \n",
    "        # Extract cards\n",
    "        cards = output.get(\"cards\", []) if isinstance(output, dict) else output\n",
    "        \n",
    "        # Validate each cloze card\n",
    "        valid_cards = []\n",
    "        for card in cards:\n",
    "            if validate_cloze(card, topic_class):\n",
    "                valid_cards.append(card)\n",
    "            else:\n",
    "                logger.debug(f\"Rejected invalid cloze card: {card.get('cloze', '')[:50]}...\")\n",
    "        \n",
    "        if valid_cards:\n",
    "            logger.debug(f\"Generated {len(valid_cards)} valid cloze cards\")\n",
    "        \n",
    "        return valid_cards\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cloze generation failed: {e}\")\n",
    "        PROCESSING_METRICS.increment(\"cloze_generation_errors\")\n",
    "        return []\n",
    "\n",
    "def generate_tradeoff_cards(atoms: Dict) -> List[Dict]:\n",
    "    \"\"\"Generate tradeoff cards from atoms.\"\"\"\n",
    "    if not atoms.get(\"has_tradeoffs\") or not CONFIG.get(\"ENABLE_TRADEOFFS\", True):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        prompt = stage2c_tradeoff_prompt(atoms)\n",
    "        if not prompt:\n",
    "            return []\n",
    "        \n",
    "        raw = llm(prompt, PIPELINE_MODELS['generate_tradeoff'])\n",
    "        output = extract_json(raw, lenient=True)\n",
    "        \n",
    "        if not output:\n",
    "            logger.debug(\"Tradeoff card generation returned no output\")\n",
    "            return []\n",
    "        \n",
    "        # Extract cards\n",
    "        cards = output.get(\"cards\", []) if isinstance(output, dict) else output\n",
    "        \n",
    "        # Validate tradeoff cards\n",
    "        valid_cards = []\n",
    "        for card in cards:\n",
    "            tradeoffs = card.get(\"tradeoffs\", [])\n",
    "            if len(tradeoffs) >= 2:\n",
    "                valid = all(\n",
    "                    len(t.get(\"pros\", [])) >= 2 and len(t.get(\"cons\", [])) >= 2\n",
    "                    for t in tradeoffs\n",
    "                )\n",
    "                if valid:\n",
    "                    valid_cards.append(card)\n",
    "                else:\n",
    "                    logger.debug(\"Rejected tradeoff card - insufficient pros/cons\")\n",
    "        \n",
    "        if valid_cards:\n",
    "            logger.debug(f\"Generated {len(valid_cards)} tradeoff cards\")\n",
    "        \n",
    "        return valid_cards\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Tradeoff generation failed: {e}\")\n",
    "        PROCESSING_METRICS.increment(\"tradeoff_generation_errors\")\n",
    "        return []\n",
    "\n",
    "def generate_adjacent_card(atoms: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Generate one adjacent card for foundation concepts.\"\"\"\n",
    "    try:\n",
    "        concept = atoms.get(\"concept\", \"\")\n",
    "        definition = atoms.get(\"definition\", \"\")\n",
    "        technical_points = atoms.get(\"technical_points\", [])\n",
    "        \n",
    "        prompt = f\"\"\"Generate ONE adjacent basic card for a foundation concept.\n",
    "\n",
    "Original concept: {concept}\n",
    "Definition: {definition}\n",
    "Key points: {technical_points[:3]}\n",
    "\n",
    "Rules:\n",
    "1. Must directly relate to the SAME concept\n",
    "2. Focus on practical application, common mistake, or edge case\n",
    "3. Keep it simple and interview-ready\n",
    "4. No new concepts - only extensions of the original\n",
    "\n",
    "OUTPUT JSON:\n",
    "{{\n",
    "  \"type\": \"basic\",\n",
    "  \"concept_source\": \"adjacent\",\n",
    "  \"front\": \"Question about practical application of {concept}\",\n",
    "  \"back\": \"Answer with practical insight, common mistake, or edge case\",\n",
    "  \"tags\": [\"domain:foundation\", \"type:adjacent\"]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "        \n",
    "        raw = llm(prompt, PIPELINE_MODELS['generate_basic'])\n",
    "        output = extract_json(raw, lenient=True)\n",
    "        \n",
    "        if not output:\n",
    "            return None\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(output, dict) and \"type\" in output:\n",
    "            return output\n",
    "        elif isinstance(output, list) and len(output) > 0:\n",
    "            return output[0]\n",
    "        elif isinstance(output, dict) and \"cards\" in output:\n",
    "            cards = output[\"cards\"]\n",
    "            return cards[0] if cards else None\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Adjacent card generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# QUALITY & COMPLETION ASSESSMENT\n",
    "# ============================================================\n",
    "\n",
    "def determine_completion_state(card_count: int, has_tradeoffs: bool, tradeoff_count: int,\n",
    "                              duplicates_filtered: int = 0, topic_class: str = \"intermediate\") -> Tuple[CompletionState, str]:\n",
    "    \"\"\"\n",
    "    Determine completion state based on generated cards.\n",
    "    \n",
    "    Returns (state, reason) for observability.\n",
    "    \"\"\"\n",
    "    if card_count < CONFIG[\"MIN_CARDS_FOR_PARTIAL\"]:\n",
    "        if duplicates_filtered > 0:\n",
    "            return CompletionState.INCOMPLETE, \"duplicates_filtered\"\n",
    "        return CompletionState.INCOMPLETE, \"low_card_count\"\n",
    "    \n",
    "    if card_count >= CONFIG[\"MIN_CARDS_FOR_FULL\"]:\n",
    "        if (topic_class != \"foundation\" and\n",
    "            has_tradeoffs and\n",
    "            tradeoff_count == 0 and\n",
    "            CONFIG.get(\"ENABLE_TRADEOFFS\", True)):\n",
    "            return CompletionState.PARTIAL, \"missing_tradeoff\"\n",
    "        return CompletionState.FULL, \"complete\"\n",
    "    \n",
    "    return CompletionState.PARTIAL, \"partial_cards\"\n",
    "\n",
    "def quality_score_fallback(card: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Enhanced quality scoring for cards.\n",
    "    Returns score 0-100.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    card_type = card.get(\"type\", \"basic\")\n",
    "    \n",
    "    front = card.get(\"front\") or \"\"\n",
    "    back = card.get(\"back\") or \"\"\n",
    "    cloze = card.get(\"cloze\") or \"\"\n",
    "    \n",
    "    full_text = f\"{front} {back} {cloze}\".strip()\n",
    "    \n",
    "    if not full_text:\n",
    "        return 0\n",
    "    \n",
    "    # Enhanced technical scoring\n",
    "    tech_count = 0\n",
    "    for kws in TECHNICAL_KEYWORDS.values():\n",
    "        for kw in kws:\n",
    "            if kw in full_text.lower():\n",
    "                tech_count += 1\n",
    "    \n",
    "    foundation_count = sum(1 for kw in FOUNDATION_ENHANCEMENT_KEYWORDS if kw in full_text.lower())\n",
    "    \n",
    "    score += min(tech_count * 6, 30)\n",
    "    score += min(foundation_count * 3, 15)\n",
    "    \n",
    "    # Card type specific scoring\n",
    "    if card_type == \"basic\":\n",
    "        if len(back) >= 150:\n",
    "            score += 15\n",
    "        bullet_count = back.count(\"â€¢\") + back.count(\"-\") + back.count(\"*\")\n",
    "        score += min(bullet_count * 3, 25)\n",
    "        if \"`\" in back or \"```\" in back:\n",
    "            score += 10\n",
    "    \n",
    "    elif card_type == \"cloze\":\n",
    "        cloze_count = cloze.count(\"{{c\")\n",
    "        score += min(cloze_count * 8, 25)\n",
    "        if len(cloze) >= 120:\n",
    "            score += 15\n",
    "    \n",
    "    elif card_type == \"tradeoff\":\n",
    "        tradeoffs = card.get(\"tradeoffs\", [])\n",
    "        if len(tradeoffs) >= 2:\n",
    "            score += 20\n",
    "            # Check for balanced pros/cons\n",
    "            balanced = all(len(t.get(\"pros\", [])) >= 2 and len(t.get(\"cons\", [])) >= 2 for t in tradeoffs)\n",
    "            if balanced:\n",
    "                score += 15\n",
    "    \n",
    "    # Question quality\n",
    "    if any(ind in front.lower() for ind in [\"what\", \"how\", \"why\", \"explain\", \"compare\", \"difference\"]):\n",
    "        score += 20\n",
    "    if len(front.split()) >= 5:\n",
    "        score += 10\n",
    "    \n",
    "    # Clamp to 0-100\n",
    "    return min(max(score, 0), 100)\n",
    "\n",
    "def assign_priority(quality: int) -> str:\n",
    "    \"\"\"Assign priority based on quality score.\"\"\"\n",
    "    if quality >= 85:\n",
    "        return \"P0\"\n",
    "    elif quality >= 70:\n",
    "        return \"P1\"\n",
    "    else:\n",
    "        return \"P2\"\n",
    "\n",
    "def calculate_confidence(atoms: Dict, cards: List[Dict], quality_dims: QualityDimensions) -> float:\n",
    "    \"\"\"\n",
    "    Topic-aware confidence calculation with floor for high-quality foundation cards.\n",
    "    \"\"\"\n",
    "    topic_class = atoms.get(\"topic_class\", \"intermediate\")\n",
    "    expected_cards = TOPIC_CLASSES.get(topic_class, {}).get(\"expected_cards\", 3)\n",
    "    \n",
    "    final_count = len(cards)\n",
    "    avg_quality = sum(quality_score_fallback(c) for c in cards) / max(final_count, 1)\n",
    "    \n",
    "    # Base confidence calculation\n",
    "    card_factor = min(final_count / expected_cards, 1.0)\n",
    "    quality_factor = avg_quality / 100\n",
    "    \n",
    "    confidence = (\n",
    "        card_factor * 0.4 +\n",
    "        quality_factor * 0.3 +\n",
    "        quality_dims.combined_score * 0.3\n",
    "    )\n",
    "    \n",
    "    # CRITICAL FIX #7: Confidence MUST align with correctness\n",
    "    if quality_dims.correctness_score >= 0.9 and final_count >= 1:\n",
    "        confidence = max(confidence, 0.6)\n",
    "        logger.debug(f\"High correctness ({quality_dims.correctness_score:.2f}) â†’ confidence floor 0.6\")\n",
    "    \n",
    "    # Adaptive foundation confidence boost\n",
    "    if topic_class == \"foundation\":\n",
    "        if quality_dims.correctness_score >= 0.95 and quality_dims.richness_score >= 0.75:\n",
    "            boost_factor = 1.35\n",
    "            confidence *= boost_factor\n",
    "            logger.debug(f\"Foundation quality boost: {confidence:.2f} (correctness={quality_dims.correctness_score:.2f}, richness={quality_dims.richness_score:.2f})\")\n",
    "        elif quality_dims.correctness_score >= 0.9:\n",
    "            boost_factor = 1.2\n",
    "            confidence *= boost_factor\n",
    "            logger.debug(f\"Foundation moderate boost: {confidence:.2f}\")\n",
    "    \n",
    "    # Confidence floor prevents penalizing high-quality foundational cards\n",
    "    if final_count >= 1 and avg_quality >= 70 and topic_class == \"foundation\":\n",
    "        confidence = max(confidence, 0.65)\n",
    "        logger.debug(f\"Foundation quality floor: {confidence:.2f}\")\n",
    "    \n",
    "    # Incomplete completion state reduces confidence\n",
    "    completion_state_obj = atoms.get(\"_completion_state_obj\")\n",
    "    if completion_state_obj == CompletionState.INCOMPLETE:\n",
    "        penalty = 0.85 if topic_class == \"foundation\" else 0.6\n",
    "        confidence *= penalty\n",
    "        logger.debug(f\"Confidence reduced due to incomplete state ({topic_class}): {confidence:.2f}\")\n",
    "    \n",
    "    # Topic-aware confidence floor\n",
    "    CONFIDENCE_FLOOR = {\n",
    "        \"foundation\": 0.45,\n",
    "        \"intermediate\": 0.50,\n",
    "        \"advanced\": 0.55\n",
    "    }\n",
    "    \n",
    "    floor = CONFIDENCE_FLOOR.get(topic_class, 0.50)\n",
    "    if confidence < floor:\n",
    "        logger.debug(f\"Applying confidence floor: {confidence:.2f} â†’ {floor:.2f} ({topic_class})\")\n",
    "        confidence = floor\n",
    "    \n",
    "    # Apply calibration\n",
    "    calibrated_confidence = calibrate_confidence(confidence, topic_class)\n",
    "    \n",
    "    return min(calibrated_confidence, 1.0)\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 1: ATOM EXTRACTION WITH RETRY\n",
    "# ============================================================\n",
    "\n",
    "def extract_atoms_with_retry(reel: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract atoms with adaptive routing and enrichment.\n",
    "    All CRITICAL FIXES applied here.\n",
    "    \"\"\"\n",
    "    reel_id = str(reel.get(\"reel_id\", reel.get(\"id\", \"\")))\n",
    "    caption = str(reel.get(\"caption\", \"\"))\n",
    "    transcript = str(reel.get(\"transcript\", \"\"))\n",
    "    category = str(reel.get(\"category\", \"\"))\n",
    "    \n",
    "    logger.info(f\"ðŸŽ¬ Processing reel {reel_id}: {caption[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check terminal rejections FIRST\n",
    "        if terminal_rejections.is_terminal(reel_id):\n",
    "            logger.info(f\"  â­ï¸ Skipping - terminally rejected\")\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": \"terminal_rejected\",\n",
    "                \"reason\": \"Previously rejected by logic - no retry allowed\"\n",
    "            }\n",
    "        \n",
    "        # Check cache first\n",
    "        cached = get_cached_result(reel_id, \"stage1_atoms\")\n",
    "        if cached and cached.get(\"status\") == \"extracted\":\n",
    "            logger.info(f\"  â­ï¸ Using cached atoms\")\n",
    "            return cached\n",
    "        \n",
    "        # PHASE 2: Apply transcript normalization\n",
    "        if CONFIG.get(\"ENABLE_TRANSCRIPT_NORMALIZATION\", True):\n",
    "            original_transcript = transcript\n",
    "            transcript = normalize_transcript(transcript, caption)\n",
    "            \n",
    "            if len(transcript) != len(original_transcript):\n",
    "                normalization_delta = len(transcript) - len(original_transcript)\n",
    "                ROUTING_METRICS.increment(\"content_signal::transcript_normalized\")\n",
    "        \n",
    "        # Classify topic\n",
    "        topic_class, topic_confidence = classify_topic_with_confidence(caption, category, transcript)\n",
    "        logger.info(f\"  ðŸ“Š Topic: {topic_class} (confidence: {topic_confidence})\")\n",
    "        \n",
    "        # Create normalized concept key upfront\n",
    "        normalized_concept = normalize_learning_key(caption)\n",
    "        \n",
    "        # Get learned strategy from rejection memory\n",
    "        learned_override = None\n",
    "        if rejection_memory:\n",
    "            best_strategy = rejection_memory.get_best_strategy(\n",
    "                concept=normalized_concept,\n",
    "                category=category,\n",
    "                topic_class=topic_class\n",
    "            )\n",
    "            if best_strategy:\n",
    "                learned_override = best_strategy.get(\"strategy\")\n",
    "                logger.info(f\"  ðŸ§  Learned strategy: {learned_override}\")\n",
    "        \n",
    "        # Select prompt strategy\n",
    "        strategy, routing_reason = select_prompt_strategy(\n",
    "            caption, transcript, category, learned_override\n",
    "        )\n",
    "        logger.info(f\"  ðŸŽ¯ Strategy: {strategy.value} ({routing_reason})\")\n",
    "        \n",
    "        # Build prompt\n",
    "        if strategy == PromptStrategy.STRICT_ADVANCED:\n",
    "            prompt = stage1_prompt_a_strict(caption, transcript)\n",
    "        elif strategy == PromptStrategy.DSA_FOCUSED:\n",
    "            prompt = stage1_prompt_c_dsa(caption, transcript)\n",
    "        else:\n",
    "            prompt = stage1_prompt_b_foundation(caption, transcript)\n",
    "        \n",
    "        # Call LLM\n",
    "        start_time = time.time()\n",
    "        raw = llm(prompt, PIPELINE_MODELS['extract'])\n",
    "        extraction_time = time.time() - start_time\n",
    "        \n",
    "        PROCESSING_METRICS.record_timing(\"atom_extraction\", extraction_time)\n",
    "        PROCESSING_METRICS.increment(\"atom_extraction_attempts\")\n",
    "        \n",
    "        atoms = extract_json(raw, lenient=True)\n",
    "        \n",
    "        # Validate atoms\n",
    "        is_valid, errors = validate_atoms(atoms)\n",
    "        if not is_valid:\n",
    "            logger.error(f\"  âŒ Invalid atoms schema: {errors}\")\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": \"error\",\n",
    "                \"reason\": f\"Invalid schema: {errors[:3]}\"\n",
    "            }\n",
    "        \n",
    "        if not atoms.get(\"valid\", True):\n",
    "            reject_reason = atoms.get(\"reject_reason\", \"Unknown\")\n",
    "            logger.info(f\"  âŒ Rejected by logic: {reject_reason}\")\n",
    "            \n",
    "            terminal_rejections.mark_terminal(\n",
    "                reel_id,\n",
    "                reason=f\"handled_by_logic: {reject_reason}\",\n",
    "                stage=\"stage1\",\n",
    "                rejection_type=\"SEMANTIC_TERMINAL\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": \"rejected\",\n",
    "                \"reason\": reject_reason,\n",
    "                \"handled_by_logic\": True\n",
    "            }\n",
    "        \n",
    "        # Use caption as canonical learning_key, not LLM output\n",
    "        atoms[\"learning_key\"] = normalized_concept\n",
    "        \n",
    "        # Calculate technical score\n",
    "        tech_score_val = technical_score(\n",
    "            f\"{atoms.get('concept','')} {atoms.get('definition','')} \"\n",
    "            f\"{' '.join(atoms.get('technical_points', []))}\"\n",
    "        )\n",
    "        \n",
    "        # Topic-aware thresholds\n",
    "        min_score = 4\n",
    "        if CONFIG.get(\"NORMALIZE_TECH_SCORES\", True):\n",
    "            adjustment = TOPIC_CLASSES.get(topic_class, {}).get(\"threshold_adjustment\", 0)\n",
    "            min_score += adjustment\n",
    "        \n",
    "        logger.info(f\"  ðŸ“Š Tech score: {tech_score_val} | threshold: {min_score}\")\n",
    "        \n",
    "        # Foundation topics: disable tradeoffs only if truly basic\n",
    "        if topic_class == \"foundation\":\n",
    "            has_comparison = any(word in transcript.lower() for word in [\"vs\", \"versus\", \"instead\", \"rather than\", \"compared\"])\n",
    "            if not has_comparison:\n",
    "                atoms[\"has_tradeoffs\"] = False\n",
    "            else:\n",
    "                atoms[\"has_tradeoffs\"] = (\n",
    "                    atoms.get(\"has_tradeoffs\", False) and\n",
    "                    len(atoms.get(\"related_concepts\", [])) >= 1\n",
    "                )\n",
    "        \n",
    "        # Structural impossibility gate\n",
    "        char_length = len(transcript.strip())\n",
    "        if char_length < 80 and topic_class != \"foundation\":\n",
    "            logger.info(f\"  ðŸš« Structural impossibility: transcript too short ({char_length} chars)\")\n",
    "            terminal_rejections.mark_terminal(\n",
    "                reel_id,\n",
    "                reason=\"insufficient_source_material\",\n",
    "                stage=\"stage1\",\n",
    "                rejection_type=\"STRUCTURAL\"\n",
    "            )\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": RejectionType.STRUCTURAL.value,\n",
    "                \"reason\": \"insufficient_source_material\",\n",
    "                \"skip_enrichment\": True\n",
    "            }\n",
    "        \n",
    "        # Check if below threshold\n",
    "        if tech_score_val < min_score:\n",
    "            if topic_class == \"foundation\":\n",
    "                is_valuable = (\n",
    "                    len(atoms.get(\"related_concepts\", [])) > 0 or\n",
    "                    len(atoms.get(\"technical_points\", [])) >= 3 or\n",
    "                    atoms.get(\"definition\", \"\") != \"\"\n",
    "                )\n",
    "                \n",
    "                if is_valuable:\n",
    "                    logger.info(f\"  âœ… Foundation topic - pedagogically valuable (accepted despite score)\")\n",
    "                    atoms[\"low_density_but_valid\"] = True\n",
    "                else:\n",
    "                    logger.info(f\"  âŒ Foundation topic - insufficient pedagogical value (rejected)\")\n",
    "            else:\n",
    "                logger.info(f\"  âš ï¸ Below depth threshold (rejected unless enriched)\")\n",
    "        else:\n",
    "            logger.info(f\"  âœ… Passed depth threshold (accepted)\")\n",
    "        \n",
    "        # Enrichment logic\n",
    "        has_rejection_history = (\n",
    "            rejection_memory and\n",
    "            rejection_memory.get_rejection_count(normalized_concept, category, topic_class) > 0\n",
    "        )\n",
    "        \n",
    "        # Predictive confidence check\n",
    "        would_have_low_confidence = (\n",
    "            len(atoms.get(\"technical_points\", [])) < 3 or\n",
    "            len(atoms.get(\"solutions\", [])) < 1 or\n",
    "            (topic_class != \"foundation\" and not atoms.get(\"has_tradeoffs\", False))\n",
    "        )\n",
    "        \n",
    "        should_enrich = (\n",
    "            (tech_score_val < min_score or has_rejection_history or would_have_low_confidence) and\n",
    "            CONFIG.get(\"ENABLE_ENRICHMENT\", True) and\n",
    "            not atoms.get(\"low_density_but_valid\", False)\n",
    "        )\n",
    "        \n",
    "        enrichment_attempted = False\n",
    "        if should_enrich:\n",
    "            concept = atoms.get(\"learning_key\")\n",
    "            enrichment_key = f\"{concept}::{category or 'unknown'}\"\n",
    "            \n",
    "            # Topic-aware enrichment budget\n",
    "            max_enrichments = CONFIG[\"MAX_ENRICHMENTS_PER_CONCEPT\"].get(\n",
    "                topic_class,\n",
    "                CONFIG[\"MAX_ENRICHMENTS_PER_CONCEPT\"].get(\"intermediate\", 2)\n",
    "            )\n",
    "            \n",
    "            # Check per-concept enrichment budget with temporal spacing\n",
    "            with ENRICHMENT_BUDGET_LOCK:\n",
    "                now = time.time()\n",
    "                last_enrichment = ENRICHMENT_TIMESTAMPS.get(enrichment_key, 0)\n",
    "                days_since_last = (now - last_enrichment) / (24 * 60 * 60)\n",
    "                \n",
    "                if days_since_last > ENRICHMENT_BUDGET_RESET_DAYS:\n",
    "                    if ENRICHMENT_BUDGET[enrichment_key] > 0:\n",
    "                        logger.info(f\"  ðŸ”„ Resetting enrichment budget ({days_since_last:.1f} days since last attempt)\")\n",
    "                    ENRICHMENT_BUDGET[enrichment_key] = 0\n",
    "                \n",
    "                concept_enrichments = ENRICHMENT_BUDGET[enrichment_key]\n",
    "                already_enriched = concept_enrichments >= max_enrichments\n",
    "            \n",
    "            if already_enriched:\n",
    "                logger.info(f\"  â­ï¸ Skipping enrichment - concept budget exhausted ({concept_enrichments}/{max_enrichments} for {topic_class})\")\n",
    "                logger.info(f\"     Budget will reset in {max(0, ENRICHMENT_BUDGET_RESET_DAYS - days_since_last):.1f} days\")\n",
    "            else:\n",
    "                # Check rejection memory\n",
    "                if rejection_memory:\n",
    "                    if rejection_memory.should_skip(concept, category, topic_class):\n",
    "                        logger.info(f\"  â­ï¸ Skipping - too many rejections\")\n",
    "                        \n",
    "                        terminal_rejections.mark_terminal(\n",
    "                            reel_id,\n",
    "                            reason=\"repeatedly_failed_enrichment\",\n",
    "                            stage=\"stage1_enrichment\",\n",
    "                            rejection_type=\"SEMANTIC_TERMINAL\"\n",
    "                        )\n",
    "                        \n",
    "                        return {\n",
    "                            \"reel_id\": reel_id,\n",
    "                            \"status\": RejectionType.SEMANTIC.value,\n",
    "                            \"reason\": \"repeatedly_failed\"\n",
    "                        }\n",
    "                    \n",
    "                    strategy_hint = rejection_memory.get_best_strategy(concept, category, topic_class)\n",
    "                else:\n",
    "                    strategy_hint = None\n",
    "                \n",
    "                enrichment_reason = (\n",
    "                    f\"low_score ({tech_score_val})\" if tech_score_val < min_score\n",
    "                    else \"predicted_low_confidence\" if would_have_low_confidence\n",
    "                    else \"rejection_history\"\n",
    "                )\n",
    "                logger.info(f\"  ðŸ”„ Enriching (reason: {enrichment_reason})\")\n",
    "                \n",
    "                # Increment using canonical key\n",
    "                with ENRICHMENT_BUDGET_LOCK:\n",
    "                    ENRICHMENT_BUDGET[enrichment_key] += 1\n",
    "                    ENRICHMENT_TIMESTAMPS[enrichment_key] = time.time()\n",
    "                \n",
    "                ROUTING_METRICS.increment(\"enriched\")\n",
    "                enrichment_attempted = True\n",
    "                \n",
    "                enrichment_start = time.time()\n",
    "                \n",
    "                try:\n",
    "                    enrichment_prompt = create_enrichment_prompt(atoms, strategy_hint.get(\"strategy\") if strategy_hint else None)\n",
    "                    enriched_raw = llm(enrichment_prompt, PIPELINE_MODELS['extract_retry'])\n",
    "                    enriched_atoms = extract_json(enriched_raw, lenient=True)\n",
    "                    \n",
    "                    if validate_atoms(enriched_atoms)[0] and enriched_atoms.get(\"valid\", True):\n",
    "                        new_score = technical_score(\n",
    "                            f\"{enriched_atoms.get('concept','')} {enriched_atoms.get('definition','')} \"\n",
    "                            f\"{' '.join(enriched_atoms.get('technical_points', []))}\"\n",
    "                        )\n",
    "                        \n",
    "                        logger.info(f\"  âœ¨ Enriched score: {new_score} (was: {tech_score_val})\")\n",
    "                        \n",
    "                        score_improvement = new_score - tech_score_val\n",
    "                        if new_score >= min_score and score_improvement > 1:\n",
    "                            atoms = enriched_atoms\n",
    "                            tech_score_val = new_score\n",
    "                            atoms[\"was_enriched\"] = True\n",
    "                            atoms[\"_enrichment_attempts\"] = 1\n",
    "                            atoms[\"_elapsed_enrichment\"] = round(time.time() - enrichment_start, 2)\n",
    "                            atoms[\"_delta_score\"] = score_improvement\n",
    "                            logger.info(f\"  âœ… Enrichment successful! (Î”={score_improvement})\")\n",
    "                        elif new_score >= min_score:\n",
    "                            logger.info(f\"  âš ï¸ Enrichment marginal (Î”={score_improvement}) - not learning from noise\")\n",
    "                        else:\n",
    "                            logger.info(f\"  âš ï¸ Still below threshold\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"  âš ï¸ Enrichment failed: {e}\")\n",
    "        \n",
    "        # Final threshold check - SEMANTIC rejection\n",
    "        if tech_score_val < min_score and not atoms.get(\"low_density_but_valid\", False):\n",
    "            if rejection_memory:\n",
    "                concept_key = atoms.get(\"learning_key\", normalized_concept)\n",
    "                rejection_memory.record_rejection(\n",
    "                    concept_key,\n",
    "                    tech_score_val,\n",
    "                    f\"Low technical signal ({tech_score_val})\",\n",
    "                    category=category,\n",
    "                    topic_class=topic_class\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"  âŒ Final rejection: Low technical signal ({tech_score_val})\")\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": RejectionType.SEMANTIC.value,\n",
    "                \"reason\": f\"Low technical signal ({tech_score_val})\"\n",
    "            }\n",
    "        \n",
    "        # Success\n",
    "        atoms[\"reel_id\"] = reel_id\n",
    "        atoms[\"status\"] = \"extracted\"\n",
    "        atoms[\"tech_score\"] = tech_score_val\n",
    "        atoms[\"topic_class\"] = topic_class\n",
    "        atoms[\"prompt_strategy\"] = strategy.value\n",
    "        atoms[\"extraction_time\"] = extraction_time\n",
    "        atoms[\"enrichment_attempted\"] = enrichment_attempted\n",
    "        \n",
    "        # Model-aware prompt versioning\n",
    "        model_name = PIPELINE_MODELS['extract'].name.replace(\":\", \"_\").replace(\".\", \"_\")\n",
    "        atoms[\"prompt_version\"] = f\"stage1_{strategy.value}_{model_name}_{CONFIG['CACHE_VERSION']}\"\n",
    "        atoms[\"routing_reason\"] = routing_reason\n",
    "        \n",
    "        ROUTING_METRICS.increment(f\"reason::{routing_reason}\")\n",
    "        \n",
    "        # Cache result\n",
    "        save_cached_result(reel_id, \"stage1_atoms\", atoms)\n",
    "        \n",
    "        logger.info(f\"  âœ… Atoms extracted successfully (score: {tech_score_val})\")\n",
    "        return atoms\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âŒ Atom extraction failed: {e}\")\n",
    "        error_tracker.record(\n",
    "            \"atom_extraction_error\",\n",
    "            str(e),\n",
    "            {\"reel_id\": reel_id, \"caption\": caption[:100]}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"reel_id\": reel_id,\n",
    "            \"status\": \"error\",\n",
    "            \"reason\": str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2: CARD GENERATION\n",
    "# ============================================================\n",
    "\n",
    "def generate_cards_from_atoms(atoms: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate all card types from atoms with density-aware routing.\n",
    "    \"\"\"\n",
    "    reel_id = atoms.get(\"reel_id\", \"\")\n",
    "    prompt_recorded = False\n",
    "    \n",
    "    try:\n",
    "        # Check cache\n",
    "        cached = get_cached_result(reel_id, \"stage2_cards\")\n",
    "        if cached and cached.get(\"status\") == \"success\":\n",
    "            logger.info(f\"  â­ï¸ Using cached cards\")\n",
    "            return cached\n",
    "        \n",
    "        logger.info(f\"  ðŸŽ´ Generating cards...\")\n",
    "        \n",
    "        # PHASE 3: Determine content density and route accordingly\n",
    "        caption = atoms.get(\"caption\", \"\")\n",
    "        transcript = atoms.get(\"transcript\", \"\")\n",
    "        category = atoms.get(\"category\", \"\")\n",
    "        \n",
    "        if CONFIG.get(\"ENABLE_HYBRID_ROUTING\", True):\n",
    "            content_density = classify_content_density(caption, transcript, category)\n",
    "            ROUTING_METRICS.increment(f\"content_density::{content_density.value}\")\n",
    "            \n",
    "            logger.info(f\"  ðŸ“š Content density: {content_density.value}\")\n",
    "            \n",
    "            if content_density == ContentDensity.DENSE:\n",
    "                logger.info(f\"    â†’ Full card generation (basic + cloze + tradeoff)\")\n",
    "                basic_cards = generate_basic_cards(atoms)\n",
    "                cloze_cards = generate_cloze_cards(atoms)\n",
    "                tradeoff_cards = generate_tradeoff_cards(atoms)\n",
    "                all_cards = basic_cards + cloze_cards + tradeoff_cards\n",
    "                \n",
    "            elif content_density == ContentDensity.LIGHT:\n",
    "                logger.info(f\"    â†’ Reference cards only (basic)\")\n",
    "                basic_cards = generate_basic_cards(atoms)\n",
    "                all_cards = basic_cards\n",
    "                for card in all_cards:\n",
    "                    card[\"content_density\"] = \"light\"\n",
    "                    \n",
    "            else:\n",
    "                logger.info(f\"    â†’ Content too sparse, skipping\")\n",
    "                return {\n",
    "                    \"reel_id\": reel_id,\n",
    "                    \"status\": \"rejected\",\n",
    "                    \"reason\": \"Content too sparse for card generation\"\n",
    "                }\n",
    "        else:\n",
    "            logger.info(f\"    â†’ Full card generation (hybrid routing disabled)\")\n",
    "            basic_cards = generate_basic_cards(atoms)\n",
    "            cloze_cards = generate_cloze_cards(atoms)\n",
    "            tradeoff_cards = generate_tradeoff_cards(atoms)\n",
    "            all_cards = basic_cards + cloze_cards + tradeoff_cards\n",
    "            content_density = ContentDensity.DENSE\n",
    "        \n",
    "        logger.info(f\"    Generated: Basic={len(basic_cards)}, Cloze={len(cloze_cards)}, Tradeoff={len(tradeoff_cards)}\")\n",
    "        \n",
    "        # Foundation Expansion (only for DENSE content)\n",
    "        topic_class = atoms.get(\"topic_class\", \"intermediate\")\n",
    "        if (content_density == ContentDensity.DENSE and\n",
    "            topic_class == \"foundation\" and\n",
    "            len(all_cards) <= 2 and\n",
    "            CONFIG.get(\"ENABLE_FOUNDATION_EXPANSION\", True)):\n",
    "            \n",
    "            temp_dims = QualityDimensions.calculate(atoms, all_cards)\n",
    "            was_enriched = atoms.get(\"was_enriched\", False)\n",
    "            tech_score = atoms.get(\"tech_score\", 0)\n",
    "            min_score_for_expansion = 3\n",
    "            \n",
    "            can_expand = (\n",
    "                temp_dims.correctness_score >= 0.9 and\n",
    "                temp_dims.richness_score < 0.7 and\n",
    "                not was_enriched and\n",
    "                tech_score >= min_score_for_expansion\n",
    "            )\n",
    "            \n",
    "            if can_expand:\n",
    "                logger.info(f\"  ðŸŒ± Attempting foundation expansion...\")\n",
    "                adjacent_card = generate_adjacent_card(atoms)\n",
    "                if adjacent_card:\n",
    "                    all_cards.append(adjacent_card)\n",
    "                    logger.info(f\"    âœ… Added adjacent card\")\n",
    "            elif was_enriched:\n",
    "                logger.info(f\"    â­ï¸ Skipping expansion - content was enriched\")\n",
    "            elif tech_score < min_score_for_expansion:\n",
    "                logger.info(f\"    â­ï¸ Skipping expansion - tech score too low ({tech_score} < {min_score_for_expansion})\")\n",
    "        \n",
    "        if not all_cards:\n",
    "            logger.info(f\"  âŒ No valid cards generated\")\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": \"rejected\",\n",
    "                \"reason\": \"No valid cards generated\"\n",
    "            }\n",
    "        \n",
    "        # Filter duplicates but DEFER fingerprint registration\n",
    "        unique_cards = []\n",
    "        duplicates_found = 0\n",
    "        pending_fingerprints = []\n",
    "        \n",
    "        for card in all_cards:\n",
    "            is_dup, fp_type = duplicate_detector.is_duplicate(card)\n",
    "            if not is_dup:\n",
    "                unique_cards.append(card)\n",
    "                fingerprint = duplicate_detector._create_fingerprint(card)\n",
    "                semantic_fp = duplicate_detector._create_semantic_fingerprint(card)\n",
    "                pending_fingerprints.append((fingerprint, semantic_fp))\n",
    "            else:\n",
    "                duplicates_found += 1\n",
    "                logger.debug(f\"    Duplicate card filtered: {fp_type}\")\n",
    "        \n",
    "        if duplicates_found > 0:\n",
    "            logger.info(f\"  ðŸ”„ Filtered {duplicates_found} duplicates\")\n",
    "        \n",
    "        final_count = len(unique_cards)\n",
    "        \n",
    "        if final_count == 0:\n",
    "            logger.info(f\"  âŒ All cards were duplicates\")\n",
    "            return {\n",
    "                \"reel_id\": reel_id,\n",
    "                \"status\": \"rejected\",\n",
    "                \"reason\": \"All cards were duplicates\"\n",
    "            }\n",
    "        \n",
    "        # Quality scoring\n",
    "        quality_dims = QualityDimensions.calculate(atoms, unique_cards)\n",
    "        logger.info(f\"  ðŸ“Š Quality: correctness={quality_dims.correctness_score:.2f}, richness={quality_dims.richness_score:.2f}\")\n",
    "        \n",
    "        # Completion state with explicit reason\n",
    "        completion_state, completion_reason = determine_completion_state(\n",
    "            final_count,\n",
    "            atoms.get(\"has_tradeoffs\", False),\n",
    "            len(tradeoff_cards),\n",
    "            duplicates_found,\n",
    "            topic_class=atoms.get(\"topic_class\", \"intermediate\")\n",
    "        )\n",
    "        \n",
    "        atoms[\"completion_state\"] = completion_state.value\n",
    "        atoms[\"completion_reason\"] = completion_reason\n",
    "        atoms[\"_completion_state_obj\"] = completion_state\n",
    "        \n",
    "        logger.info(f\"  ðŸŽ¯ Completion: {completion_state.value} ({completion_reason})\")\n",
    "        \n",
    "        # Confidence calculation\n",
    "        confidence = calculate_confidence(atoms, unique_cards, quality_dims)\n",
    "        logger.info(f\"  ðŸ’ª Confidence: {confidence:.2f}\")\n",
    "        \n",
    "        # Assign priorities and reel_id to cards\n",
    "        for card in unique_cards:\n",
    "            card[\"quality\"] = quality_score_fallback(card)\n",
    "            card[\"priority\"] = assign_priority(card[\"quality\"])\n",
    "            card[\"reel_id\"] = reel_id\n",
    "            card[\"topic_class\"] = topic_class\n",
    "            card[\"confidence\"] = confidence\n",
    "        \n",
    "        # NOW register fingerprints after cards passed all quality gates\n",
    "        for fingerprint, semantic_fp in pending_fingerprints:\n",
    "            duplicate_detector.add_fingerprint(fingerprint, semantic_fp)\n",
    "        \n",
    "        # Explicit approval logic\n",
    "        auto_approved = (\n",
    "            confidence >= 0.85 and\n",
    "            quality_dims.correctness_score >= 0.9\n",
    "        )\n",
    "        \n",
    "        if auto_approved:\n",
    "            logger.info(f\"  âœ… Auto-approved (high confidence)\")\n",
    "        \n",
    "        result = {\n",
    "            \"reel_id\": reel_id,\n",
    "            \"status\": \"success\",\n",
    "            \"cards\": unique_cards,\n",
    "            \"basic_count\": len(basic_cards),\n",
    "            \"cloze_count\": len(cloze_cards),\n",
    "            \"tradeoff_count\": len(tradeoff_cards),\n",
    "            \"duplicates_filtered\": duplicates_found,\n",
    "            \"final_count\": final_count,\n",
    "            \"confidence\": round(confidence, 2),\n",
    "            \"completion_state\": completion_state.value,\n",
    "            \"completion_reason\": completion_reason,\n",
    "            \"correctness_score\": quality_dims.correctness_score,\n",
    "            \"richness_score\": quality_dims.richness_score,\n",
    "            \"attempted_enrichment\": atoms.get(\"was_enriched\", False),\n",
    "            \"topic_class\": atoms.get(\"topic_class\", \"intermediate\"),\n",
    "            \"prompt_strategy\": atoms.get(\"prompt_strategy\", \"unknown\"),\n",
    "            \"prompt_version\": atoms.get(\"prompt_version\", \"unknown\"),\n",
    "            \"auto_approved\": auto_approved\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        save_cached_result(reel_id, \"stage2_cards\", result)\n",
    "        \n",
    "        # Record prompt version success ONLY if cards meet minimum threshold\n",
    "        prompt_version = atoms.get(\"prompt_version\")\n",
    "        if prompt_version and not prompt_recorded:\n",
    "            if final_count >= CONFIG[\"MIN_CARDS_FOR_PARTIAL\"]:\n",
    "                record_prompt_version_result(prompt_version, True, stage=\"complete\")\n",
    "            else:\n",
    "                record_prompt_version_result(prompt_version, False, stage=\"generation\")\n",
    "            prompt_recorded = True\n",
    "        \n",
    "        # Record success in rejection memory if enriched\n",
    "        if atoms.get(\"was_enriched\") and rejection_memory:\n",
    "            concept_key = atoms.get(\"learning_key\", atoms.get(\"concept\", \"\"))\n",
    "            delta_score = atoms.get(\"_delta_score\", 0)\n",
    "            rejection_memory.record_success(\n",
    "                concept_key,\n",
    "                atoms.get(\"prompt_strategy\", \"\"),\n",
    "                confidence=confidence,\n",
    "                category=atoms.get(\"category\", \"\"),\n",
    "                topic_class=atoms.get(\"topic_class\", \"\"),\n",
    "                delta_score=delta_score\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"  âœ… Generated {final_count} cards\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âŒ Card generation failed: {e}\")\n",
    "        \n",
    "        # Record prompt version failure\n",
    "        prompt_version = atoms.get(\"prompt_version\")\n",
    "        if prompt_version and not prompt_recorded:\n",
    "            record_prompt_version_result(prompt_version, False, stage=\"mechanical\")\n",
    "        \n",
    "        error_tracker.record(\n",
    "            \"card_generation_error\",\n",
    "            str(e),\n",
    "            {\"reel_id\": reel_id, \"atoms_concept\": atoms.get(\"concept\", \"\")}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"reel_id\": reel_id,\n",
    "            \"status\": \"error\",\n",
    "            \"reason\": str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# SINGLE REEL PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def process_single_reel(reel: Dict) -> Dict:\n",
    "    \"\"\"Process one reel through the full pipeline\"\"\"\n",
    "    start_time = time.time()\n",
    "    reel_id = str(reel.get(\"reel_id\", reel.get(\"id\", \"\")))\n",
    "    \n",
    "    # Create ProcessingResult\n",
    "    result = ProcessingResult(\n",
    "        reel_id=reel_id,\n",
    "        status=\"pending\",\n",
    "        processing_time=0.0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Extract atoms\n",
    "        atoms = extract_atoms_with_retry(reel)\n",
    "        \n",
    "        if atoms.get(\"status\") != \"extracted\":\n",
    "            result.status = atoms.get(\"status\", \"error\")\n",
    "            result.reason = atoms.get(\"reason\", \"Unknown error\")\n",
    "            result.processing_time = time.time() - start_time\n",
    "            \n",
    "            # Track rejection\n",
    "            if result.status == \"rejected\":\n",
    "                STATISTICS.update(result)\n",
    "            \n",
    "            return result.to_dict()\n",
    "        \n",
    "        logger.info(f\"  âœ… Atoms extracted (score: {atoms.get('tech_score')}, class: {atoms.get('topic_class')})\")\n",
    "        \n",
    "        # Stage 2: Generate cards\n",
    "        card_result = generate_cards_from_atoms(atoms)\n",
    "        \n",
    "        result.processing_time = time.time() - start_time\n",
    "        \n",
    "        if card_result.get(\"status\") == \"success\":\n",
    "            result.status = \"success\"\n",
    "            result.cards = card_result.get(\"cards\", [])\n",
    "            result.confidence = card_result.get(\"confidence\", 0)\n",
    "            result.tech_score = atoms.get(\"tech_score\", 0)\n",
    "            result.topic_class = atoms.get(\"topic_class\", \"\")\n",
    "            result.prompt_strategy = atoms.get(\"prompt_strategy\", \"\")\n",
    "            \n",
    "            logger.info(f\"  âœ… {card_result['final_count']} cards (confidence: {card_result['confidence']:.2f}, time: {result.processing_time:.1f}s)\")\n",
    "        else:\n",
    "            result.status = card_result.get(\"status\", \"error\")\n",
    "            result.reason = card_result.get(\"reason\", \"Unknown error\")\n",
    "            logger.info(f\"  âŒ {result.status}: {result.reason}\")\n",
    "        \n",
    "        # Update statistics\n",
    "        STATISTICS.update(result)\n",
    "        \n",
    "        return result.to_dict()\n",
    "        \n",
    "    except Exception as e:\n",
    "        result.status = \"error\"\n",
    "        result.reason = str(e)\n",
    "        result.processing_time = time.time() - start_time\n",
    "        result.error_details = {\"traceback\": traceback.format_exc()}\n",
    "        \n",
    "        logger.error(f\"  âŒ Processing failed: {e}\")\n",
    "        error_tracker.record(\n",
    "            \"reel_processing_error\",\n",
    "            str(e),\n",
    "            {\"reel_id\": reel_id, \"processing_time\": result.processing_time}\n",
    "        )\n",
    "        \n",
    "        return result.to_dict()\n",
    "\n",
    "# ============================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def process_batch(reels: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Process reels in parallel\"\"\"\n",
    "    logger.info(f\"ðŸ“¦ Processing batch of {len(reels)} reels\")\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=CONFIG[\"MAX_WORKERS\"]) as executor:\n",
    "        future_to_reel = {\n",
    "            executor.submit(process_single_reel, reel): reel\n",
    "            for reel in reels\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_reel):\n",
    "            reel = future_to_reel[future]\n",
    "            reel_id = str(reel.get(\"reel_id\", reel.get(\"id\", \"\")))\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Periodic saves\n",
    "                duplicate_detector.save_if_dirty()\n",
    "                progress_tracker.flush()\n",
    "                \n",
    "                # Mark as processed\n",
    "                if result.get(\"reel_id\"):\n",
    "                    progress_tracker.mark_processed(result[\"reel_id\"])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Error processing reel {reel_id}: {e}\")\n",
    "                error_tracker.record(\n",
    "                    \"batch_processing_error\",\n",
    "                    str(e),\n",
    "                    {\"reel_id\": reel_id}\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"reel_id\": reel_id,\n",
    "                    \"status\": \"error\",\n",
    "                    \"reason\": str(e)\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================\n",
    "# CSV LOADING & DIAGNOSTICS\n",
    "# ============================================================\n",
    "\n",
    "def diagnose_csv_file(csv_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Diagnose CSV file issues and provide actionable feedback.\n",
    "    \"\"\"\n",
    "    diagnosis = {\n",
    "        \"exists\": False,\n",
    "        \"readable\": False,\n",
    "        \"line_count\": 0,\n",
    "        \"issues\": [],\n",
    "        \"sample_lines\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        csv_file = Path(csv_path)\n",
    "        diagnosis[\"exists\"] = csv_file.exists()\n",
    "        \n",
    "        if not diagnosis[\"exists\"]:\n",
    "            diagnosis[\"issues\"].append(f\"File not found: {csv_path}\")\n",
    "            return diagnosis\n",
    "        \n",
    "        # Try to read with different encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(csv_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                    lines = f.readlines()\n",
    "                diagnosis[\"readable\"] = True\n",
    "                diagnosis[\"encoding\"] = encoding\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if not diagnosis[\"readable\"]:\n",
    "            diagnosis[\"issues\"].append(\"Cannot read file with any standard encoding\")\n",
    "            return diagnosis\n",
    "        \n",
    "        diagnosis[\"line_count\"] = len(lines)\n",
    "        \n",
    "        if len(lines) < 2:\n",
    "            diagnosis[\"issues\"].append(\"File has fewer than 2 lines (need header + data)\")\n",
    "            return diagnosis\n",
    "        \n",
    "        # Analyze header\n",
    "        header = lines[0].strip()\n",
    "        diagnosis[\"header\"] = header\n",
    "        \n",
    "        # Count fields in header\n",
    "        header_fields = len(header.split('|'))\n",
    "        diagnosis[\"expected_fields\"] = header_fields\n",
    "        \n",
    "        # Sample first few data lines\n",
    "        for i, line in enumerate(lines[1:6], start=2):\n",
    "            if line.strip():\n",
    "                diagnosis[\"sample_lines\"].append(f\"Line {i}: {line.strip()[:100]}\")\n",
    "        \n",
    "        # Check for consistent field counts\n",
    "        bad_lines = []\n",
    "        for i, line in enumerate(lines[1:21], start=2):  # Check first 20 data lines\n",
    "            if line.strip():\n",
    "                field_count = len(line.strip().split('|'))\n",
    "                if field_count != header_fields:\n",
    "                    bad_lines.append((i, field_count, line.strip()[:80]))\n",
    "        \n",
    "        if bad_lines:\n",
    "            diagnosis[\"issues\"].append(f\"Found {len(bad_lines)} lines with inconsistent field counts\")\n",
    "            for line_num, count, preview in bad_lines[:3]:\n",
    "                diagnosis[\"issues\"].append(f\"Line {line_num}: has {count} fields (expected {header_fields}): {preview}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        diagnosis[\"issues\"].append(f\"Error reading file: {e}\")\n",
    "    \n",
    "    return diagnosis\n",
    "\n",
    "def load_csv_file(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV file with robust error handling\"\"\"\n",
    "    logger.info(f\"Loading CSV: {csv_path}\")\n",
    "    \n",
    "    # First diagnose\n",
    "    diagnosis = diagnose_csv_file(csv_path)\n",
    "    \n",
    "    if not diagnosis[\"exists\"]:\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    \n",
    "    if diagnosis[\"issues\"]:\n",
    "        logger.warning(f\"CSV issues detected: {diagnosis['issues']}\")\n",
    "    \n",
    "    try:\n",
    "        # Try standard parsing first\n",
    "        df = pd.read_csv(csv_path, sep=\"|\", encoding='utf-8')\n",
    "        logger.info(f\"âœ… CSV loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "        \n",
    "    except pd.errors.ParserError as e:\n",
    "        logger.warning(f\"Parser error: {e}\")\n",
    "        \n",
    "        # Try with error handling\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                csv_path,\n",
    "                sep=\"|\",\n",
    "                on_bad_lines='skip',\n",
    "                engine='python',\n",
    "                quoting=1,\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            logger.info(f\"âœ… CSV recovered with lenient parsing: {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Lenient parsing also failed: {e2}\")\n",
    "            raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error loading CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "def normalize_reel(reel: dict) -> dict:\n",
    "    \"\"\"Normalize reel data to prevent NaN and type issues\"\"\"\n",
    "    normalized = {}\n",
    "    \n",
    "    for k, v in reel.items():\n",
    "        if isinstance(v, float):\n",
    "            if math.isnan(v):\n",
    "                normalized[k] = \"\"\n",
    "            else:\n",
    "                # Convert to int if it's a whole number\n",
    "                if v.is_integer():\n",
    "                    normalized[k] = str(int(v))\n",
    "                else:\n",
    "                    normalized[k] = str(v)\n",
    "        elif v is None:\n",
    "            normalized[k] = \"\"\n",
    "        else:\n",
    "            normalized[k] = str(v)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution function\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"STARTING ANKI GENERATION PIPELINE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Check Ollama health\n",
    "    if not check_ollama_health():\n",
    "        logger.error(\"âŒ Ollama server not responding!\")\n",
    "        logger.error(\"Please start Ollama: ollama serve\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Load CSV\n",
    "    if not Path(CONFIG[\"CSV_FILE\"]).exists():\n",
    "        logger.error(f\"âŒ CSV not found: {CONFIG['CSV_FILE']}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df = load_csv_file(CONFIG[\"CSV_FILE\"])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to load CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logger.info(f\"ðŸ“„ Loaded {len(df)} reels from CSV\")\n",
    "    \n",
    "    # Normalize and filter unprocessed reels\n",
    "    unprocessed = []\n",
    "    for _, row in df.iterrows():\n",
    "        reel_dict = normalize_reel(row.to_dict())\n",
    "        reel_id = str(reel_dict.get(\"reel_id\", \"\"))\n",
    "        \n",
    "        if reel_id and not progress_tracker.is_processed(reel_id):\n",
    "            unprocessed.append(reel_dict)\n",
    "    \n",
    "    logger.info(f\"â™»ï¸ {len(progress_tracker.processed):,} already processed\")\n",
    "    logger.info(f\"ðŸ†• {len(unprocessed):,} remaining\")\n",
    "    \n",
    "    if not unprocessed:\n",
    "        logger.info(\"âœ… All reels already processed!\")\n",
    "        return\n",
    "    \n",
    "    # PHASE 1: Apply pre-filtering\n",
    "    if CONFIG.get(\"ENABLE_CONTENT_FILTERING\", True):\n",
    "        logger.info(f\"ðŸ” Applying content quality filters...\")\n",
    "        filtered_reels = []\n",
    "        skip_stats = defaultdict(int)\n",
    "        \n",
    "        for reel in unprocessed:\n",
    "            is_worthy, skip_reason, quality_assessment = should_process_reel(reel)\n",
    "            if is_worthy:\n",
    "                filtered_reels.append(reel)\n",
    "            else:\n",
    "                skip_stats[skip_reason] += 1\n",
    "                reel_id = str(reel.get(\"reel_id\", reel.get(\"id\", \"\")))\n",
    "                if reel_id:\n",
    "                    progress_tracker.mark_processed(reel_id)\n",
    "        \n",
    "        logger.info(f\"âœ… {len(filtered_reels)} reels passed quality filters\")\n",
    "        logger.info(f\"â­ï¸ {len(unprocessed) - len(filtered_reels)} reels skipped:\")\n",
    "        for reason, count in sorted(skip_stats.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"   - {reason}: {count}\")\n",
    "        \n",
    "        if not filtered_reels:\n",
    "            logger.info(\"âš ï¸ No reels passed quality filters!\")\n",
    "            return\n",
    "    else:\n",
    "        logger.info(f\"â­ï¸ Content filtering disabled - processing all reels\")\n",
    "        filtered_reels = unprocessed\n",
    "    \n",
    "    # Limit to MAX_REELS\n",
    "    to_process = filtered_reels[:CONFIG[\"MAX_REELS\"]]\n",
    "    \n",
    "    if len(to_process) < len(filtered_reels):\n",
    "        logger.info(f\"âš ï¸ Processing first {CONFIG['MAX_REELS']} reels (use MAX_REELS config to adjust)\")\n",
    "    \n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    batch_size = CONFIG[\"BATCH_SIZE\"]\n",
    "    \n",
    "    total_batches = (len(to_process) - 1) // batch_size + 1\n",
    "    \n",
    "    for i in range(0, len(to_process), batch_size):\n",
    "        batch = to_process[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        logger.info(f\"\\nðŸ“¦ Batch {batch_num}/{total_batches} ({len(batch)} reels)\")\n",
    "        \n",
    "        batch_results = process_batch(batch)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Intermediate save\n",
    "        if batch_num % 5 == 0:\n",
    "            logger.info(\"ðŸ’¾ Intermediate save...\")\n",
    "            duplicate_detector.save_if_dirty(force=True)\n",
    "            progress_tracker.flush()\n",
    "            if rejection_memory:\n",
    "                rejection_memory.save()\n",
    "    \n",
    "    # Final save\n",
    "    logger.info(\"\\nðŸ’¾ Final save...\")\n",
    "    duplicate_detector.save_if_dirty(force=True)\n",
    "    progress_tracker.flush()\n",
    "    if rejection_memory:\n",
    "        rejection_memory.save()\n",
    "    terminal_rejections.save()\n",
    "    prompt_version_manager.save()\n",
    "    confidence_calibrator.save()\n",
    "    \n",
    "    # Save routing metrics\n",
    "    try:\n",
    "        with open(ROUTING_METRICS_FILE, 'w') as f:\n",
    "            json.dump(dict(ROUTING_METRICS.counts), f, indent=2)\n",
    "        logger.info(f\"ðŸ’¾ Routing metrics saved to: {ROUTING_METRICS_FILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âš ï¸ Failed to save routing metrics: {e}\")\n",
    "    \n",
    "    # Generate output\n",
    "    successful = [r for r in all_results if r.get(\"status\") == \"success\"]\n",
    "    \n",
    "    all_cards = []\n",
    "    for result in successful:\n",
    "        all_cards.extend(result.get(\"cards\", []))\n",
    "    \n",
    "    # Save cards\n",
    "    output_file = CONFIG[\"OUT_DIR\"] / \"anki_cards.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_cards, f, indent=2)\n",
    "    \n",
    "    # Needs review\n",
    "    needs_review = [r for r in successful if 0.65 <= r.get(\"confidence\", 0) < 0.85]\n",
    "    if needs_review:\n",
    "        review_file = CONFIG[\"OUT_DIR\"] / \"needs_review.json\"\n",
    "        with open(review_file, 'w') as f:\n",
    "            json.dump(needs_review, f, indent=2)\n",
    "    \n",
    "    # Generate summary\n",
    "    generate_summary(all_results, successful, all_cards, needs_review, start_time)\n",
    "    \n",
    "    logger.info(\"\\nâœ… PIPELINE COMPLETE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "def generate_summary(all_results: List[Dict], successful: List[Dict], \n",
    "                    all_cards: List[Dict], needs_review: List[Dict], start_time: float):\n",
    "    \"\"\"Generate comprehensive summary of pipeline execution\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_processed = len(all_results)\n",
    "    successful_count = len(successful)\n",
    "    error_count = len([r for r in all_results if r.get(\"status\") == \"error\"])\n",
    "    rejected_count = len([r for r in all_results if r.get(\"status\") == \"rejected\"])\n",
    "    \n",
    "    # Card statistics\n",
    "    total_cards = len(all_cards)\n",
    "    basic_cards = sum(1 for c in all_cards if c.get(\"type\") == \"basic\")\n",
    "    cloze_cards = sum(1 for c in all_cards if c.get(\"type\") == \"cloze\")\n",
    "    tradeoff_cards = sum(1 for c in all_cards if c.get(\"type\") == \"tradeoff\")\n",
    "    \n",
    "    total_duplicates = sum(r.get(\"duplicates_filtered\", 0) for r in successful)\n",
    "    \n",
    "    # Quality statistics\n",
    "    if successful:\n",
    "        avg_quality = sum(c.get(\"quality\", 0) for c in all_cards) / max(len(all_cards), 1)\n",
    "        avg_confidence = sum(r.get(\"confidence\", 0) for r in successful) / max(len(successful), 1)\n",
    "        avg_correctness = sum(r.get(\"correctness_score\", 0) for r in successful) / max(len(successful), 1)\n",
    "        avg_richness = sum(r.get(\"richness_score\", 0) for r in successful) / max(len(successful), 1)\n",
    "    else:\n",
    "        avg_quality = avg_confidence = avg_correctness = avg_richness = 0\n",
    "    \n",
    "    # Priority distribution\n",
    "    p0_cards = sum(1 for c in all_cards if c.get(\"priority\") == \"P0\")\n",
    "    p1_cards = sum(1 for c in all_cards if c.get(\"priority\") == \"P1\")\n",
    "    p2_cards = sum(1 for c in all_cards if c.get(\"priority\") == \"P2\")\n",
    "    \n",
    "    # Confidence breakdown\n",
    "    high_confidence = [r for r in successful if r.get(\"confidence\", 0) >= 0.85]\n",
    "    low_confidence = [r for r in successful if r.get(\"confidence\", 0) < 0.65]\n",
    "    \n",
    "    auto_approved_count = len([r for r in successful if r.get(\"auto_approved\", False)])\n",
    "    \n",
    "    # Topic class distribution\n",
    "    topic_class_dist = defaultdict(int)\n",
    "    for r in successful:\n",
    "        topic_class_dist[r.get(\"topic_class\", \"unknown\")] += 1\n",
    "    \n",
    "    # Completion states\n",
    "    completion_states = defaultdict(int)\n",
    "    for r in successful:\n",
    "        completion_states[r.get(\"completion_state\", \"unknown\")] += 1\n",
    "    \n",
    "    # Enrichment statistics\n",
    "    enrichment_attempts = len([r for r in all_results if r.get(\"attempted_enrichment\")])\n",
    "    enrichment_successes = len([r for r in successful if r.get(\"attempted_enrichment\")])\n",
    "    \n",
    "    # Print report\n",
    "    logger.info(\"\\n\" + \"=\" * 60)\n",
    "    logger.info(\"FINAL STATS\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"â±ï¸  Time: {elapsed:.1f}s\")\n",
    "    logger.info(f\"ðŸŽ¬ Processed: {total_processed} reels\")\n",
    "    logger.info(f\"âœ… Success: {successful_count} reels\")\n",
    "    logger.info(f\"âŒ Errors: {error_count} reels\")\n",
    "    logger.info(f\"ðŸš« Rejected: {rejected_count} reels\")\n",
    "    logger.info(f\"ðŸŽ´ Cards Generated: {total_cards}\")\n",
    "    logger.info(f\"   Basic: {basic_cards}, Cloze: {cloze_cards}, Tradeoff: {tradeoff_cards}\")\n",
    "    logger.info(f\"ðŸ”„ Duplicates filtered: {total_duplicates}\")\n",
    "    logger.info(f\"â­ Avg Quality: {avg_quality:.1f}/100\")\n",
    "    logger.info(f\"ðŸ“Š Avg Confidence: {avg_confidence:.2f}\")\n",
    "    logger.info(f\"ðŸš€ Throughput: {total_processed/elapsed:.2f} reels/s\")\n",
    "    logger.info(f\"ðŸ“Š QUALITY DIMENSIONS:\")\n",
    "    logger.info(f\"   Correctness: {avg_correctness:.2f}/1.0\")\n",
    "    logger.info(f\"   Richness: {avg_richness:.2f}/1.0\")\n",
    "    logger.info(f\"ðŸŽ¯ COMPLETION STATES:\")\n",
    "    for state, count in sorted(completion_states.items()):\n",
    "        pct = (count / successful_count * 100) if successful_count > 0 else 0\n",
    "        logger.info(f\"   {state}: {count} ({pct:.1f}%)\")\n",
    "    logger.info(f\"ðŸ“Œ Priority Distribution:\")\n",
    "    logger.info(f\"   P0 (â‰¥85): {p0_cards} cards\")\n",
    "    logger.info(f\"   P1 (70-84): {p1_cards} cards\")\n",
    "    logger.info(f\"   P2 (<70): {p2_cards} cards\")\n",
    "    logger.info(f\"ðŸ“Š Confidence Breakdown:\")\n",
    "    logger.info(f\"   High (â‰¥0.85): {len(high_confidence)} reels â€” Auto-approved âœ…\")\n",
    "    logger.info(f\"   Medium (0.65-0.84): {len(needs_review)} reels â€” Review needed âš ï¸\")\n",
    "    logger.info(f\"   Low (<0.65): {len(low_confidence)} reels â€” Consider re-prompt ðŸ”„\")\n",
    "    logger.info(f\"   Auto-approved: {auto_approved_count} reels ðŸŽ¯\")\n",
    "    logger.info(f\"ðŸ§  ADAPTIVE LEARNING STATS\")\n",
    "    logger.info(f\"-\" * 60)\n",
    "    logger.info(f\"ðŸ”„ Enrichment attempts: {enrichment_attempts}\")\n",
    "    logger.info(f\"âœ… Enrichment successes: {enrichment_successes}\")\n",
    "    if enrichment_attempts > 0:\n",
    "        success_rate = (enrichment_successes / enrichment_attempts) * 100\n",
    "        logger.info(f\"ðŸ“ˆ Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Get more detailed statistics\n",
    "    if rejection_memory:\n",
    "        mem_stats = rejection_memory.get_learning_velocity()\n",
    "        logger.info(f\"ðŸ§  Rejection memory:\")\n",
    "        logger.info(f\"   Concepts tracked: {mem_stats['total_concepts']}\")\n",
    "        logger.info(f\"   Concepts learned: {mem_stats['concepts_learned']}\")\n",
    "        logger.info(f\"   Concepts still learning: {mem_stats['concepts_learning']}\")\n",
    "        logger.info(f\"   Avg attempts to success: {mem_stats['avg_attempts_until_success']:.1f}\")\n",
    "    \n",
    "    term_stats = terminal_rejections.get_stats()\n",
    "    if term_stats['total'] > 0:\n",
    "        logger.info(f\"ðŸš« Terminal Rejections: {term_stats['total']} reels\")\n",
    "    \n",
    "    logger.info(f\"ðŸ“š Topic Class Distribution:\")\n",
    "    for topic_class, count in sorted(topic_class_dist.items()):\n",
    "        logger.info(f\"   {topic_class}: {count} reels\")\n",
    "    \n",
    "    # System statistics\n",
    "    cache_stats = pipeline_cache.get_stats()\n",
    "    logger.info(f\"ðŸ’¾ Cache: {cache_stats['hit_rate']:.1%} hit rate ({cache_stats['hits']}/{cache_stats['hits']+cache_stats['misses']})\")\n",
    "    \n",
    "    llm_stats = llm_interface.get_stats()\n",
    "    logger.info(f\"ðŸ¤– LLM: {llm_stats['total_requests']} requests, {llm_stats['avg_response_time']:.2f}s avg\")\n",
    "    \n",
    "    logger.info(\"\\nðŸ“ Output Files:\")\n",
    "    logger.info(f\"   Cards: {CONFIG['OUT_DIR']}/anki_cards.json\")\n",
    "    if needs_review:\n",
    "        logger.info(f\"   Review needed: {CONFIG['OUT_DIR']}/needs_review.json\")\n",
    "    logger.info(f\"   Progress: {PROGRESS_FILE}\")\n",
    "    logger.info(f\"   Cache: {CONFIG['CACHE_DIR']}\")\n",
    "    logger.info(f\"   Metrics: {ROUTING_METRICS_FILE}\")\n",
    "    \n",
    "    logger.info(\"\\nâœ… SUMMARY COMPLETE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTION GUARD\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\n\\nâš ï¸  Interrupted - saving state...\")\n",
    "        duplicate_detector.save_if_dirty(force=True)\n",
    "        progress_tracker.flush()\n",
    "        if rejection_memory:\n",
    "            rejection_memory.save()\n",
    "        terminal_rejections.save()\n",
    "        prompt_version_manager.save()\n",
    "        confidence_calibrator.save()\n",
    "        \n",
    "        try:\n",
    "            with open(ROUTING_METRICS_FILE, 'w') as f:\n",
    "                json.dump(dict(ROUTING_METRICS.counts), f, indent=2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        logger.info(\"State saved. Exiting.\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\n\\nâŒ ERROR: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Emergency save\n",
    "        duplicate_detector.save_if_dirty(force=True)\n",
    "        if rejection_memory:\n",
    "            rejection_memory.save()\n",
    "        prompt_version_manager.save()\n",
    "        \n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
